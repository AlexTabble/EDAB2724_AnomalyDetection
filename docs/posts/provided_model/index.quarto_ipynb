{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Provided Model Tuning\n",
        "format: html\n",
        "categories: ['Model']\n",
        "---\n"
      ],
      "id": "b1ee5996"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_file_names = os.listdir(\"train/\")\n",
        "train_file_names.sort()\n",
        "\n",
        "train_files = []\n",
        "for file in train_file_names:\n",
        "    train_files.append(pd.read_csv(f\"train/{file}\", sep=\";\"))\n",
        "\n",
        "test_file_names = os.listdir(\"test/\")\n",
        "test_file_names.sort()\n",
        "\n",
        "test_files = []\n",
        "for file in test_file_names:\n",
        "    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n",
        "\n",
        "test_files[0].head()"
      ],
      "id": "98f302ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Student EDA\n",
        "Use this cell to explore the signal (e.g., plot, summary stats).\n"
      ],
      "id": "e0ffe2ed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# STUDENT EDA\n",
        "try:\n",
        "    df = test_files[0]\n",
        "    print(df.head())\n",
        "except Exception as e:\n",
        "    print('EDA note: run the original data-loading cells first (the ones that populate train_files/test_files).')\n",
        "    df.info()\n",
        "    df.describe()\n",
        "\n",
        "    df.plot(x='time')"
      ],
      "id": "41613020",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **The Model**"
      ],
      "id": "10add0cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numba import njit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"Determinant has increased; this should not happen\"\n",
        ")\n",
        "\n",
        "@njit\n",
        "def create_windows_numba(series, window_size):\n",
        "    n_windows = len(series) - window_size + 1\n",
        "    windows = np.empty((n_windows, window_size), dtype=np.float32)\n",
        "    for i in range(n_windows):\n",
        "        windows[i, :] = series[i : i + window_size]\n",
        "    return windows\n",
        "\n",
        "@njit\n",
        "def normalize_scores(scores):\n",
        "    mn = np.min(scores)\n",
        "    mx = np.max(scores)\n",
        "    return (scores - mn) / (mx - mn + 1e-8)\n",
        "\n",
        "\n",
        "class AnomalyDetectionModel:\n",
        "    def __init__(self, window_size=30, contamination=0.01):\n",
        "        self.window_size = window_size\n",
        "        self.offset = window_size // 2\n",
        "        self.contamination = contamination\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "        self.models = {\n",
        "            'IsolationForest': IsolationForest(contamination=contamination, random_state=42),\n",
        "            'OneClassSVM': OneClassSVM(kernel='rbf', gamma='scale', nu=contamination),\n",
        "            'EllipticEnvelope': EllipticEnvelope(contamination=contamination,\n",
        "                                                 support_fraction=0.75,\n",
        "                                                 random_state=42),\n",
        "        }\n",
        "\n",
        "        self.use_lof = True\n",
        "        self.lof_model = LocalOutlierFactor(n_neighbors=20,\n",
        "                                            contamination=contamination,\n",
        "                                            novelty=True)\n",
        "        self.full_anomaly_mask = None\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray = None):\n",
        "        self.train_windows = self._create_windows(X)\n",
        "        self.scaled_train_windows = self.scaler.fit_transform(self.train_windows)\n",
        "        for model in self.models.values():\n",
        "            model.fit(self.scaled_train_windows)\n",
        "        if self.use_lof:\n",
        "            self.lof_model.fit(self.scaled_train_windows)\n",
        "\n",
        "    def predict(self, X: np.ndarray):\n",
        "\n",
        "        test_windows = self._create_windows(X)\n",
        "        scaled = self.scaler.transform(test_windows)\n",
        "\n",
        "\n",
        "        all_scores = []\n",
        "        for model in self.models.values():\n",
        "            if hasattr(model, \"decision_function\"):\n",
        "                s = model.decision_function(scaled)\n",
        "                all_scores.append(normalize_scores(s))\n",
        "            else:\n",
        "                preds = model.predict(scaled)\n",
        "                all_scores.append(np.where(preds == -1, 0.0, 1.0))\n",
        "\n",
        "        if self.use_lof:\n",
        "            lof_s = self.lof_model.decision_function(scaled)\n",
        "            all_scores.append(normalize_scores(lof_s))\n",
        "\n",
        "        avg_scores = np.mean(np.stack(all_scores, axis=0), axis=0)\n",
        "        thresh = np.percentile(avg_scores, self.contamination * 100)\n",
        "        mask = np.zeros(len(X), dtype=int)\n",
        "        mask[self.offset : self.offset + len(avg_scores)] = (avg_scores <= thresh).astype(int)\n",
        "        self.full_anomaly_mask = mask\n",
        "        idx = np.argmin(avg_scores)\n",
        "        return idx + self.offset\n",
        "    def _create_windows(self, series: np.ndarray):\n",
        "        return create_windows_numba(series, self.window_size)"
      ],
      "id": "3efb3756",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# hybrid_anomaly_detector.py\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numba import njit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import ParameterGrid, train_test_split\n",
        "import itertools\n",
        "import math\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"Determinant has increased; this should not happen\"\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Low-level utilities\n",
        "# -------------------------\n",
        "@njit\n",
        "def create_windows_numba(series, window_size):\n",
        "    n_windows = len(series) - window_size + 1\n",
        "    windows = np.empty((n_windows, window_size), dtype=np.float32)\n",
        "    for i in range(n_windows):\n",
        "        windows[i, :] = series[i : i + window_size]\n",
        "    return windows\n",
        "\n",
        "@njit\n",
        "def normalize_scores_numba(scores):\n",
        "    mn = np.min(scores)\n",
        "    mx = np.max(scores)\n",
        "    out = (scores - mn) / (mx - mn + 1e-8)\n",
        "    return out\n",
        "\n",
        "def extract_ranges_from_mask(mask):\n",
        "    \"\"\"\n",
        "    Convert binary mask (1 anomaly, 0 normal) into list of (start, end) ranges [start, end)\n",
        "    \"\"\"\n",
        "    ranges = []\n",
        "    n = len(mask)\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        if mask[i] == 1:\n",
        "            start = i\n",
        "            j = i + 1\n",
        "            while j < n and mask[j] == 1:\n",
        "                j += 1\n",
        "            ranges.append((start, j))\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return ranges\n",
        "\n",
        "def extract_true_ranges(labels):\n",
        "    \"\"\"Assumes labels is 0/1 vector. Returns list of (start,end).\"\"\"\n",
        "    return extract_ranges_from_mask(labels.astype(int))\n",
        "\n",
        "# -------------------------\n",
        "# Evaluator\n",
        "# -------------------------\n",
        "class AnomalyEvaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate_pointwise(self, true_labels, pred_labels):\n",
        "        \"\"\"Return dict of standard point-wise metrics.\"\"\"\n",
        "        metrics = {}\n",
        "        metrics['precision'] = precision_score(true_labels, pred_labels, zero_division=0)\n",
        "        metrics['recall'] = recall_score(true_labels, pred_labels, zero_division=0)\n",
        "        metrics['f1'] = f1_score(true_labels, pred_labels, zero_division=0)\n",
        "        metrics['accuracy'] = accuracy_score(true_labels, pred_labels)\n",
        "        # AUC only if both classes present\n",
        "        try:\n",
        "            if len(np.unique(true_labels)) == 2:\n",
        "                metrics['roc_auc'] = roc_auc_score(true_labels, pred_labels)\n",
        "            else:\n",
        "                metrics['roc_auc'] = np.nan\n",
        "        except Exception:\n",
        "            metrics['roc_auc'] = np.nan\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_ranges(self, true_ranges, pred_ranges, signal_length=None):\n",
        "        \"\"\"\n",
        "        Range-level evaluation:\n",
        "          - precision = #pred_ranges that overlap any true_range / #pred_ranges\n",
        "          - recall = #true_ranges detected / #true_ranges\n",
        "          - f1 from precision & recall\n",
        "          - avg_overlap_ratio: average (over matched predicted ranges) of intersection/union\n",
        "          - avg_delay: average (pred_start - true_start) for matched ranges (if pred_start >= true_start),\n",
        "                       negative if predicted earlier.\n",
        "        \"\"\"\n",
        "        if signal_length is None:\n",
        "            # if no ranges and can't determine, set to 0\n",
        "            signal_length = 0\n",
        "\n",
        "        def overlap(a, b):\n",
        "            # a and b are (s,e)\n",
        "            s = max(a[0], b[0])\n",
        "            e = min(a[1], b[1])\n",
        "            return max(0, e - s)\n",
        "\n",
        "        matched_pred = 0\n",
        "        matched_true = 0\n",
        "        overlap_ratios = []\n",
        "        delays = []\n",
        "\n",
        "        true_matched_flags = [False] * len(true_ranges)\n",
        "\n",
        "        for p in pred_ranges:\n",
        "            # find best overlapping true range (largest overlap)\n",
        "            best_overlap = 0\n",
        "            best_idx = -1\n",
        "            for i, t in enumerate(true_ranges):\n",
        "                ov = overlap(p, t)\n",
        "                if ov > best_overlap:\n",
        "                    best_overlap = ov\n",
        "                    best_idx = i\n",
        "            if best_overlap > 0:\n",
        "                matched_pred += 1\n",
        "                true_t = true_ranges[best_idx]\n",
        "                if not true_matched_flags[best_idx]:\n",
        "                    matched_true += 1\n",
        "                    true_matched_flags[best_idx] = True\n",
        "\n",
        "                # compute union\n",
        "                union_len = (max(p[1], true_t[1]) - min(p[0], true_t[0]))\n",
        "                overlap_ratios.append(best_overlap / (union_len + 1e-8))\n",
        "\n",
        "                # detection delay: predicted start - true start\n",
        "                delays.append(p[0] - true_t[0])\n",
        "\n",
        "        n_pred = len(pred_ranges)\n",
        "        n_true = len(true_ranges)\n",
        "\n",
        "        precision = matched_pred / (n_pred + 1e-8) if n_pred > 0 else 0.0\n",
        "        recall = matched_true / (n_true + 1e-8) if n_true > 0 else 0.0\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "        avg_overlap = np.mean(overlap_ratios) if len(overlap_ratios) > 0 else 0.0\n",
        "        avg_delay = np.mean(delays) if len(delays) > 0 else np.nan\n",
        "\n",
        "        return {\n",
        "            \"range_precision\": precision,\n",
        "            \"range_recall\": recall,\n",
        "            \"range_f1\": f1,\n",
        "            \"avg_overlap_ratio\": float(avg_overlap),\n",
        "            \"avg_delay\": float(avg_delay) if not math.isnan(avg_delay) else np.nan,\n",
        "            \"n_pred_ranges\": n_pred,\n",
        "            \"n_true_ranges\": n_true\n",
        "        }\n",
        "\n",
        "# -------------------------\n",
        "# Hybrid Anomaly Detector\n",
        "# -------------------------\n",
        "class HybridAnomalyDetector:\n",
        "    \"\"\"\n",
        "    Hybrid anomaly detector with reproducibility and a tuning method (requires labelled validation set).\n",
        "    - window_size: sliding window length used to transform time series -> windows\n",
        "    - contamination: expected proportion of anomalies (used as thresholding fallback)\n",
        "    - models: default ensemble of IsolationForest, OneClassSVM, EllipticEnvelope, LocalOutlierFactor\n",
        "    - random_state: global random seed for reproducibility\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=30, contamination=0.01, random_state=42, use_lof=True):\n",
        "        self.window_size = int(window_size)\n",
        "        self.offset = self.window_size // 2\n",
        "        self.contamination = contamination\n",
        "        self.random_state = int(random_state)\n",
        "        np.random.seed(self.random_state)\n",
        "        random.seed(self.random_state)\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "        # instantiate default models with a place to tune their params\n",
        "        self.models = {\n",
        "            'IsolationForest': IsolationForest(contamination=contamination, random_state=self.random_state),\n",
        "            'OneClassSVM': OneClassSVM(kernel='rbf', gamma='scale', nu=contamination),\n",
        "            'EllipticEnvelope': EllipticEnvelope(contamination=contamination, support_fraction=0.75, random_state=self.random_state),\n",
        "        }\n",
        "\n",
        "        self.use_lof = use_lof\n",
        "        if self.use_lof:\n",
        "            self.models['LocalOutlierFactor'] = LocalOutlierFactor(n_neighbors=20, contamination=contamination, novelty=True)\n",
        "\n",
        "        # placeholders filled after fit:\n",
        "        self.train_windows = None\n",
        "        self.scaled_train_windows = None\n",
        "        self.fitted_models = {}\n",
        "        self.full_anomaly_mask = None\n",
        "        self.predicted_labels = None\n",
        "        self.ranges = None\n",
        "        self.scores = None  # ensemble scores per window\n",
        "\n",
        "    # ---------- window helpers ----------\n",
        "    def _create_windows(self, series: np.ndarray):\n",
        "        if len(series) < self.window_size:\n",
        "            raise ValueError(\"Series shorter than window_size.\")\n",
        "        return create_windows_numba(series.astype(np.float32), self.window_size)\n",
        "\n",
        "    # ---------- fitting ----------\n",
        "    def fit(self, X: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit all submodels on X (1D numpy array). No labels required.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        self.train_windows = self._create_windows(X)\n",
        "        self.scaled_train_windows = self.scaler.fit_transform(self.train_windows)\n",
        "        self.fitted_models = {}\n",
        "        # fit a fresh copy of each model\n",
        "        for name, model in self.models.items():\n",
        "            # clone by re-instantiating with same parameters to avoid state sharing\n",
        "            params = model.get_params()\n",
        "            cls = model.__class__\n",
        "            params['random_state'] = self.random_state if 'random_state' in params else params.get('random_state', None)\n",
        "            new_model = cls(**{k: v for k, v in params.items() if v is not None})\n",
        "            new_model.set_params(**{k: v for k, v in model.get_params().items() if k not in ['random_state']})\n",
        "            # Fit\n",
        "            try:\n",
        "                new_model.fit(self.scaled_train_windows)\n",
        "            except Exception:\n",
        "                # some models may require different interfaces; fallback to original fit\n",
        "                model.fit(self.scaled_train_windows)\n",
        "                new_model = model\n",
        "            self.fitted_models[name] = new_model\n",
        "\n",
        "    # ---------- score aggregation ----------\n",
        "    def _get_model_scores_on_windows(self, windows_scaled):\n",
        "        \"\"\"\n",
        "        Return dict of arrays: model_name -> normalized score array (higher -> more normal)\n",
        "        We'll normalize so that higher = more normal; we'll invert later to have anomaly score.\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        for name, model in self.fitted_models.items():\n",
        "            # prefer decision_function, then score_samples, else predict\n",
        "            if hasattr(model, \"decision_function\"):\n",
        "                s = model.decision_function(windows_scaled)\n",
        "                s = np.asarray(s, dtype=np.float32)\n",
        "                s = normalize_scores_numba(s)  # normalized 0..1\n",
        "                scores[name] = s\n",
        "            elif hasattr(model, \"score_samples\"):\n",
        "                s = model.score_samples(windows_scaled)\n",
        "                s = np.asarray(s, dtype=np.float32)\n",
        "                s = normalize_scores_numba(s)\n",
        "                scores[name] = s\n",
        "            else:\n",
        "                # predict: returns 1 (inlier) or -1 (outlier)\n",
        "                p = model.predict(windows_scaled)\n",
        "                s = np.where(np.asarray(p) == -1, 0.0, 1.0).astype(np.float32)\n",
        "                scores[name] = s\n",
        "        return scores\n",
        "\n",
        "    def predict_scores(self, X: np.ndarray):\n",
        "        \"\"\"\n",
        "        Produce ensemble anomaly scores for each sliding window and the aligned point-wise anomaly score.\n",
        "        Returns:\n",
        "          - avg_anomaly_scores_windows: np.array of length n_windows where higher = more anomalous\n",
        "          - aligned_point_scores: np.array length len(X) with 0..1 scores aligned to original samples\n",
        "        \"\"\"\n",
        "        test_windows = self._create_windows(X)\n",
        "        scaled = self.scaler.transform(test_windows)\n",
        "        model_scores = self._get_model_scores_on_windows(scaled)\n",
        "\n",
        "        # combine: convert 'normal' score->anomaly score = 1 - normalized_normal_score\n",
        "        stack = []\n",
        "        for v in model_scores.values():\n",
        "            stack.append(1.0 - v)\n",
        "        stacked = np.vstack(stack)  # shape (n_models, n_windows)\n",
        "        avg_scores = np.mean(stacked, axis=0)\n",
        "\n",
        "        # normalize avg_scores 0..1\n",
        "        avg_scores = (avg_scores - avg_scores.min()) / (avg_scores.max() - avg_scores.min() + 1e-8)\n",
        "\n",
        "        # align to original points by placing each window's score at its center index\n",
        "        aligned = np.zeros(len(X), dtype=np.float32)\n",
        "        n_windows = len(avg_scores)\n",
        "        start = self.offset\n",
        "        aligned[start : start + n_windows] = avg_scores\n",
        "\n",
        "        return avg_scores, aligned\n",
        "\n",
        "    def detect_anomalies(self, X: np.ndarray, top_k_ranges=None):\n",
        "        \"\"\"\n",
        "        Run detection and produce:\n",
        "         - predicted_labels (0/1 array)\n",
        "         - ranges list of (start,end) for contiguous anomaly segments\n",
        "         - scores aligned with original series\n",
        "         - returns peak_index (index of most anomalous point)\n",
        "        Optional: top_k_ranges - keep only top_k ranges by average score magnitude (descending)\n",
        "        \"\"\"\n",
        "        avg_scores_windows, aligned = self.predict_scores(X)\n",
        "        self.scores = aligned\n",
        "\n",
        "        # threshold: percentile based on contamination\n",
        "        thresh = np.percentile(avg_scores_windows, 100 * (1 - self.contamination))  # high anomaly = high score\n",
        "        # produce mask on aligned\n",
        "        mask = np.zeros(len(X), dtype=int)\n",
        "        mask[self.offset : self.offset + len(avg_scores_windows)] = (avg_scores_windows >= thresh).astype(int)\n",
        "        self.full_anomaly_mask = mask\n",
        "\n",
        "        # extract ranges\n",
        "        ranges = extract_ranges_from_mask(mask)\n",
        "        if top_k_ranges is not None and len(ranges) > top_k_ranges:\n",
        "            # rank by avg anomaly score within range\n",
        "            range_scores = [np.mean(aligned[s:e]) for s, e in ranges]\n",
        "            ranked = [r for _, r in sorted(zip(range_scores, ranges), key=lambda x: x[0], reverse=True)]\n",
        "            ranges = ranked[:top_k_ranges]\n",
        "            # rebuild mask and predicted_labels\n",
        "            new_mask = np.zeros_like(mask)\n",
        "            for s, e in ranges:\n",
        "                new_mask[s:e] = 1\n",
        "            mask = new_mask\n",
        "\n",
        "        self.ranges = ranges\n",
        "        self.predicted_labels = mask\n",
        "        # return index of most anomalous window center\n",
        "        peak_window_idx = int(np.argmax(avg_scores_windows))\n",
        "        peak_index = peak_window_idx + self.offset\n",
        "        return peak_index\n",
        "\n",
        "    # ---------- simple per-model hyperparameter tuning using labeled validation set ----------\n",
        "    def tune(self, X_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, param_grids: dict = None, max_iters_per_model=None):\n",
        "        \"\"\"\n",
        "        Tune each model's parameters using a labeled validation set.\n",
        "        - X_train: 1D numpy array used to fit scaler & models baseline\n",
        "        - X_val: 1D numpy array used for evaluating model behaviour\n",
        "        - y_val: 0/1 vector of same length as X_val with ground-truth labels\n",
        "        - param_grids: dict: model_name -> param_grid dict (as in sklearn). Example:\n",
        "            {\n",
        "              'IsolationForest': {'n_estimators': [100,200], 'max_samples': [0.5, 1.0]},\n",
        "              'OneClassSVM': {'nu': [0.005, 0.01, 0.05], 'gamma':['scale','auto']},\n",
        "            }\n",
        "        - If param_grids is None, uses reasonable defaults.\n",
        "        Returns best_params dict.\n",
        "        \"\"\"\n",
        "        # default grids\n",
        "        if param_grids is None:\n",
        "            param_grids = {\n",
        "                'IsolationForest': {'n_estimators': [100, 200], 'max_samples': [0.5, 1.0], 'contamination': [self.contamination]},\n",
        "                'OneClassSVM': {'nu': [max(1e-4, self.contamination/2), self.contamination, min(0.5, max(0.1, self.contamination*5))], 'gamma': ['scale', 'auto']},\n",
        "                'EllipticEnvelope': {'support_fraction': [0.6, 0.75, 1.0], 'contamination': [self.contamination]},\n",
        "                'LocalOutlierFactor': {'n_neighbors': [10, 20, 35], 'contamination': [self.contamination]}\n",
        "            }\n",
        "\n",
        "        # prepare training\n",
        "        self.fit(X_train)  # initial fit with default params so scaler available\n",
        "        best_params = {}\n",
        "\n",
        "        X_val_windows = self._create_windows(X_val)\n",
        "        scaled_val_windows = self.scaler.transform(X_val_windows)\n",
        "\n",
        "        # We'll iterate over parameter grid per model, fit on scaled train windows and evaluate using F1 on validation series\n",
        "        for model_name, model in list(self.models.items()):\n",
        "            grid = param_grids.get(model_name, {})\n",
        "            if len(grid) == 0:\n",
        "                best_params[model_name] = model.get_params()\n",
        "                continue\n",
        "\n",
        "            best_f1 = -1.0\n",
        "            best_param = model.get_params()\n",
        "\n",
        "            # create iterable of parameter combos\n",
        "            combos = list(ParameterGrid(grid))\n",
        "            if max_iters_per_model is not None and len(combos) > max_iters_per_model:\n",
        "                combos = random.sample(combos, max_iters_per_model)\n",
        "\n",
        "            for params in combos:\n",
        "                # instantiate fresh model with these params\n",
        "                cls = model.__class__\n",
        "                # ensure random_state consistency if supported\n",
        "                if 'random_state' in cls().get_params().keys():\n",
        "                    params_local = {**params, 'random_state': self.random_state}\n",
        "                else:\n",
        "                    params_local = params\n",
        "\n",
        "                try:\n",
        "                    candidate = cls(**params_local)\n",
        "                except Exception:\n",
        "                    # fallback: try without random_state\n",
        "                    candidate = cls(**{k: v for k, v in params.items()})\n",
        "\n",
        "                # fit candidate on scaled_train_windows\n",
        "                try:\n",
        "                    candidate.fit(self.scaled_train_windows)\n",
        "                except Exception:\n",
        "                    # some models (LOF with novelty=True OK). Try continue\n",
        "                    continue\n",
        "\n",
        "                # compute scores on validation windows\n",
        "                if hasattr(candidate, \"decision_function\"):\n",
        "                    s = candidate.decision_function(scaled_val_windows)\n",
        "                    s = normalize_scores_numba(np.asarray(s, dtype=np.float32))\n",
        "                    anom_scores = 1.0 - s\n",
        "                elif hasattr(candidate, \"score_samples\"):\n",
        "                    s = candidate.score_samples(scaled_val_windows)\n",
        "                    s = normalize_scores_numba(np.asarray(s, dtype=np.float32))\n",
        "                    anom_scores = 1.0 - s\n",
        "                else:\n",
        "                    p = candidate.predict(scaled_val_windows)\n",
        "                    anom_scores = np.where(np.asarray(p) == -1, 1.0, 0.0)\n",
        "\n",
        "                # align to original X_val\n",
        "                aligned = np.zeros(len(X_val), dtype=np.float32)\n",
        "                aligned[self.offset : self.offset + len(anom_scores)] = anom_scores\n",
        "\n",
        "                # threshold using contamination percentile\n",
        "                thr = np.percentile(anom_scores, 100 * (1 - self.contamination))\n",
        "                mask = np.zeros(len(X_val), dtype=int)\n",
        "                mask[self.offset : self.offset + len(anom_scores)] = (anom_scores >= thr).astype(int)\n",
        "\n",
        "                # compute F1 vs y_val\n",
        "                f1 = f1_score(y_val, mask, zero_division=0)\n",
        "\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_param = params\n",
        "\n",
        "            best_params[model_name] = best_param\n",
        "\n",
        "            # Update the model in self.models with best params and refit on full train\n",
        "            try:\n",
        "                cls = model.__class__\n",
        "                if 'random_state' in model.get_params().keys():\n",
        "                    final_params = {**best_param, 'random_state': self.random_state}\n",
        "                else:\n",
        "                    final_params = best_param\n",
        "                self.models[model_name] = cls(**final_params)\n",
        "            except Exception:\n",
        "                # if instantiation failed, keep original\n",
        "                pass\n",
        "\n",
        "            # re-fit all models at the end of loop\n",
        "            self.fit(X_train)\n",
        "\n",
        "        return best_params\n",
        "\n",
        "# -------------------------\n",
        "# Example usage with evaluator + test loop the user provided\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Example showing how to wire up with the evaluation loop you gave:\n",
        "    import os\n",
        "    from glob import glob\n",
        "\n",
        "    # create detector and evaluator\n",
        "    detector = HybridAnomalyDetector(window_size=30, contamination=0.01, random_state=42, use_lof=True)\n",
        "    evaluator = AnomalyEvaluator()\n",
        "\n",
        "    # test file names (user provided)\n",
        "    test_file_names = [\n",
        "        \"01.csv\", \"02.csv\", \"03.csv\", \"04.csv\", \"05.csv\",\n",
        "        \"06.csv\", \"07.csv\", \"08.csv\", \"09.csv\", \"10.csv\"\n",
        "    ]\n",
        "\n",
        "    point_metrics_list = []\n",
        "    range_metrics_list = []\n",
        "\n",
        "    def simple_top_k_refine(df, detector, k=10):\n",
        "        \"\"\"Refine detected ranges to top-k by amplitude / avg anomaly score\"\"\"\n",
        "        anomalies = detector.ranges or []\n",
        "        if len(anomalies) == 0:\n",
        "            return []\n",
        "        scores = []\n",
        "        for a in anomalies:\n",
        "            s = np.mean(np.abs(df['Value1'].iloc[a[0]:a[1]].to_numpy() - df['Value1'].mean()))\n",
        "            scores.append(s)\n",
        "        if len(anomalies) > k:\n",
        "            top_10 = [x for _, x in sorted(zip(scores, anomalies), reverse=True)[:k]]\n",
        "        else:\n",
        "            top_10 = anomalies\n",
        "        return top_10\n",
        "\n",
        "    for file in test_file_names:\n",
        "        path = os.path.join(\"test\", file)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Warning: {path} not found; skipping in example run.\")\n",
        "            continue\n",
        "\n",
        "        df = pd.read_csv(path, sep=\";\")\n",
        "        series = df['Value1'].to_numpy()\n",
        "\n",
        "        # Fit using the file itself for unsupervised case (or you can provide historical normal series)\n",
        "        try:\n",
        "            detector.fit(series)\n",
        "        except Exception as e:\n",
        "            print(\"Fit failed:\", e)\n",
        "            continue\n",
        "\n",
        "        # Detect anomalies normally first\n",
        "        detector.detect_anomalies(series)\n",
        "\n",
        "        # Refine to top 10 ranges by magnitude\n",
        "        anomalies = detector.ranges or []\n",
        "        top_10 = simple_top_k_refine(df, detector, k=10)\n",
        "\n",
        "        # Update detector output\n",
        "        detector.ranges = top_10\n",
        "        predicted_labels = np.zeros(len(df), dtype=int)\n",
        "        for start, end in top_10:\n",
        "            predicted_labels[start:end] = 1\n",
        "        detector.predicted_labels = predicted_labels\n",
        "\n",
        "        # Evaluate\n",
        "        true_labels = df['Labels'].to_numpy()\n",
        "        true_ranges = extract_true_ranges(true_labels)\n",
        "\n",
        "        point_metrics = evaluator.evaluate_pointwise(true_labels, detector.predicted_labels)\n",
        "        range_metrics = evaluator.evaluate_ranges(true_ranges, detector.ranges)\n",
        "\n",
        "        point_metrics_list.append(point_metrics)\n",
        "        range_metrics_list.append(range_metrics)\n",
        "\n",
        "        print(f\"\\nFile: {file}\")\n",
        "        print(\"Point-wise metrics:\", {k: f\"{v:.4f}\" for k, v in point_metrics.items()})\n",
        "        print(\"Range-level metrics:\", {k: f\"{v:.4f}\" for k, v in range_metrics.items()})\n",
        "        print(\"Top 10 anomaly ranges:\", top_10)"
      ],
      "id": "58d6d636",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation\n",
        "\n",
        "This pipeline works on the idea that:\n",
        "\n",
        "        1) it builds upon sliding windows\n",
        "        2) gathers normalised anomaly scores from each sub-model and uses them\n",
        "        3) averages the anomaly scores\n",
        "        4) computes a binary mask by thresholding at the 1st percentile so that it can compare outputs\n",
        "        5) stores self.full_anomaly_mask (same length as the placeholder value)\n",
        "        6) returns the single index of the lowest‐score window center which closes the loop on the sliding window idea\n",
        "\n",
        "\n",
        "## **STUDENT TODO — Implement your anomaly detector**\n",
        "Implement Machine Learning/ Statistical models or both. Use the test_files (test series) to train your models and list of anomaly index range for example Anomaly 1:   2001-2005\n",
        "Anomaly 2:   2010-2012\n",
        "\n",
        "\n",
        "**Constraints**\n",
        "\n",
        "- Keep it efficient; we will run this over 10 datasets and additional novel datasets in class.\n",
        "\n",
        "\n",
        "# **Importing Libraries**\n",
        "In this section, we import all required libraries.\n",
        "The anomaly detection system relies on several classical machine learning models (Isolation Forest, One-Class SVM, etc.) and combines them into a hybrid ensemble.\n",
        "We’ll also set warning filters and random seeds for reproducibility."
      ],
      "id": "fc8a9b1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Suppress unnecessary warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"Determinant has increased; this should not happen\"\n",
        ")\n",
        "\n",
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "# Machine learning models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
        ")\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Performance optimization\n",
        "from numba import njit"
      ],
      "id": "ebca3a2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Normalization of scores functions**\n",
        "\n",
        "These helper functions handle window creation (for temporal context) and normalization of model scores.\n",
        "The use of numba’s @njit decorator accelerates numerical operations by compiling them to machine code."
      ],
      "id": "c5bf1e3a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@njit\n",
        "def create_windows_numba(series, window_size):\n",
        "    n_windows = len(series) - window_size + 1\n",
        "    windows = np.empty((n_windows, window_size), dtype=np.float32)\n",
        "    for i in range(n_windows):\n",
        "        windows[i, :] = series[i : i + window_size]\n",
        "    return windows\n",
        "\n",
        "@njit\n",
        "def normalize_scores_numba(scores):\n",
        "    mn = np.min(scores)\n",
        "    mx = np.max(scores)\n",
        "    out = (scores - mn) / (mx - mn + 1e-8)\n",
        "    return out\n",
        "\n",
        "def extract_ranges_from_mask(mask):\n",
        "    \"\"\"\n",
        "    Convert binary mask (1 anomaly, 0 normal) into list of (start, end) ranges [start, end)\n",
        "    \"\"\"\n",
        "    ranges = []\n",
        "    n = len(mask)\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        if mask[i] == 1:\n",
        "            start = i\n",
        "            j = i + 1\n",
        "            while j < n and mask[j] == 1:\n",
        "                j += 1\n",
        "            ranges.append((start, j))\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return ranges\n",
        "\n",
        "def extract_true_ranges(labels):\n",
        "    \"\"\"Assumes labels is 0/1 vector. Returns list of (start,end).\"\"\"\n",
        "    return extract_ranges_from_mask(labels.astype(int))"
      ],
      "id": "463a41fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Evaluation Class**\n",
        "\n",
        "The AnomalyEvaluator class provides two complementary evaluation views:\n",
        "\n",
        "\n",
        "*   Point-wise metrics: Treat each time step as an independent prediction.\n",
        "\n",
        "*   Range-based metrics: Evaluate whether the detected anomaly overlaps with true anomalies, considering timing and coverage.\n"
      ],
      "id": "be564ffe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------\n",
        "# Evaluator\n",
        "# -------------------------\n",
        "class AnomalyEvaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate_pointwise(self, true_labels, pred_labels):\n",
        "        \"\"\"Return dict of standard point-wise metrics.\"\"\"\n",
        "        metrics = {}\n",
        "        metrics['precision'] = precision_score(true_labels, pred_labels, zero_division=0)\n",
        "        metrics['recall'] = recall_score(true_labels, pred_labels, zero_division=0)\n",
        "        metrics['f1'] = f1_score(true_labels, pred_labels, zero_division=0)\n",
        "        metrics['accuracy'] = accuracy_score(true_labels, pred_labels)\n",
        "        # AUC only if both classes present\n",
        "        try:\n",
        "            if len(np.unique(true_labels)) == 2:\n",
        "                metrics['roc_auc'] = roc_auc_score(true_labels, pred_labels)\n",
        "            else:\n",
        "                metrics['roc_auc'] = np.nan\n",
        "        except Exception:\n",
        "            metrics['roc_auc'] = np.nan\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_ranges(self, true_ranges, pred_ranges, signal_length=None):\n",
        "        \"\"\"\n",
        "        Range-level evaluation:\n",
        "          - precision = #pred_ranges that overlap any true_range / #pred_ranges\n",
        "          - recall = #true_ranges detected / #true_ranges\n",
        "          - f1 from precision & recall\n",
        "          - avg_overlap_ratio: average (over matched predicted ranges) of intersection/union\n",
        "          - avg_delay: average (pred_start - true_start) for matched ranges (if pred_start >= true_start),\n",
        "                       negative if predicted earlier.\n",
        "        \"\"\"\n",
        "        if signal_length is None:\n",
        "            # if no ranges and can't determine, set to 0\n",
        "            signal_length = 0\n",
        "\n",
        "        def overlap(a, b):\n",
        "            # a and b are (s,e)\n",
        "            s = max(a[0], b[0])\n",
        "            e = min(a[1], b[1])\n",
        "            return max(0, e - s)\n",
        "\n",
        "        matched_pred = 0\n",
        "        matched_true = 0\n",
        "        overlap_ratios = []\n",
        "        delays = []\n",
        "\n",
        "        true_matched_flags = [False] * len(true_ranges)\n",
        "\n",
        "        for p in pred_ranges:\n",
        "            # find best overlapping true range (largest overlap)\n",
        "            best_overlap = 0\n",
        "            best_idx = -1\n",
        "            for i, t in enumerate(true_ranges):\n",
        "                ov = overlap(p, t)\n",
        "                if ov > best_overlap:\n",
        "                    best_overlap = ov\n",
        "                    best_idx = i\n",
        "            if best_overlap > 0:\n",
        "                matched_pred += 1\n",
        "                true_t = true_ranges[best_idx]\n",
        "                if not true_matched_flags[best_idx]:\n",
        "                    matched_true += 1\n",
        "                    true_matched_flags[best_idx] = True\n",
        "\n",
        "                # compute union\n",
        "                union_len = (max(p[1], true_t[1]) - min(p[0], true_t[0]))\n",
        "                overlap_ratios.append(best_overlap / (union_len + 1e-8))\n",
        "\n",
        "                # detection delay: predicted start - true start\n",
        "                delays.append(p[0] - true_t[0])\n",
        "\n",
        "        n_pred = len(pred_ranges)\n",
        "        n_true = len(true_ranges)\n",
        "\n",
        "        precision = matched_pred / (n_pred + 1e-8) if n_pred > 0 else 0.0\n",
        "        recall = matched_true / (n_true + 1e-8) if n_true > 0 else 0.0\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "        avg_overlap = np.mean(overlap_ratios) if len(overlap_ratios) > 0 else 0.0\n",
        "        avg_delay = np.mean(delays) if len(delays) > 0 else np.nan\n",
        "\n",
        "        return {\n",
        "            \"range_precision\": precision,\n",
        "            \"range_recall\": recall,\n",
        "            \"range_f1\": f1,\n",
        "            \"avg_overlap_ratio\": float(avg_overlap),\n",
        "            \"avg_delay\": float(avg_delay) if not math.isnan(avg_delay) else np.nan,\n",
        "            \"n_pred_ranges\": n_pred,\n",
        "            \"n_true_ranges\": n_true\n",
        "        }"
      ],
      "id": "5e948212",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Hybrid Anomaly Class**\n",
        "\n",
        "\n",
        "*   Builds sliding windows\n",
        "*   Scales data\n",
        "*   Fits multiple models\n",
        "*   Aggregates anomaly scores\n",
        "*   Detects anomaly ranges\n"
      ],
      "id": "1e4040db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------\n",
        "# Hybrid Anomaly Detector\n",
        "# -------------------------\n",
        "class HybridAnomalyDetector:\n",
        "    \"\"\"\n",
        "    Hybrid anomaly detector with reproducibility and a tuning method (requires labelled validation set).\n",
        "    - window_size: sliding window length used to transform time series -> windows\n",
        "    - contamination: expected proportion of anomalies (used as thresholding fallback)\n",
        "    - models: default ensemble of IsolationForest, OneClassSVM, EllipticEnvelope, LocalOutlierFactor\n",
        "    - random_state: global random seed for reproducibility\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=30, contamination=0.01, random_state=42, use_lof=True):\n",
        "        self.window_size = int(window_size)\n",
        "        self.offset = self.window_size // 2\n",
        "        self.contamination = contamination\n",
        "        self.random_state = int(random_state)\n",
        "        np.random.seed(self.random_state)\n",
        "        random.seed(self.random_state)\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "        # instantiate default models with a place to tune their params\n",
        "        self.models = {\n",
        "            'IsolationForest': IsolationForest(contamination=contamination, random_state=self.random_state),\n",
        "            'OneClassSVM': OneClassSVM(kernel='rbf', gamma='scale', nu=contamination),\n",
        "            'EllipticEnvelope': EllipticEnvelope(contamination=contamination, support_fraction=0.75, random_state=self.random_state),\n",
        "        }\n",
        "\n",
        "        self.use_lof = use_lof\n",
        "        if self.use_lof:\n",
        "            self.models['LocalOutlierFactor'] = LocalOutlierFactor(n_neighbors=20, contamination=contamination, novelty=True)\n",
        "\n",
        "        # placeholders filled after fit:\n",
        "        self.train_windows = None\n",
        "        self.scaled_train_windows = None\n",
        "        self.fitted_models = {}\n",
        "        self.full_anomaly_mask = None\n",
        "        self.predicted_labels = None\n",
        "        self.ranges = None\n",
        "        self.scores = None  # ensemble scores per window\n",
        "\n",
        "    # ---------- window helpers ----------\n",
        "    def _create_windows(self, series: np.ndarray):\n",
        "        if len(series) < self.window_size:\n",
        "            raise ValueError(\"Series shorter than window_size.\")\n",
        "        return create_windows_numba(series.astype(np.float32), self.window_size)\n",
        "\n",
        "    # ---------- fitting ----------\n",
        "    def fit(self, X: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit all submodels on X (1D numpy array). No labels required.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        self.train_windows = self._create_windows(X)\n",
        "        self.scaled_train_windows = self.scaler.fit_transform(self.train_windows)\n",
        "        self.fitted_models = {}\n",
        "        # fit a fresh copy of each model\n",
        "        for name, model in self.models.items():\n",
        "            # clone by re-instantiating with same parameters to avoid state sharing\n",
        "            params = model.get_params()\n",
        "            cls = model.__class__\n",
        "            params['random_state'] = self.random_state if 'random_state' in params else params.get('random_state', None)\n",
        "            new_model = cls(**{k: v for k, v in params.items() if v is not None})\n",
        "            new_model.set_params(**{k: v for k, v in model.get_params().items() if k not in ['random_state']})\n",
        "            # Fit\n",
        "            try:\n",
        "                new_model.fit(self.scaled_train_windows)\n",
        "            except Exception:\n",
        "                # some models may require different interfaces; fallback to original fit\n",
        "                model.fit(self.scaled_train_windows)\n",
        "                new_model = model\n",
        "            self.fitted_models[name] = new_model\n",
        "\n",
        "    # ---------- score aggregation ----------\n",
        "    def _get_model_scores_on_windows(self, windows_scaled):\n",
        "        \"\"\"\n",
        "        Return dict of arrays: model_name -> normalized score array (higher -> more normal)\n",
        "        We'll normalize so that higher = more normal; we'll invert later to have anomaly score.\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        for name, model in self.fitted_models.items():\n",
        "            # prefer decision_function, then score_samples, else predict\n",
        "            if hasattr(model, \"decision_function\"):\n",
        "                s = model.decision_function(windows_scaled)\n",
        "                s = np.asarray(s, dtype=np.float32)\n",
        "                s = normalize_scores_numba(s)  # normalized 0..1\n",
        "                scores[name] = s\n",
        "            elif hasattr(model, \"score_samples\"):\n",
        "                s = model.score_samples(windows_scaled)\n",
        "                s = np.asarray(s, dtype=np.float32)\n",
        "                s = normalize_scores_numba(s)\n",
        "                scores[name] = s\n",
        "            else:\n",
        "                # predict: returns 1 (inlier) or -1 (outlier)\n",
        "                p = model.predict(windows_scaled)\n",
        "                s = np.where(np.asarray(p) == -1, 0.0, 1.0).astype(np.float32)\n",
        "                scores[name] = s\n",
        "        return scores\n",
        "\n",
        "    def predict_scores(self, X: np.ndarray):\n",
        "        \"\"\"\n",
        "        Produce ensemble anomaly scores for each sliding window and the aligned point-wise anomaly score.\n",
        "        Returns:\n",
        "          - avg_anomaly_scores_windows: np.array of length n_windows where higher = more anomalous\n",
        "          - aligned_point_scores: np.array length len(X) with 0..1 scores aligned to original samples\n",
        "        \"\"\"\n",
        "        test_windows = self._create_windows(X)\n",
        "        scaled = self.scaler.transform(test_windows)\n",
        "        model_scores = self._get_model_scores_on_windows(scaled)\n",
        "\n",
        "        # combine: convert 'normal' score->anomaly score = 1 - normalized_normal_score\n",
        "        stack = []\n",
        "        for v in model_scores.values():\n",
        "            stack.append(1.0 - v)\n",
        "        stacked = np.vstack(stack)  # shape (n_models, n_windows)\n",
        "        avg_scores = np.mean(stacked, axis=0)\n",
        "\n",
        "        # normalize avg_scores 0..1\n",
        "        avg_scores = (avg_scores - avg_scores.min()) / (avg_scores.max() - avg_scores.min() + 1e-8)\n",
        "\n",
        "        # align to original points by placing each window's score at its center index\n",
        "        aligned = np.zeros(len(X), dtype=np.float32)\n",
        "        n_windows = len(avg_scores)\n",
        "        start = self.offset\n",
        "        aligned[start : start + n_windows] = avg_scores\n",
        "\n",
        "        return avg_scores, aligned\n",
        "\n",
        "    def detect_anomalies(self, X: np.ndarray, top_k_ranges=None):\n",
        "        \"\"\"\n",
        "        Run detection and produce:\n",
        "         - predicted_labels (0/1 array)\n",
        "         - ranges list of (start,end) for contiguous anomaly segments\n",
        "         - scores aligned with original series\n",
        "         - returns peak_index (index of most anomalous point)\n",
        "        Optional: top_k_ranges - keep only top_k ranges by average score magnitude (descending)\n",
        "        \"\"\"\n",
        "        avg_scores_windows, aligned = self.predict_scores(X)\n",
        "        self.scores = aligned\n",
        "\n",
        "        # threshold: percentile based on contamination\n",
        "        thresh = np.percentile(avg_scores_windows, 100 * (1 - self.contamination))  # high anomaly = high score\n",
        "        # produce mask on aligned\n",
        "        mask = np.zeros(len(X), dtype=int)\n",
        "        mask[self.offset : self.offset + len(avg_scores_windows)] = (avg_scores_windows >= thresh).astype(int)\n",
        "        self.full_anomaly_mask = mask\n",
        "\n",
        "        # extract ranges\n",
        "        ranges = extract_ranges_from_mask(mask)\n",
        "        if top_k_ranges is not None and len(ranges) > top_k_ranges:\n",
        "            # rank by avg anomaly score within range\n",
        "            range_scores = [np.mean(aligned[s:e]) for s, e in ranges]\n",
        "            ranked = [r for _, r in sorted(zip(range_scores, ranges), key=lambda x: x[0], reverse=True)]\n",
        "            ranges = ranked[:top_k_ranges]\n",
        "            # rebuild mask and predicted_labels\n",
        "            new_mask = np.zeros_like(mask)\n",
        "            for s, e in ranges:\n",
        "                new_mask[s:e] = 1\n",
        "            mask = new_mask\n",
        "\n",
        "        self.ranges = ranges\n",
        "        self.predicted_labels = mask\n",
        "        # return index of most anomalous window center\n",
        "        peak_window_idx = int(np.argmax(avg_scores_windows))\n",
        "        peak_index = peak_window_idx + self.offset\n",
        "        return peak_index\n",
        "\n",
        "    # ---------- simple per-model hyperparameter tuning using labeled validation set ----------\n",
        "    def tune(self, X_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, param_grids: dict = None, max_iters_per_model=None):\n",
        "        \"\"\"\n",
        "        Tune each model's parameters using a labeled validation set.\n",
        "        - X_train: 1D numpy array used to fit scaler & models baseline\n",
        "        - X_val: 1D numpy array used for evaluating model behaviour\n",
        "        - y_val: 0/1 vector of same length as X_val with ground-truth labels\n",
        "        - param_grids: dict: model_name -> param_grid dict (as in sklearn). Example:\n",
        "            {\n",
        "              'IsolationForest': {'n_estimators': [100,200], 'max_samples': [0.5, 1.0]},\n",
        "              'OneClassSVM': {'nu': [0.005, 0.01, 0.05], 'gamma':['scale','auto']},\n",
        "            }\n",
        "        - If param_grids is None, uses reasonable defaults.\n",
        "        Returns best_params dict.\n",
        "        \"\"\"\n",
        "        # default grids\n",
        "        if param_grids is None:\n",
        "            param_grids = {\n",
        "                'IsolationForest': {'n_estimators': [100, 200], 'max_samples': [0.5, 1.0], 'contamination': [self.contamination]},\n",
        "                'OneClassSVM': {'nu': [max(1e-4, self.contamination/2), self.contamination, min(0.5, max(0.1, self.contamination*5))], 'gamma': ['scale', 'auto']},\n",
        "                'EllipticEnvelope': {'support_fraction': [0.6, 0.75, 1.0], 'contamination': [self.contamination]},\n",
        "                'LocalOutlierFactor': {'n_neighbors': [10, 20, 35], 'contamination': [self.contamination]}\n",
        "            }\n",
        "\n",
        "        # prepare training\n",
        "        self.fit(X_train)  # initial fit with default params so scaler available\n",
        "        best_params = {}\n",
        "\n",
        "        X_val_windows = self._create_windows(X_val)\n",
        "        scaled_val_windows = self.scaler.transform(X_val_windows)\n",
        "\n",
        "        # We'll iterate over parameter grid per model, fit on scaled train windows and evaluate using F1 on validation series\n",
        "        for model_name, model in list(self.models.items()):\n",
        "            grid = param_grids.get(model_name, {})\n",
        "            if len(grid) == 0:\n",
        "                best_params[model_name] = model.get_params()\n",
        "                continue\n",
        "\n",
        "            best_f1 = -1.0\n",
        "            best_param = model.get_params()\n",
        "\n",
        "            # create iterable of parameter combos\n",
        "            combos = list(ParameterGrid(grid))\n",
        "            if max_iters_per_model is not None and len(combos) > max_iters_per_model:\n",
        "                combos = random.sample(combos, max_iters_per_model)\n",
        "\n",
        "            for params in combos:\n",
        "                # instantiate fresh model with these params\n",
        "                cls = model.__class__\n",
        "                # ensure random_state consistency if supported\n",
        "                if 'random_state' in cls().get_params().keys():\n",
        "                    params_local = {**params, 'random_state': self.random_state}\n",
        "                else:\n",
        "                    params_local = params\n",
        "\n",
        "                try:\n",
        "                    candidate = cls(**params_local)\n",
        "                except Exception:\n",
        "                    # fallback: try without random_state\n",
        "                    candidate = cls(**{k: v for k, v in params.items()})\n",
        "\n",
        "                # fit candidate on scaled_train_windows\n",
        "                try:\n",
        "                    candidate.fit(self.scaled_train_windows)\n",
        "                except Exception:\n",
        "                    # some models (LOF with novelty=True OK). Try continue\n",
        "                    continue\n",
        "\n",
        "                # compute scores on validation windows\n",
        "                if hasattr(candidate, \"decision_function\"):\n",
        "                    s = candidate.decision_function(scaled_val_windows)\n",
        "                    s = normalize_scores_numba(np.asarray(s, dtype=np.float32))\n",
        "                    anom_scores = 1.0 - s\n",
        "                elif hasattr(candidate, \"score_samples\"):\n",
        "                    s = candidate.score_samples(scaled_val_windows)\n",
        "                    s = normalize_scores_numba(np.asarray(s, dtype=np.float32))\n",
        "                    anom_scores = 1.0 - s\n",
        "                else:\n",
        "                    p = candidate.predict(scaled_val_windows)\n",
        "                    anom_scores = np.where(np.asarray(p) == -1, 1.0, 0.0)\n",
        "\n",
        "                # align to original X_val\n",
        "                aligned = np.zeros(len(X_val), dtype=np.float32)\n",
        "                aligned[self.offset : self.offset + len(anom_scores)] = anom_scores\n",
        "\n",
        "                # threshold using contamination percentile\n",
        "                thr = np.percentile(anom_scores, 100 * (1 - self.contamination))\n",
        "                mask = np.zeros(len(X_val), dtype=int)\n",
        "                mask[self.offset : self.offset + len(anom_scores)] = (anom_scores >= thr).astype(int)\n",
        "\n",
        "                # compute F1 vs y_val\n",
        "                f1 = f1_score(y_val, mask, zero_division=0)\n",
        "\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_param = params\n",
        "\n",
        "            best_params[model_name] = best_param\n",
        "\n",
        "            # Update the model in self.models with best params and refit on full train\n",
        "            try:\n",
        "                cls = model.__class__\n",
        "                if 'random_state' in model.get_params().keys():\n",
        "                    final_params = {**best_param, 'random_state': self.random_state}\n",
        "                else:\n",
        "                    final_params = best_param\n",
        "                self.models[model_name] = cls(**final_params)\n",
        "            except Exception:\n",
        "                # if instantiation failed, keep original\n",
        "                pass\n",
        "\n",
        "            # re-fit all models at the end of loop\n",
        "            self.fit(X_train)\n",
        "\n",
        "        return best_params"
      ],
      "id": "08fb701f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Evaluation**\n",
        "Below, we show how to use the HybridAnomalyDetector and AnomalyEvaluator classes on test CSVs to detect and evaluate anomalies."
      ],
      "id": "eb94ad49"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example showing how to wire up with the evaluation loop you gave:\n",
        "    import os\n",
        "    from glob import glob\n",
        "\n",
        "    # create detector and evaluator\n",
        "    detector = HybridAnomalyDetector(window_size=30, contamination=0.01, random_state=42, use_lof=True)\n",
        "    evaluator = AnomalyEvaluator()\n",
        "\n",
        "    # test file names (user provided)\n",
        "    test_file_names = [\n",
        "        \"01.csv\", \"02.csv\", \"03.csv\", \"04.csv\", \"05.csv\",\n",
        "        \"06.csv\", \"07.csv\", \"08.csv\", \"09.csv\", \"10.csv\"\n",
        "    ]\n",
        "\n",
        "    point_metrics_list = []\n",
        "    range_metrics_list = []\n",
        "\n",
        "    def simple_top_k_refine(df, detector, k=10):\n",
        "        \"\"\"Refine detected ranges to top-k by amplitude / avg anomaly score\"\"\"\n",
        "        anomalies = detector.ranges or []\n",
        "        if len(anomalies) == 0:\n",
        "            return []\n",
        "        scores = []\n",
        "        for a in anomalies:\n",
        "            s = np.mean(np.abs(df['Value1'].iloc[a[0]:a[1]].to_numpy() - df['Value1'].mean()))\n",
        "            scores.append(s)\n",
        "        if len(anomalies) > k:\n",
        "            top_10 = [x for _, x in sorted(zip(scores, anomalies), reverse=True)[:k]]\n",
        "        else:\n",
        "            top_10 = anomalies\n",
        "        return top_10\n",
        "\n",
        "    for file in test_file_names:\n",
        "        path = os.path.join(\"test\", file)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Warning: {path} not found; skipping in example run.\")\n",
        "            continue\n",
        "\n",
        "        df = pd.read_csv(path, sep=\";\")\n",
        "        series = df['Value1'].to_numpy()\n",
        "\n",
        "        # Fit using the file itself for unsupervised case (or you can provide historical normal series)\n",
        "        try:\n",
        "            detector.fit(series)\n",
        "        except Exception as e:\n",
        "            print(\"Fit failed:\", e)\n",
        "            continue\n",
        "\n",
        "        # Detect anomalies normally first\n",
        "        detector.detect_anomalies(series)\n",
        "\n",
        "        # Refine to top 10 ranges by magnitude\n",
        "        anomalies = detector.ranges or []\n",
        "        top_10 = simple_top_k_refine(df, detector, k=10)\n",
        "\n",
        "        # Update detector output\n",
        "        detector.ranges = top_10\n",
        "        predicted_labels = np.zeros(len(df), dtype=int)\n",
        "        for start, end in top_10:\n",
        "            predicted_labels[start:end] = 1\n",
        "        detector.predicted_labels = predicted_labels\n",
        "\n",
        "        # Evaluate\n",
        "        true_labels = df['Labels'].to_numpy()\n",
        "        true_ranges = extract_true_ranges(true_labels)\n",
        "\n",
        "        point_metrics = evaluator.evaluate_pointwise(true_labels, detector.predicted_labels)\n",
        "        range_metrics = evaluator.evaluate_ranges(true_ranges, detector.ranges)\n",
        "\n",
        "        point_metrics_list.append(point_metrics)\n",
        "        range_metrics_list.append(range_metrics)\n",
        "\n",
        "        print(f\"\\nFile: {file}\")\n",
        "        print(\"Point-wise metrics:\", {k: f\"{v:.4f}\" for k, v in point_metrics.items()})\n",
        "        print(\"Range-level metrics:\", {k: f\"{v:.4f}\" for k, v in range_metrics.items()})\n",
        "        print(\"Top 10 anomaly ranges:\", top_10)"
      ],
      "id": "8ed03024",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Results Interpretation**\n",
        "| Metric Type                | Score                   | Interpretation                                                                                                                                    |\n",
        "| -------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Precision ≈ 1.0**        | Very high  |rarely producing false positives(cause for concern). |\n",
        "| **Recall ≈ 0.02 – 0.17**   | Very low                          | Detects only a small fraction of true anomaly points.                                    |\n",
        "| **F1 ≈ 0.04 – 0.29**       | Low                               |  Model is under-sensitive to anomalies.                                                         |\n",
        "| **Accuracy ≈ 0.90 – 0.95** | High but misleading               | Accuracy is inflated .                |\n",
        "| **ROC-AUC ≈ 0.51 – 0.58**  | Close to random                   | Is only slightly better than guessing at point level.                |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "| Range Metric                           | Score     | Interpretation                                                                                                                             |\n",
        "| -------------------------------------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Range Precision ≈ 1.0**              | Perfect            | Every detected range is correct no false anomaly windows.                                                       |\n",
        "| **Range Recall ≈ 0.4 – 0.8**           | Moderate           | The model captures 40–80 % of all true anomalous periods                                         |\n",
        "| **Range F1 ≈ 0.57 – 0.89**             | Moderate to strong | It catches the right events but not the full duration. |\n",
        "| **Average Overlap ≈ 0.03 – 0.09**      | Small              |  The model flags the start or peak, not the whole event.               |\n",
        "| **Average Delay ≈ 40 – 60 points**     | Noticeable lag     | The detector reacts only after anomalies become pronounced.                                                                                |\n",
        "| **n_pred_ranges ≈ 10 = n_true_ranges** | Balanced           | The refinement procedure ensures a consistent number of predictions, simplifying evaluation.                                               |\n",
        "\n",
        "\n",
        "\n",
        "# **Evaluation**\n",
        " The higher the accuracy the better."
      ],
      "id": "9fed6d9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use other various evaluation metrics applicable to your models."
      ],
      "id": "66bed0e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Limitations**\n",
        "While the ensemble sliding-window model seems to be a good fir,it does have some downsides.\n",
        "\n",
        "##**Computational Cost**:\n",
        "Because the model creates overlapping windows and runs multiple anomaly detection algorithms on each window, it can be computationally intensive—especially for long time series or when using a small window size (which results in many windows).\n",
        "\n",
        "##**This means it will require increased memory usage**\n",
        "\n",
        "##**It also means longer runtime compared to a single-model approach**\n",
        "\n",
        "It may not be suitable for very large datasets or real-time applications unless optimized or run on powerful hardware and there are some constructive bial issues that still need to be tested.\n",
        "\n",
        "**_For faster experiments, we could use a larger window size, downsampling the data, or disabling one or more models in the ensemble, but for this we need testing_**\n",
        "\n",
        "\n",
        "# **Visualisation of the anomalies**  \n",
        "\n",
        "Reuse this code to visualize the anomalies."
      ],
      "id": "a0a8a9c7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def visualize_anomaly_detection(test_df, model, file_idx=None):\n",
        "    \"\"\"\n",
        "    Visualizes:\n",
        "    - Signal (black)\n",
        "    - Ground truth anomalies (red)\n",
        "    - Predicted anomalies (green)\n",
        "    - Most anomalous index (blue dot)\n",
        "    \"\"\"\n",
        "    series = test_df['Value1'].to_numpy()\n",
        "    true_mask = test_df['Labels'].to_numpy().astype(bool)\n",
        "    pred_mask = model.full_anomaly_mask.astype(bool)\n",
        "    most_anomalous = np.argmin(pred_mask) if pred_mask.any() else None\n",
        "    pred_index = model.predict(series)  # triggers .full_anomaly_mask\n",
        "\n",
        "    plt.figure(figsize=(14, 4))\n",
        "    plt.plot(series, color='black', lw=1, label='Signal')\n",
        "\n",
        "    if pred_mask.any():\n",
        "        plt.fill_between(np.arange(len(series)), series,\n",
        "                         where=pred_mask, color='green', alpha=0.3,\n",
        "                         label='Predicted Anomaly')\n",
        "\n",
        "    if true_mask.any():\n",
        "        plt.fill_between(np.arange(len(series)), series,\n",
        "                         where=true_mask, color='red', alpha=0.3,\n",
        "                         label='True Anomaly')\n",
        "\n",
        "    if 0 <= pred_index < len(series):\n",
        "        plt.scatter(pred_index, series[pred_index], color='blue', s=50, label='Most Anomalous Point')\n",
        "\n",
        "    title = f\"File {file_idx}\" if file_idx is not None else \"Anomaly Detection\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -- Loop over all files and visualize each --\n",
        "for idx, (train, test) in enumerate(zip(train_files, test_files), 1):\n",
        "    model = AnomalyDetectionModel(window_size=30, contamination=0.01)\n",
        "    model.fit(train['Value1'].to_numpy(), train['Labels'].to_numpy())\n",
        "    model.predict(test['Value1'].to_numpy())  # sets .full_anomaly_mask\n",
        "    visualize_anomaly_detection(test, model, file_idx=idx)"
      ],
      "id": "b851c15a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/alex/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}