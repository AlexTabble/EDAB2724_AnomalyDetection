{
  "hash": "9eafdfbf8b1a668f592bbef49ed2190d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Exploratory Data Analysis'\n---\n\n::: {#385ef1a4 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain_file_names = os.listdir(\"train/\")\ntrain_file_names.sort()\n\ntrain_files = []\nfor file in train_file_names:\n    train_files.append(pd.read_csv(f\"train/{file}\", sep=\";\"))\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\nfor file in test_file_names:\n    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\ntest_files[0].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value1</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20.801402</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26.800208</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33.154527</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39.189824</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40.631321</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Distribution of Training Data\n\n::: {#109114fb .cell execution_count=2}\n``` {.python .cell-code}\nimport plotnine as p9\n\n(\n    p9.ggplot(train_files[0], p9.aes(x='Value1',fill='Value1',color='Value1'))+\n        p9.geom_density(color='red',alpha=0.1)+\n        p9.labs(\n            title = 'Distribution of Train File 1',\n            x = 'Value 1',\n            y = 'Density'\n        )\n).show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=640 height=480}\n:::\n:::\n\n\nThe training data is not normally distributed but we suspect its a combination \nof two normal distributions:\n- long-term trend\n- short-term fluctuations\n\nWe can break up the plot into the specific regions by using a **Fast Fourier Transform**\n\n::: {#f9b5535b .cell execution_count=3}\n``` {.python .cell-code}\nimport plotnine as p9\n\n(\n    p9.ggplot(train_files[0].reset_index(),p9.aes(y='Value1',x='index'))+\n        p9.geom_line()+\n        p9.geom_smooth(method='loess')\n).show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=640 height=480}\n:::\n:::\n\n\n::: {#25cc31dc .cell execution_count=4}\n``` {.python .cell-code}\nfrom scipy.fft import fft, ifft, fftfreq\n\nfft = fft(train_files[0]['Value1'])\nn = fftfreq(len(train_files[0]))\nt = train_files[0].index\ncutoff = 0.05\nlong_term_fft = fft.copy()\nlong_term_fft[np.abs(n) > cutoff] = 0\n\nshort_term_fft = fft.copy()\nshort_term_fft[np.abs(n) <= cutoff] = 0\n\nlong_term_signal = np.real(ifft(long_term_fft))\nshort_term_signal = np.real(ifft(short_term_fft))\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(3, 1, 1)\nplt.plot(t, train_files[0]['Value1'], color='gray')\nplt.title(\"Original Signal\")\n\nplt.subplot(3, 1, 2)\nplt.plot(t, long_term_signal, color='green')\nplt.title(\"Long-Term\")\n\nplt.subplot(3, 1, 3)\nplt.plot(t, short_term_signal, color='orange')\nplt.title(\"Short-Term\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=1142 height=566}\n:::\n:::\n\n\nThat didn't work exactly as we wanted.\n\nWe will try to reconstruct the signal as\n$$\nF(x)=asin(b\\theta) + dsin(c\\alpha) \n$$\n\n::: {#9cf510a6 .cell execution_count=5}\n``` {.python .cell-code}\nplt.plot(train_files[0]['Value1'])\n\nf = 3*np.sin(0.02*np.arange(1,2000))+33\n\nplt.plot(f)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=566 height=411}\n:::\n:::\n\n\n::: {#5349fa06 .cell execution_count=6}\n``` {.python .cell-code}\nsignal = train_files[0]['Value1']\nQ1 = np.quantile(signal, 0.25)\nQ3 = np.quantile(signal, 0.75)\nplt.plot(signal)\nplt.axhline(Q1)\nplt.axhline(Q3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=566 height=411}\n:::\n:::\n\n\n## Distribution of Testing Data\n\n::: {#50f5bdbc .cell execution_count=7}\n``` {.python .cell-code}\nsignal = test_files[0].reset_index()\n\n(\n    p9.ggplot(signal, p9.aes(x='Value1'))+\n        p9.geom_density()\n).show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=640 height=480}\n:::\n:::\n\n\n## Conclusion\n\nThe training data cannot be used to predict the testing data for two reasons:\n\n1. The training data is 2000 observations per file totaling 20,000 compared to \n    the testing data which is 10,000 per file totaling 100,000\n\n2. The training data does not follow the same distribution as the testing data\n    meaning a statistical model which makes an assumption about the distribution\n    isn't appropriate\n\nThe biggest problem is that the training data is so much less than the testing\ndata.\n\nIf we combine the files together, the train-test split won't be 80% and 20%\nfor train and test respectively but will be the other way around i.e:\n\n- train = 17% of all data\n- test = 83% of all data\n\nTo balance it properly would require sampling strategies but the goal is \nanomaly detection so oversampling to balance anomalies would add a lot of noise\nto the data.\n\nThe training data also has no anomalies to detect so the algorithm won't learn\nfrom the data itself.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}