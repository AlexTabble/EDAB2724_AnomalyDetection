[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models",
    "section": "",
    "text": "Benchmarking\n\n\n\nDocumentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZ Score Classifier\n\n\n\nModel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIsolation Forest\n\n\n\nModel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReadMe\n\n\n\nDocumentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProvided Model Tuning\n\n\n\nModel\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ReadME/index.html",
    "href": "posts/ReadME/index.html",
    "title": "ReadMe",
    "section": "",
    "text": "Model development and EDA was done in separate notebooks which consists of:\n\nZ-score classifier\nIsolation Forest classifier\nProvided Model\n\nA benchmarking class was created to evaluate model performance on the 10 test files for the z-score model."
  },
  {
    "objectID": "posts/ReadME/index.html#workflow",
    "href": "posts/ReadME/index.html#workflow",
    "title": "ReadMe",
    "section": "",
    "text": "Model development and EDA was done in separate notebooks which consists of:\n\nZ-score classifier\nIsolation Forest classifier\nProvided Model\n\nA benchmarking class was created to evaluate model performance on the 10 test files for the z-score model."
  },
  {
    "objectID": "posts/ReadME/index.html#breakdown-of-each-model",
    "href": "posts/ReadME/index.html#breakdown-of-each-model",
    "title": "ReadMe",
    "section": "Breakdown of each model",
    "text": "Breakdown of each model\n\nZ-score classifier\n\nPreprocessing\nIncluded using scipy.signal.detrend to remove seasonality from data and to make it stationery.\nIf not included, certain of the classifier’s decision metrics would flag the start and ends of certain data files as anomalous due to seasonality.\nThis step will also become necessary if the unseen data also has some seasonality which would decrease the precision of the model.\n\n\nModel Development\nThe classifier was made to be compatible around the scikit-learn API such other functionalities of sklearn can wrap around it.\nThis requires the model to inherit from BaseEstimator and ClassifierMixin, the fit method must return self and there must be a score function which predicts within the function.\nThis allows wrappers like GridSearchCV, LearningCurveDisplay and RocCurveDisplay to be used on the model which avoids having to reinvent the wheel.\nThe model’s classification is a consensus of multiple statistical metrics:\n\nZ-score (normal z-score which depends on train data metrics)\nIQR\nMean Absolute Deviation\nRolling Z-Score (Provided functions in the notebook deliverable)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe Rolling Z-Score does not use the train data metrics. It relies on a convolution of the data with a smoothing factor which is used to compute the residuals of that data.\nThe residuals of the training data will be different to the testing data residuals meaning the \\(\\mu\\) and \\(\\mu^2\\) will be dependent on the underlying input data\n\n\nThe default scoring function is recall.\nThe model was hyperparameter tuned for both recall and balanced accuracy.\n\n\n\n\nIsolation Forest\n\nPreprocessing\nThe model does internal feature engineering and preprocessing by gnerating rolling window features used for smoothing.\nIncludes a hyperparameter called target_anomalies\n\n\n\n\n\n\nWarning\n\n\n\nThe target_anomalies parameter is based of that the training data anomalies are known but the testing set anomalies are not.\nIf used, the training dataset anomalies can be set but it is based on the assumption that the testing data won’t know the true number of anomalies thus the test set has to be passed to the model after the data was trained.\nIts also possible that the test data will have far more or far less anomalies than the training data. If this is the case the model will perform worse as the contamination factor is not aligned properly but this is the trade-off to avoid data leakage.\n\n\nAfter determining the optimal contamination parameters and creating rolling windows, an IsolationForest model is fit to the data and the number of anomaly ranges are returned internally as well.\n\n\n\nProvided Model\nThe provided model was altered in testing notebooks for testing purposes but the best results will be passed to the unaltered model in the deliverable notebook.\nThe model was also made to wrap around the scikit-learn API like the Z-Score classifier such that hyperparameter tuning can be done on it."
  },
  {
    "objectID": "posts/EDA/index.html",
    "href": "posts/EDA/index.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "import pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain_file_names = os.listdir(\"train/\")\ntrain_file_names.sort()\n\ntrain_files = []\nfor file in train_file_names:\n    train_files.append(pd.read_csv(f\"train/{file}\", sep=\";\"))\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\nfor file in test_file_names:\n    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\ntest_files[0].head()\n\n\n\n\n\n\n\n\nValue1\nLabels\n\n\n\n\n0\n20.801402\n0\n\n\n1\n26.800208\n0\n\n\n2\n33.154527\n0\n\n\n3\n39.189824\n0\n\n\n4\n40.631321\n0"
  },
  {
    "objectID": "posts/EDA/index.html#distribution-of-training-data",
    "href": "posts/EDA/index.html#distribution-of-training-data",
    "title": "Exploratory Data Analysis",
    "section": "Distribution of Training Data",
    "text": "Distribution of Training Data\n\nimport plotnine as p9\n\n(\n    p9.ggplot(train_files[0], p9.aes(x='Value1',fill='Value1',color='Value1'))+\n        p9.geom_density(color='red',alpha=0.1)+\n        p9.labs(\n            title = 'Distribution of Train File 1',\n            x = 'Value 1',\n            y = 'Density'\n        )\n).show()\n\n\n\n\n\n\n\n\nThe training data is not normally distributed but we suspect its a combination of two normal distributions: - long-term trend - short-term fluctuations\nWe can break up the plot into the specific regions by using a Fast Fourier Transform\n\nimport plotnine as p9\n\n(\n    p9.ggplot(train_files[0].reset_index(),p9.aes(y='Value1',x='index'))+\n        p9.geom_line()+\n        p9.geom_smooth(method='loess')\n).show()\n\n\n\n\n\n\n\n\n\nfrom scipy.fft import fft, ifft, fftfreq\n\nfft = fft(train_files[0]['Value1'])\nn = fftfreq(len(train_files[0]))\nt = train_files[0].index\ncutoff = 0.05\nlong_term_fft = fft.copy()\nlong_term_fft[np.abs(n) &gt; cutoff] = 0\n\nshort_term_fft = fft.copy()\nshort_term_fft[np.abs(n) &lt;= cutoff] = 0\n\nlong_term_signal = np.real(ifft(long_term_fft))\nshort_term_signal = np.real(ifft(short_term_fft))\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(3, 1, 1)\nplt.plot(t, train_files[0]['Value1'], color='gray')\nplt.title(\"Original Signal\")\n\nplt.subplot(3, 1, 2)\nplt.plot(t, long_term_signal, color='green')\nplt.title(\"Long-Term\")\n\nplt.subplot(3, 1, 3)\nplt.plot(t, short_term_signal, color='orange')\nplt.title(\"Short-Term\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThat didn’t work exactly as we wanted.\nWe will try to reconstruct the signal as \\[\nF(x)=asin(b\\theta) + dsin(c\\alpha)\n\\]\n\nplt.plot(train_files[0]['Value1'])\n\nf = 3*np.sin(0.02*np.arange(1,2000))+33\n\nplt.plot(f)\nplt.show()\n\n\n\n\n\n\n\n\n\nsignal = train_files[0]['Value1']\nQ1 = np.quantile(signal, 0.25)\nQ3 = np.quantile(signal, 0.75)\nplt.plot(signal)\nplt.axhline(Q1)\nplt.axhline(Q3)\nplt.show()"
  },
  {
    "objectID": "posts/EDA/index.html#distribution-of-testing-data",
    "href": "posts/EDA/index.html#distribution-of-testing-data",
    "title": "Exploratory Data Analysis",
    "section": "Distribution of Testing Data",
    "text": "Distribution of Testing Data\n\nsignal = test_files[0].reset_index()\n\n(\n    p9.ggplot(signal, p9.aes(x='Value1'))+\n        p9.geom_density()\n).show()"
  },
  {
    "objectID": "posts/EDA/index.html#conclusion",
    "href": "posts/EDA/index.html#conclusion",
    "title": "Exploratory Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThe training data cannot be used to predict the testing data for two reasons:\n\nThe training data is 2000 observations per file totaling 20,000 compared to the testing data which is 10,000 per file totaling 100,000\nThe training data does not follow the same distribution as the testing data meaning a statistical model which makes an assumption about the distribution isn’t appropriate\n\nThe biggest problem is that the training data is so much less than the testing data.\nIf we combine the files together, the train-test split won’t be 80% and 20% for train and test respectively but will be the other way around i.e:\n\ntrain = 17% of all data\ntest = 83% of all data\n\nTo balance it properly would require sampling strategies but the goal is anomaly detection so oversampling to balance anomalies would add a lot of noise to the data.\nThe training data also has no anomalies to detect so the algorithm won’t learn from the data itself."
  },
  {
    "objectID": "posts/benchmarking/index.html",
    "href": "posts/benchmarking/index.html",
    "title": "Benchmarking",
    "section": "",
    "text": "Grouping the anomalous regions have proved to be a challenge and requires a robust method for discovering them."
  },
  {
    "objectID": "posts/benchmarking/index.html#the-class",
    "href": "posts/benchmarking/index.html#the-class",
    "title": "Benchmarking",
    "section": "The Class",
    "text": "The Class\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score\nfrom sklearn.metrics import precision_score, recall_score\n\nfrom great_tables import GT,md\n\n\nclass Benchmarking:\n    \"\"\"\n    Class for benchmarking anomaly detection models\n    \"\"\"\n\n    def __init__():\n        pass\n\n    @staticmethod\n    def create_anomaly_groups(data : pd.DataFrame|pd.Series, col='outlier',\n                              include_single_groups=False,\n                              show_printout=True,\n                              merge_tolerance=5,\n                              noise_tolerance=3) -&gt; list[tuple[int]]:\n        \"\"\"\n        Creates list of tuples containing start and end indices of anomalous regions\n        \n        ---\n        Parameters\n        - data (datalike): Either a dataframe or series\n        - col (str): Needs to be specified if a dataframe \n        - include_single_groups (boolean): whether to include anomalous regions\n                                            which has length of 1\n        - merge_tolerance (int) : threshold for gaps between anomalies and when\n                                    its appropriate to merge them.\n        - noise tolerance (int) : threshold for when groups are considered noise\n                                    and not truly anomalous regions\n        ---\n        Output\n         [(start_1,end_1),...,(start_n,end_n)]\n\n        ---\n        Example\n\n        output = Benchmarking.create_anomaly_groups(data)\n        print(output)\n\n        &gt;&gt;&gt; [(30,35),...,(8000,8029)]\n        \"\"\"\n        \n        group_ids = None\n        # StackOverflow magic that creates cumsum of anomaly col\n        if type(data) is pd.DataFrame:\n            group_ids = data[col].ne(data[col].shift()).cumsum()\n        else:\n            group_ids = data.ne(data.shift()).cumsum()\n            \n\n        grouped = data.groupby(group_ids)\n\n        groups = []\n\n        for group_id, group in grouped:\n\n            if type(group) is pd.DataFrame:\n                if group[col].iloc[0] == False:\n                    continue\n            else:\n                if group.iloc[0] == False:\n                    continue\n            \n            # If a single instance is an anomaly, skip or not?\n            if len(group) == 1 & include_single_groups: continue \n            \n            indices = group.index.tolist()\n            \n            groups.append(\n                (\n                    indices[0],\n                    indices[-1] + 1 # Last index is exclusive so increment by 1\n                )\n            )\n\n        merged = []\n        start_prev,end_prev = groups[0][0], groups[0][1]\n        for idx, _ in enumerate(groups):\n            \n            if idx == 0: continue\n            \n            start_current = groups[idx][0]\n            end_current = groups[idx][1]\n            \n            if (start_current - end_prev) &lt;= merge_tolerance:\n                end_prev = end_current\n            else:\n                merged.append((start_prev, end_prev))\n                start_prev = start_current\n                end_prev = end_current\n        merged.append((start_prev, end_prev))\n                \n        groups = merged\n        \n        groups = [group for group in groups if (group[1] - group[0]) &gt; noise_tolerance] \n            \n            \n        if show_printout:\n            print(f'{len(groups)} anomaly groups identified')\n        return groups\n\n    @staticmethod\n    def evaluate_model(y_true : np.array, y_pred : np.array,show_printout=True) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns a DataFrame which contains the metrics for the model\n\n        ---\n        Parameters\n        - y_true (np.array) : True outlier series\n        - y_pred (np.array) : Predicted outlier series\n\n        ---\n        Output\n        pd.DataFrame\n\n        |                   | Score  |\n        |-------------------|--------|\n        | Accuracy          | 20     |\n        | Precision         | 40     |\n        | Recall            | 89     |\n        | Balanced Accuracy | 89     |\n        | Groups Accuracy   | 40     |\n\n        ---\n        Example\n        \n        metrics = Benchmarking.evaluate_model(y_true,y_pred)\n        print(metrics)\n        \n        \"\"\"\n\n        accuracy = accuracy_score(y_true, y_pred)\n        precision = precision_score(y_true, y_pred)\n        recall = recall_score(y_true, y_pred)\n        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n\n        predicted_groups = Benchmarking.create_anomaly_groups(pd.Series(y_pred),\n                                                              show_printout=show_printout)\n        true_groups = Benchmarking.create_anomaly_groups(pd.Series(y_true),\n                                                         show_printout=show_printout)\n\n        group_accuracy = Benchmarking._evaluate_groups(predicted_groups, true_groups,\n                                                       group_penalty=False,\n                                                       show_printout=show_printout)\n        penalized_group_accuracy = Benchmarking._evaluate_groups(predicted_groups,\n                                                                 true_groups,\n                                                                 show_printout=show_printout)\n\n        metrics = pd.DataFrame({\n            'Score' : [\n                round(accuracy*100,2),\n                round(precision*100,2),\n                round(recall*100,2),\n                round(balanced_accuracy*100,2),\n                round(group_accuracy*100,2),\n                round(penalized_group_accuracy*100,2)\n            ]\n        }, index = ['Accuracy','Precision','Recall','Balanced Accuracy',\n                    'Group Accuracy','Penalised Group Accuracy'])\n        return metrics\n\n    @staticmethod\n    def _evaluate_groups(predicted_groups : list[tuple[int]],\n                         true_groups : list[tuple[int]],\n                         group_penalty : bool = True,\n                         show_printout :bool = True) -&gt; float:\n\n        if show_printout:\n            if len(predicted_groups) &gt; 10:\n                print(f'Model predicts {len(predicted_groups) -10} more than 10')\n            elif len(predicted_groups) &lt; 10:\n                print(f'Model predicts {10 - len(predicted_groups)} less than 10')\n            else:\n                print('Number of groups match!')\n                   \n        actual_starts = [idx[0] for idx in true_groups]\n        actual_ends = [idx[1] for idx in true_groups]\n\n        valid_preds = 0\n        bad_preds = []\n        for pred in predicted_groups:\n            is_start_correct = pred[0] in actual_starts\n            is_end_correct = pred[1] in actual_ends\n            \n            if is_end_correct & is_end_correct : valid_preds +=1\n            else:\n                bad_preds.append(pred)\n\n        accuracy = valid_preds / len(true_groups)\n\n        # PERF: I'm not sure wether this is appropriate\n        # This is on the assumption that the unseen data also contains\n        # exactly 10 anomalies which means the model does not necessarly generalise\n        # to unseen data where the anomaly count is known.\n        # NOTE: I will ask whether this is appropriate\n        # For testing purposes, I'm including this but use it by keeping data leakage\n        # in mind\n        if group_penalty:\n            penalty = min(len(predicted_groups), len(true_groups)) / max(\n                len(predicted_groups), len(true_groups)\n            )\n            accuracy = accuracy * penalty\n        \n        return accuracy\n        \n\n    @staticmethod\n    def print_evaluation(y_true : np.array, y_pred : np.array, model_name: str) -&gt; None:\n\n        metrics = Benchmarking.evaluate_model(y_true, y_pred)\n        metrics = metrics.reset_index().rename({'index' : 'Metric'})\n\n        (\n            GT(metrics)\n            .tab_header(md(f'Model Results for **{model_name}**'))\n            .tab_source_note(md(\"Metrics are in percentage(%)\"))\n        ).show()"
  },
  {
    "objectID": "posts/benchmarking/index.html#benchmark-documentation",
    "href": "posts/benchmarking/index.html#benchmark-documentation",
    "title": "Benchmarking",
    "section": "Benchmark Documentation",
    "text": "Benchmark Documentation\nThe most important function in the class is create_anomaly_groups()\n\ncreate_anomaly_groups\nParameters\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npd.DataFrame or pd.Series\nInput data containing anomalies. If DataFrame, the column must be specified.\n\n\ncol\nstr\nColumn name to check for anomalies (only needed if data is a DataFrame). Default is 'outlier'.\n\n\ninclude_single_groups\nbool\nWhether to include anomalous regions of length 1. Default is False.\n\n\nshow_printout\nbool\nWhether to print the number of anomaly groups identified. Default is True.\n\n\nmerge_tolerance\nint\nMaximum gap between consecutive anomaly regions that will be merged. Default is 5.\n\n\nnoise_tolerance\nint\nMinimum length for a group to be considered a true anomaly; shorter groups are treated as noise. Default is 3.\n\n\n\nOutput | Output | Type | Description | |—————-|——————–|———————–| | groups | list[tuple[int]] | \\({ (start_1,end_1),(start_2,end_2), \\dots, (start_n, end_n)}\\) This function can be used regardless of the chosen classifier.\n\n\nDoes it generalise?\nMost models have good accuracies and precision,recall values so actual classifiers are not the main problem.\nTo accurately state where the anomalous regions are is a challenge as you will have to tune the merge_tolerance and noise_tolerance parameters for the grouper which could introduce possible bias.\nIf the nature of the data is known(i.e. Financial, Geogolicial, etc) those parameters can be tuned with relevant business knowledge.\nThe dataset is called ec2_utilization.zip which monitors the cpu usage of an AWS EC2 instance over time. More research can be done to determine the norm in terms of deviation frequency and length of deviation. (I’m assuming the data is about cpu usage but we probably need to confirm)\nTo generalise the classification of anomalous period, I believe some leniency in the classification of what an anomalous period is necessary.\nFor example: If the daily LIBOR rate decreased for 5 days during the 2007-2008 GFC, it does not mean the entire period is not anomalous. If this 5-day period is not ignored as noise, the GFC would be classified as two anomalous periods(one before the 5 days and one after) which essentially fragments the period.\nThis is seen extensively in the Z-Score predictor where a single period is fragmented into multiple smaller periods with small gaps between them.\n\n\nPossible Improvements\ninclude_single_groups might cause a fragmentation of a a single anomalous region into two anomalous regions with incorrect \\(start_{n-1\\) and \\(end_{n}\\) values.\nThe reason I included the parameter in the first place is that if an anomaly has length of 1, its start- and end points are the same like \\((n,n)\\) which I didn’t want to deal with.\nAfter making the grouper more tolerant towards gaps and noise, I noticed that it might not be a good parameter to include."
  },
  {
    "objectID": "posts/benchmarking/index.html#available-functions-in-benchmarking.py",
    "href": "posts/benchmarking/index.html#available-functions-in-benchmarking.py",
    "title": "Benchmarking",
    "section": "Available Functions in Benchmarking.py",
    "text": "Available Functions in Benchmarking.py\n\nevaluate_model\nInput\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ny_true\nnp.array\nTrue binary anomaly labels (1 for anomaly, 0 for normal).\n\n\ny_pred\nnp.array\nPredicted binary anomaly labels.\n\n\n\nOutput\n\n\n\nOutput\nDataType\n\n\n\n\nmetrics\nDataFrame\n\n\n\n\n\n\n\n\n\n\nMetric\nDescription\n\n\n\n\nAccuracy\nStandard classification accuracy.\n\n\nPrecision\nFraction of predicted anomalies that are true anomalies.\n\n\nRecall\nFraction of true anomalies that were detected.\n\n\nBalanced Accuracy\nAverage of recall per class (handles imbalance).\n\n\nGroup Accuracy\nAccuracy at the anomaly group level (based on _evaluate_groups).\n\n\nPenalised Group Accuracy\nGroup accuracy with penalty applied for mismatch in number of groups.\n\n\n\nExample usage\nmetrics = Benchmarking.evaluate_model(y_true, y_pred)\nprint(metrics)\n\n\nprint_evaluation\nInput | Parameter | Type | Description | | ———— | ———- | ——————————————————– | | y_true | np.array | True anomaly labels. | | y_pred | np.array | Predicted anomaly labels. | | model_name | str | Name of the model, used for display in the table header. |\nOutput | Output | DataType | |———–|————| | printed DataFrame | None|\nExample usage\n\nBenchmarking.print_evaluation(y_true, y_pred, \"ARIMA(Tuned)\")\n\n\nBenchmarking usage\n\nFor Google Colab\nCopy and paste the file contents into a cell\n\n\nFor Personal Machine\nMake sure benchmarking.py file is in same directory as .ipynb/.qmd file\n.\n├── benchmarking.py\n├── test\n│   ├── 01.csv\n│   ├── 02.csv\n│   ├── 03.csv\n│   ├── 04.csv\n│   ├── 05.csv\n│   ├── 06.csv\n│   ├── 07.csv\n│   ├── 08.csv\n│   ├── 09.csv\n│   └── 10.csv\n└── Z*score_model.qmd -&gt; \\_Same directory level as benchmarking.py*\n\nfrom benchmarking import Benchmarking\n\nmetrics = Benchmarking.evaluate_model(y_true, y_pred)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nothing to see here :)"
  },
  {
    "objectID": "posts/z_score_model/index.html",
    "href": "posts/z_score_model/index.html",
    "title": "Z Score Classifier",
    "section": "",
    "text": "import pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom great_tables import GT, md\n\nfrom scipy.signal import detrend\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\n# for file in test_file_names:\n#    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\n\n# NOTE: I'm changing the way the data is imported by preprocessing\n# it while being imported. In the official colab, this should be done separately\nfor file in test_file_names:\n    data = pd.read_csv(f\"test/{file}\", sep=\";\")\n    data[\"Value1\"] = detrend(data[\"Value1\"])\n    test_files.append(data)\n\n\ntrain_file_names = os.listdir(\"train/\")\ntrain_files = []\nfor train in train_file_names:\n    train_files.append(pd.read_csv(f\"train/{file}\", sep=\";\"))\n\nThe test data is normally distributed meaning normal outlier detection techniques like z scores and IQR is applicable.\n\n\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n\nclass StatsModel(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Ensemble stats outlier classifier using:\n    - IQR\n    - Normal z_scores\n    - Rolling z_scores\n    - Mean Absolute Deviation\n    \"\"\"\n\n    def __init__(\n        self,\n        w_smooth=51,\n        w=61,\n        iqr_threshold=1.5,\n        mad_threshold=2.465,\n        normal_z_threshold=2,\n        rolling_z_threshold=2,\n        metric_consensus=3,\n    ):\n\n        self.w = w\n        self.w_smooth = w_smooth\n        self.iqr_threshold = iqr_threshold\n        self.mad_threshold = mad_threshold\n        self.normal_z_threshold = normal_z_threshold\n        self.rolling_z_threshold = rolling_z_threshold\n        self.metric_consensus = metric_consensus\n\n    def _determine_rolling_z_mask(self, X: np.array) -&gt; np.array:\n\n        k = np.ones(self.w_smooth) / self.w_smooth\n        smooth = np.convolve(X, k, mode=\"same\")\n\n        # I'm quite sure this step makes it less applicable for machine learning so\n        # its not used in fit\n        resid = X - smooth\n        kw = np.ones(self.w) / self.w\n        mu = np.convolve(resid, kw, mode=\"same\")\n        mu2 = np.convolve(resid * resid, kw, mode=\"same\")\n        var = np.maximum(mu2 - mu * mu, 1e-08)\n        sigma = np.sqrt(var)\n\n        z_rolling = np.abs((resid - mu) / (sigma + 1e-08))\n        return np.abs(z_rolling &gt; self.rolling_z_threshold)\n\n    def fit(self, X: np.array, y=None) -&gt; None:\n        self.iqr_ = np.quantile(X, 0.75) - np.quantile(X, 0.25)\n        self.mean_ = np.mean(X)\n        self.std_ = np.std(X)\n        self.median_ = np.median(X)\n        self.mad_ = np.sum(np.abs(X - self.mean_)) / len(X)\n\n        self.iqr_ub_ = self.mean_ + self.iqr_threshold * self.iqr_\n        self.iqr_lb_ = self.mean_ - self.iqr_threshold * self.iqr_\n        return self\n\n    def predict(self, X=None) -&gt; np.array:\n\n        IQR_mask = (X &gt; self.iqr_ub_) | (X &lt; self.iqr_lb_)\n        MAD_mask = np.abs(X - self.median_) &gt; self.mad_ * self.mad_threshold\n        Normal_Z_Mask = np.abs((X - self.mean_) / self.std_) &gt; self.normal_z_threshold\n        Rolling_Z_Mask = self._determine_rolling_z_mask(X)\n\n        mask_df = pd.DataFrame(\n            {\n                \"IQR\": IQR_mask,\n                \"MAD\": MAD_mask,\n                \"Normal_z\": Normal_Z_Mask,\n                \"Rolling_z\": Rolling_Z_Mask,\n            }\n        )\n\n        mask_df[\"outlier\"] = mask_df.sum(axis=1) &gt;= self.metric_consensus\n        return mask_df[\"outlier\"].to_numpy()\n\n    def score(self, X, y_true) -&gt; float:\n        y_pred = self.predict(X)\n\n        return recall_score(y_true, y_pred)"
  },
  {
    "objectID": "posts/z_score_model/index.html#preparing-data-and-motivation",
    "href": "posts/z_score_model/index.html#preparing-data-and-motivation",
    "title": "Z Score Classifier",
    "section": "",
    "text": "import pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom great_tables import GT, md\n\nfrom scipy.signal import detrend\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\n# for file in test_file_names:\n#    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\n\n# NOTE: I'm changing the way the data is imported by preprocessing\n# it while being imported. In the official colab, this should be done separately\nfor file in test_file_names:\n    data = pd.read_csv(f\"test/{file}\", sep=\";\")\n    data[\"Value1\"] = detrend(data[\"Value1\"])\n    test_files.append(data)\n\n\ntrain_file_names = os.listdir(\"train/\")\ntrain_files = []\nfor train in train_file_names:\n    train_files.append(pd.read_csv(f\"train/{file}\", sep=\";\"))\n\nThe test data is normally distributed meaning normal outlier detection techniques like z scores and IQR is applicable.\n\n\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n\nclass StatsModel(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Ensemble stats outlier classifier using:\n    - IQR\n    - Normal z_scores\n    - Rolling z_scores\n    - Mean Absolute Deviation\n    \"\"\"\n\n    def __init__(\n        self,\n        w_smooth=51,\n        w=61,\n        iqr_threshold=1.5,\n        mad_threshold=2.465,\n        normal_z_threshold=2,\n        rolling_z_threshold=2,\n        metric_consensus=3,\n    ):\n\n        self.w = w\n        self.w_smooth = w_smooth\n        self.iqr_threshold = iqr_threshold\n        self.mad_threshold = mad_threshold\n        self.normal_z_threshold = normal_z_threshold\n        self.rolling_z_threshold = rolling_z_threshold\n        self.metric_consensus = metric_consensus\n\n    def _determine_rolling_z_mask(self, X: np.array) -&gt; np.array:\n\n        k = np.ones(self.w_smooth) / self.w_smooth\n        smooth = np.convolve(X, k, mode=\"same\")\n\n        # I'm quite sure this step makes it less applicable for machine learning so\n        # its not used in fit\n        resid = X - smooth\n        kw = np.ones(self.w) / self.w\n        mu = np.convolve(resid, kw, mode=\"same\")\n        mu2 = np.convolve(resid * resid, kw, mode=\"same\")\n        var = np.maximum(mu2 - mu * mu, 1e-08)\n        sigma = np.sqrt(var)\n\n        z_rolling = np.abs((resid - mu) / (sigma + 1e-08))\n        return np.abs(z_rolling &gt; self.rolling_z_threshold)\n\n    def fit(self, X: np.array, y=None) -&gt; None:\n        self.iqr_ = np.quantile(X, 0.75) - np.quantile(X, 0.25)\n        self.mean_ = np.mean(X)\n        self.std_ = np.std(X)\n        self.median_ = np.median(X)\n        self.mad_ = np.sum(np.abs(X - self.mean_)) / len(X)\n\n        self.iqr_ub_ = self.mean_ + self.iqr_threshold * self.iqr_\n        self.iqr_lb_ = self.mean_ - self.iqr_threshold * self.iqr_\n        return self\n\n    def predict(self, X=None) -&gt; np.array:\n\n        IQR_mask = (X &gt; self.iqr_ub_) | (X &lt; self.iqr_lb_)\n        MAD_mask = np.abs(X - self.median_) &gt; self.mad_ * self.mad_threshold\n        Normal_Z_Mask = np.abs((X - self.mean_) / self.std_) &gt; self.normal_z_threshold\n        Rolling_Z_Mask = self._determine_rolling_z_mask(X)\n\n        mask_df = pd.DataFrame(\n            {\n                \"IQR\": IQR_mask,\n                \"MAD\": MAD_mask,\n                \"Normal_z\": Normal_Z_Mask,\n                \"Rolling_z\": Rolling_Z_Mask,\n            }\n        )\n\n        mask_df[\"outlier\"] = mask_df.sum(axis=1) &gt;= self.metric_consensus\n        return mask_df[\"outlier\"].to_numpy()\n\n    def score(self, X, y_true) -&gt; float:\n        y_pred = self.predict(X)\n\n        return recall_score(y_true, y_pred)"
  },
  {
    "objectID": "posts/z_score_model/index.html#helper-functions",
    "href": "posts/z_score_model/index.html#helper-functions",
    "title": "Z Score Classifier",
    "section": "Helper Functions",
    "text": "Helper Functions\n\nfrom benchmarking import Benchmarking\nimport time\n\n\ndef run_benchmark(model, show_printout=True):\n    benchmark_df = pd.DataFrame()\n    file_no = 1\n    for df in test_files:\n        if show_printout:\n            print(\n                f\"\\n=========================File {file_no}==========================\\n\"\n            )\n        model = model\n        X = df[\"Value1\"]\n        y_true = df[\"Labels\"]\n        model.fit(X)\n        y_pred = model.predict(X)\n        results = Benchmarking.evaluate_model(\n            y_true, y_pred, show_printout=show_printout\n        )\n        results = results.rename(columns={\"Score\": f\"File {file_no}\"})\n        benchmark_df = pd.concat([benchmark_df, results], axis=1)\n        file_no += 1\n\n    return benchmark_df\n\n\ndef create_benchmark_table(\n    benchmark_df: pd.DataFrame,\n    model_name: str = \"Stats Model\",\n    subtitle: str = \"results **before hyperparameter turning**\",\n) -&gt; None:\n\n    totals = benchmark_df.sum(axis=1) / 10\n    totals = totals.apply(lambda x: round(x, 2))\n\n    totals = totals.to_frame().transpose()\n    totals[\"group\"] = \"AGGREGATE\"\n    benchmark_df = benchmark_df.transpose()\n    benchmark_df[\"group\"] = \"File\"\n    benchmark_df = pd.concat([benchmark_df, totals])\n\n    benchmark_df = benchmark_df.reset_index().rename(columns={\"index\": \"File\"})\n    benchmark_df\n    (\n        GT(benchmark_df)\n        .tab_header(\n            title=md(f\"Benchmark Results: **{model_name}**\"), subtitle=md(subtitle)\n        )\n        .tab_source_note(\n            md(\"See benchmarks for how penalised group accuracy is calculated\")\n        )\n        .tab_stub(rowname_col=\"File\", groupname_col=\"group\")\n    ).show()"
  },
  {
    "objectID": "posts/z_score_model/index.html#benchmarking-time-complexity",
    "href": "posts/z_score_model/index.html#benchmarking-time-complexity",
    "title": "Z Score Classifier",
    "section": "Benchmarking Time complexity",
    "text": "Benchmarking Time complexity\n\nstart = time.perf_counter()\nms = []\nfor epoch in range(1, 1000):\n    start = time.perf_counter()\n    model = StatsModel()\n    run_benchmark(model, show_printout=False)\n    end = time.perf_counter()\n    ms.append((end - start))\nend = time.perf_counter()\nprint(\"Time for 1000 epochs\", end - start)\n\nTime for 1000 epochs 0.16048398699786048\n\n\n\nplt.plot(ms)\nplt.title(\"Total execution time for 10 Files\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Execution Time(in seconds)\")\nplt.show()\n\n\n\n\n\n\n\n\nThe execution time varies from 24 ms to 300 ms meaning we can tune this model for thousands of iterations to determine the optimal parameters"
  },
  {
    "objectID": "posts/z_score_model/index.html#base-benchmark",
    "href": "posts/z_score_model/index.html#base-benchmark",
    "title": "Z Score Classifier",
    "section": "Base Benchmark",
    "text": "Base Benchmark\n\nbenchmark_df = run_benchmark(StatsModel())\ncreate_benchmark_table(benchmark_df)\n\n\n=========================File 1==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 2==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 3==========================\n\n12 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 more than 10\nModel predicts 2 more than 10\n\n=========================File 4==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 5==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 6==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 7==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 8==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 9==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 10==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Stats Model\n\n\nresults before hyperparameter turning\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n98.63\n99.87\n84.75\n92.37\n90.0\n81.82\n\n\nFile 2\n96.66\n99.86\n68.79\n84.39\n80.0\n72.73\n\n\nFile 3\n96.64\n99.78\n57.27\n78.63\n70.0\n58.33\n\n\nFile 4\n97.84\n99.83\n73.26\n86.62\n90.0\n90.0\n\n\nFile 5\n98.99\n99.76\n80.31\n90.15\n90.0\n90.0\n\n\nFile 6\n97.04\n99.86\n70.62\n85.3\n90.0\n81.82\n\n\nFile 7\n98.18\n99.68\n77.81\n88.89\n100.0\n90.91\n\n\nFile 8\n96.0\n99.81\n57.37\n78.68\n70.0\n63.64\n\n\nFile 9\n96.81\n99.76\n56.62\n78.3\n80.0\n80.0\n\n\nFile 10\n97.57\n99.87\n75.99\n87.99\n90.0\n81.82\n\n\nAGGREGATE\n\n\n0\n97.44\n99.81\n70.28\n85.13\n85.0\n79.11\n\n\n\nSee benchmarks for how penalised group accuracy is calculated\n\n\n\n\n\n\n\n\n\n\nBase Benchmark with different grouping hyperparameters\nThe grouping function used for anomalous regions has two hyperparameters:\n\nmerge_tolerance\n\nacceptable gap between predicted anomalous regions for when a merge is appropriate\n\nnoise_tolerance\n\nminimum length of an anomalous region(end - start) for it to be classified as an anomalous region\n\n\nThe group accuracy is penalised if precision or recall is very high. Given each set has a recall of 100%, we can hyperparameter tune the model to maximize recall to get these two metrics closer together.\nGiven precision is 100% (no false positives) this indicates all true anomalies are deteced by the model. The problem is that there is false negatives in the model indicating multiple false indications of anomalous regions.\n\n\nHyperparameter Tuning for Penalised Group Accuracy\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom tqdm import tqdm\n\nparam_grid = {\n    \"w_smooth\": [31, 51, 71, 91, 111, 131, 151, 171],\n    \"w\": [41, 61, 81, 101, 121, 141, 161, 181],\n    \"iqr_threshold\": [1.5, 1.75, 2],\n    \"mad_threshold\": [2.2, 2.465, 2.7, 3],\n    \"normal_z_threshold\": [2.25, 2.5, 3],\n    \"rolling_z_threshold\": [2.0, 2.25, 2.5],\n    \"metric_consensus\": [3, 4],\n}\ncombined = pd.concat(test_files)\n\n\ndef calculate_estimated_tuning_time(params: dict, cv: int):\n    total_iterations = 1\n    for key, value in params.items():\n        total_iterations *= len(value)\n    print(total_iterations)\n    print(\"Estimated Tuning Time\", (total_iterations / 16) * cv, \"seconds\")\n\n\ncalculate_estimated_tuning_time(param_grid, 5)\n\nimport itertools\nimport numpy as np\n\n\ndef grid_search(model, X, y, param_grid):\n    best_score = -np.inf\n    best_params = {}\n    for combination in tqdm(itertools.product(*param_grid.values()), desc=\"Iterations\"):\n        params = dict(zip(param_grid.keys(), combination))\n        model.set_params(**params)\n        model.fit(X, y)\n        score = model.score(X, y)\n        if score &gt; best_score:\n            best_score = score\n            best_params = params\n    return best_params, best_score\n\n13824\nEstimated Tuning Time 4320.0 seconds\n\n\n\nimport os\n\ngrid = GridSearchCV(\n    StatsModel(),\n    param_grid=param_grid,\n    cv=5,\n    verbose=1,\n    n_jobs=-1,\n    scoring=\"balanced_accuracy\",\n)\n# grid.fit(combined['Value1'],combined[\"Labels\"])\n# os.system(\"ffplay ~/Music/notify.mp3\") # I use this to know when its done executing\n\n\n# grid.best_params_\n\n\nbest_recall_params = {\n    \"iqr_threshold\": 1.5,\n    \"mad_threshold\": 2.2,\n    \"metric_consensus\": 3,\n    \"normal_z_threshold\": 2.25,\n    \"rolling_z_threshold\": 2.0,\n    \"w\": 41,\n    \"w_smooth\": 31,\n}\n\n\nbest_balanced_acc_params = {\n    \"iqr_threshold\": 1.5,\n    \"mad_threshold\": 3,\n    \"metric_consensus\": 3,\n    \"normal_z_threshold\": 2.25,\n    \"rolling_z_threshold\": 2.0,\n    \"w\": 41,\n    \"w_smooth\": 31,\n}\n\nbest_recall_clf = StatsModel()\nbest_recall_clf.set_params(**best_recall_params)\n\nbenchmark_df = run_benchmark(best_recall_clf)\ncreate_benchmark_table(\n    benchmark_df,\n    model_name=\"Z_Score Model\",\n    subtitle=\"results **after hyperparameter tuning for recall**\",\n)\n\n\n=========================File 1==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 2==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 3==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 4==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 5==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 6==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 7==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 8==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 9==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 10==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Z_Score Model\n\n\nresults after hyperparameter tuning for recall\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n98.54\n99.87\n83.74\n91.87\n90.0\n81.82\n\n\nFile 2\n96.49\n99.72\n67.29\n83.63\n80.0\n80.0\n\n\nFile 3\n96.51\n99.77\n55.61\n77.8\n70.0\n63.64\n\n\nFile 4\n97.82\n99.83\n73.01\n86.5\n90.0\n90.0\n\n\nFile 5\n98.9\n99.5\n78.74\n89.36\n90.0\n81.82\n\n\nFile 6\n96.86\n99.86\n68.82\n84.41\n90.0\n81.82\n\n\nFile 7\n98.11\n99.68\n76.94\n88.46\n100.0\n90.91\n\n\nFile 8\n95.82\n99.62\n55.56\n77.77\n70.0\n70.0\n\n\nFile 9\n96.76\n99.52\n56.07\n78.02\n80.0\n80.0\n\n\nFile 10\n97.38\n99.87\n74.11\n87.05\n90.0\n81.82\n\n\nAGGREGATE\n\n\n0\n97.32\n99.72\n68.99\n84.49\n85.0\n80.18\n\n\n\nSee benchmarks for how penalised group accuracy is calculated\n\n\n\n\n\n\n\n\n\n\nbest_balacc_clf = StatsModel().set_params(**best_balanced_acc_params)\ncreate_benchmark_table(\n    run_benchmark(best_balacc_clf),\n    model_name=\"Z Score Model\",\n    subtitle=\"results **after hyperparameter tuning for balanced accuracy**\",\n)\n\n\n=========================File 1==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 2==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 3==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 4==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 5==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 6==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 7==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 8==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 9==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 10==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Z Score Model\n\n\nresults after hyperparameter tuning for balanced accuracy\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n98.55\n100.0\n83.74\n91.87\n90.0\n81.82\n\n\nFile 2\n96.51\n100.0\n67.29\n83.65\n80.0\n80.0\n\n\nFile 3\n96.52\n100.0\n55.61\n77.81\n70.0\n63.64\n\n\nFile 4\n97.83\n100.0\n73.01\n86.5\n90.0\n90.0\n\n\nFile 5\n98.91\n99.75\n78.74\n89.36\n90.0\n81.82\n\n\nFile 6\n96.87\n100.0\n68.82\n84.41\n90.0\n81.82\n\n\nFile 7\n98.12\n99.84\n76.94\n88.47\n100.0\n90.91\n\n\nFile 8\n95.83\n99.81\n55.56\n77.77\n70.0\n70.0\n\n\nFile 9\n96.77\n99.76\n56.07\n78.03\n80.0\n80.0\n\n\nFile 10\n97.39\n100.0\n74.11\n87.05\n90.0\n81.82\n\n\nAGGREGATE\n\n\n0\n97.33\n99.92\n68.99\n84.49\n85.0\n80.18\n\n\n\nSee benchmarks for how penalised group accuracy is calculated\n\n\n\n\n\n\n\n\n\n\nmodel = StatsModel(iqr_threshold=1.5, normal_z_threshold=3, rolling_z_threshold=3)\ncreate_benchmark_table(run_benchmark(model))\nX = test_files[1][\"Value1\"]\ny = test_files[1][\"Labels\"]\n\nmodel.fit(X)\ny_pred = model.predict(X)\n\npred_groups = np.array(Benchmarking.create_anomaly_groups(pd.Series(y_pred)))\ntrue_groups = np.array(Benchmarking.create_anomaly_groups(y))\n\nprint(\"Pred Groups\\n\", pred_groups)\nprint(\"Actual Groups\\n\", true_groups)\n\n\n=========================File 1==========================\n\n5 anomaly groups identified\n10 anomaly groups identified\nModel predicts 5 less than 10\nModel predicts 5 less than 10\n\n=========================File 2==========================\n\n8 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 less than 10\nModel predicts 2 less than 10\n\n=========================File 3==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 4==========================\n\n9 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 less than 10\nModel predicts 1 less than 10\n\n=========================File 5==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 6==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 7==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 8==========================\n\n12 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 more than 10\nModel predicts 2 more than 10\n\n=========================File 9==========================\n\n12 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 more than 10\nModel predicts 2 more than 10\n\n=========================File 10==========================\n\n7 anomaly groups identified\n10 anomaly groups identified\nModel predicts 3 less than 10\nModel predicts 3 less than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Stats Model\n\n\nresults before hyperparameter turning\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n93.48\n99.59\n27.02\n63.5\n30.0\n15.0\n\n\nFile 2\n93.86\n99.78\n42.55\n71.27\n60.0\n48.0\n\n\nFile 3\n96.1\n99.75\n50.38\n75.19\n70.0\n63.64\n\n\nFile 4\n95.96\n99.75\n49.88\n74.93\n70.0\n63.0\n\n\nFile 5\n98.8\n99.74\n76.57\n88.28\n90.0\n90.0\n\n\nFile 6\n93.91\n99.75\n39.44\n69.72\n70.0\n70.0\n\n\nFile 7\n95.87\n99.5\n49.32\n74.65\n80.0\n72.73\n\n\nFile 8\n95.17\n99.78\n48.5\n74.25\n60.0\n50.0\n\n\nFile 9\n96.47\n99.74\n51.98\n75.98\n80.0\n66.67\n\n\nFile 10\n94.71\n99.79\n47.62\n73.8\n50.0\n35.0\n\n\nAGGREGATE\n\n\n0\n95.43\n99.72\n48.33\n74.16\n66.0\n57.4\n\n\n\nSee benchmarks for how penalised group accuracy is calculated\n\n\n\n\n\n\n\n\n\n8 anomaly groups identified\n10 anomaly groups identified\nPred Groups\n [[2928 2979]\n [3834 3927]\n [4758 4879]\n [6850 6953]\n [7919 7955]\n [8379 8429]\n [9112 9221]\n [9403 9472]]\nActual Groups\n [[2206 2326]\n [2864 2979]\n [3483 3566]\n [3834 3927]\n [4758 4879]\n [6850 6958]\n [7852 7955]\n [8319 8429]\n [9112 9226]\n [9372 9472]]\n\n\n\nfor file in test_files:\n    model = StatsModel().set_params(**best_balanced_acc_params)\n    X = file[\"Value1\"]\n    y = file[\"Labels\"]\n    model.fit(X)\n    y_pred = model.predict(X)\n\n    pred_groups = Benchmarking.create_anomaly_groups(pd.Series(y))\n    true_groups = Benchmarking.create_anomaly_groups(y)\n\n    plt.figure(figsize=(14, 4))\n    plt.plot(X, color=\"black\", lw=1)\n    # for start,end in true_groups:\n    #  plt.axvspan(start, end, color='green',alpha=0.3)\n\n    for start, end in pred_groups:\n        plt.axvspan(start, end, color=\"red\", alpha=0.5)\n    plt.show()\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import LearningCurveDisplay\n\nLearningCurveDisplay.from_estimator(\n    StatsModel().set_params(**best_balanced_acc_params),\n    combined[\"Value1\"],\n    combined[\"Labels\"],\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nLearningCurveDisplay.from_estimator(\n    StatsModel().set_params(**best_recall_params),\n    combined[\"Value1\"],\n    combined[\"Labels\"],\n    cv=5,\n)\nplt.title(\"Learning Curve for Best Recall (cv=5)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import RocCurveDisplay\n\nmodel = StatsModel().set_params(**best_balanced_acc_params)\n\nTest = combined[-20000:]\nmodel.fit(combined[:80000][\"Value1\"])\ny_pred = model.predict(Test[\"Value1\"])\ny_true = Test[\"Labels\"]\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\n\nRocCurveDisplay.from_predictions(y_true, y_pred)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nPrecisionRecallDisplay.from_predictions(y_true, y_pred)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(combined['Value1'],\n                                                    combined['Labels'],\n                                                    stratify=combined[\"Labels\"],\n                                                    random_state=42)\n\nmodel = StatsModel().set_params(**best_balanced_acc_params)\nmodel.fit(X_train)\ny_pred = model.predict(X_test)\n\nmetrics = Benchmarking.print_evaluation(y_test, y_pred,'Train-Test split results')\n\n237 anomaly groups identified\n18 anomaly groups identified\nModel predicts 227 more than 10\nModel predicts 227 more than 10\n\n\n\n\n\n\n\n\nModel Results for Train-Test split results\n\n\nindex\nScore\n\n\n\n\nAccuracy\n97.45\n\n\nPrecision\n99.93\n\n\nRecall\n70.19\n\n\nBalanced Accuracy\n85.09\n\n\nGroup Accuracy\n0.0\n\n\nPenalised Group Accuracy\n0.0\n\n\n\nMetrics are in percentage(%)"
  },
  {
    "objectID": "posts/isolation_forest/index.html",
    "href": "posts/isolation_forest/index.html",
    "title": "Isolation Forest",
    "section": "",
    "text": "Read this - Do not add or remove Python libraries. Stick to the imports already present in this notebook. Changing libraries is an automatic −100%. - You may use machine learning, statistics, or a hybrid approach — but your method must generalize to new, unseen datasets. - Datasets: We have 10 time-series with 10 000 rows each; anomalies: 10 segments per dataset. You can upload the zip to you Google drive and use the ID from Google drive url. - Scoring in class: we will run your detector on novel datasets. #correct/10 × 100 is your percentage. - Over/under-fitting penalties may apply (−50%)."
  },
  {
    "objectID": "posts/isolation_forest/index.html#student-eda",
    "href": "posts/isolation_forest/index.html#student-eda",
    "title": "Isolation Forest",
    "section": "Student EDA",
    "text": "Student EDA\nUse this cell to explore the signal (e.g., plot, summary stats).\n\n# STUDENT EDA\ntry:\n    df = test_files[0]\n    print(df.head())\nexcept Exception as e:\n    print('EDA note: run the original data-loading cells first (the ones that populate train_files/test_files).')\n\n      Value1  Labels\n0  20.801402       0\n1  26.800208       0\n2  33.154527       0\n3  39.189824       0\n4  40.631321       0"
  },
  {
    "objectID": "posts/isolation_forest/index.html#explanation",
    "href": "posts/isolation_forest/index.html#explanation",
    "title": "Isolation Forest",
    "section": "Explanation",
    "text": "Explanation\nThis pipeline works on the idea that:\n    1) it builds upon sliding windows\n    2) gathers normalised anomaly scores from each sub-model and uses them\n    3) averages the anomaly scores\n    4) computes a binary mask by thresholding at the 1st percentile so that it can compare outputs\n    5) stores self.full_anomaly_mask (same length as the placeholder value)\n    6) returns the single index of the lowest‐score window center which closes the loop on the sliding window idea"
  },
  {
    "objectID": "posts/isolation_forest/index.html#student-todo-implement-your-anomaly-detector",
    "href": "posts/isolation_forest/index.html#student-todo-implement-your-anomaly-detector",
    "title": "Isolation Forest",
    "section": "STUDENT TODO — Implement your anomaly detector",
    "text": "STUDENT TODO — Implement your anomaly detector\nImplement Machine Learning/ Statistical models or both. Use the test_files (test series) to train your models and list of anomaly index range for example Anomaly 1: 2001-2005 Anomaly 2: 2010-2012\nConstraints\n\nKeep it efficient; we will run this over 10 datasets and additional novel datasets in class.\n\n#EDA on given model\n\n#Normalization\nscaler = StandardScaler()\n\n\n#Reshape\nrx = df['Value1'].values.reshape(-1,1)\n\nnp_scaled = scaler.fit_transform(rx)\ndata = pd.DataFrame(np_scaled)\n\n\ndata.head()\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n-1.222425\n\n\n1\n-0.824437\n\n\n2\n-0.402862\n\n\n3\n-0.002453\n\n\n4\n0.093183\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\ndef detect_peaks_anomalies(x, min_height=None, distance=20):\n    \"\"\"Peak detection for spike anomalies - WITH StandardScaler\"\"\"\n    from scipy.signal import find_peaks\n\n    # Normalize using StandardScaler\n    scaler = StandardScaler()\n    x_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\n\n    if min_height is None:\n        min_height = 3.0  # Now in standard deviation units\n\n    peaks, _ = find_peaks(x_normalized, height=min_height, distance=distance)\n    neg_peaks, _ = find_peaks(-x_normalized, height=min_height, distance=distance)\n\n    return list(peaks) + list(neg_peaks)\n\ndef detect_change_point_anomalies(x, window_size=50):\n    \"\"\"Detect anomalies based on distribution changes - WITH StandardScaler\"\"\"\n    anomalies = []\n\n    # Normalize using StandardScaler\n    scaler = StandardScaler()\n    x_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\n\n    for i in range(window_size, len(x_normalized) - window_size):\n        before_window = x_normalized[i-window_size:i]\n        after_window = x_normalized[i:i+window_size]\n\n        # Statistical test on standardized data\n        mean_diff = np.abs(np.mean(after_window) - np.mean(before_window))\n        std_before = np.std(before_window)\n\n        # Threshold in standard deviation units\n        if std_before &gt; 0 and mean_diff &gt; 3.0:  # 2 standard deviations\n            anomalies.append(i)\n\n    return anomalies\n\n\n# Implement your anomaly detector/ detectors. You can edit this or use your own\nimport numpy as np\n\ndef student_detect_anomalies(series: np.ndarray) -&gt; list:\n    \"\"\"\n    Input:\n        series: 1D array-like of floats (test series)\n    Output:\n        List of (start, end) index pairs (0-based, end exclusive) for anomaly ranges.\n    \"\"\"\n    x = np.asarray(series, dtype=float)\n    n = len(x)\n    if n == 0:\n        return []\n\n    # Rolling mean/std z-score on a smoothed series\n    # Smooth to get residuals\n    w_smooth = 51\n    k = np.ones(w_smooth) / w_smooth\n    smooth = np.convolve(x, k, mode='same')\n    resid = x - smooth\n\n    # 2) Rolling mean/std using convolution (no extra libs)\n    w = 61  # odd; students may tune\n    kw = np.ones(w) / w\n    mu = np.convolve(resid, kw, mode='same')\n    mu2 = np.convolve(resid*resid, kw, mode='same')\n    var = np.maximum(mu2 - mu*mu, 1e-8)\n    sigma = np.sqrt(var)\n    z = np.abs((resid - mu) / (sigma + 1e-8))\n\n\n\nprint('anomalies identified.')\n\nanomalies identified."
  },
  {
    "objectID": "posts/provided_model/index.html",
    "href": "posts/provided_model/index.html",
    "title": "Provided Model Tuning",
    "section": "",
    "text": "import pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain_file_names = os.listdir(\"train/\")\ntrain_file_names.sort()\n\ntrain_files = []\nfor file in train_file_names:\n    train_files.append(pd.read_csv(f\"train/{file}\", sep=\";\"))\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\nfor file in test_file_names:\n    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\ntest_files[0].head()\n\n\n\n\n\n\n\n\nValue1\nLabels\n\n\n\n\n0\n20.801402\n0\n\n\n1\n26.800208\n0\n\n\n2\n33.154527\n0\n\n\n3\n39.189824\n0\n\n\n4\n40.631321\n0"
  },
  {
    "objectID": "posts/provided_model/index.html#student-eda",
    "href": "posts/provided_model/index.html#student-eda",
    "title": "Provided Model Tuning",
    "section": "Student EDA",
    "text": "Student EDA\nUse this cell to explore the signal (e.g., plot, summary stats).\n\n# STUDENT EDA\ntry:\n    df = test_files[0]\n    print(df.head())\nexcept Exception as e:\n    print('EDA note: run the original data-loading cells first (the ones that populate train_files/test_files).')\n    df.info()\n    df.describe()\n\n    df.plot(x='time')\n\n      Value1  Labels\n0  20.801402       0\n1  26.800208       0\n2  33.154527       0\n3  39.189824       0\n4  40.631321       0"
  },
  {
    "objectID": "posts/provided_model/index.html#explanation",
    "href": "posts/provided_model/index.html#explanation",
    "title": "Provided Model Tuning",
    "section": "Explanation",
    "text": "Explanation\nThis pipeline works on the idea that:\n    1) it builds upon sliding windows\n    2) gathers normalised anomaly scores from each sub-model and uses them\n    3) averages the anomaly scores\n    4) computes a binary mask by thresholding at the 1st percentile so that it can compare outputs\n    5) stores self.full_anomaly_mask (same length as the placeholder value)\n    6) returns the single index of the lowest‐score window center which closes the loop on the sliding window idea"
  },
  {
    "objectID": "posts/provided_model/index.html#student-todo-implement-your-anomaly-detector",
    "href": "posts/provided_model/index.html#student-todo-implement-your-anomaly-detector",
    "title": "Provided Model Tuning",
    "section": "STUDENT TODO — Implement your anomaly detector",
    "text": "STUDENT TODO — Implement your anomaly detector\nImplement Machine Learning/ Statistical models or both. Use the test_files (test series) to train your models and list of anomaly index range for example Anomaly 1: 2001-2005 Anomaly 2: 2010-2012\nConstraints\n\nKeep it efficient; we will run this over 10 datasets and additional novel datasets in class."
  },
  {
    "objectID": "posts/z_score_model/index.html#looking-at-results-for-1000-splits",
    "href": "posts/z_score_model/index.html#looking-at-results-for-1000-splits",
    "title": "Z Score Classifier",
    "section": "Looking at results for 1000 splits",
    "text": "Looking at results for 1000 splits\n\nfrom sklearn.model_selection import train_test_split\n\nresults = pd.DataFrame()\nfor epoch in range(1, 1000):\n    model = StatsModel().set_params(**best_recall_params)\n    X_train, X_test, y_train, y_test = train_test_split(\n        combined[\"Value1\"],\n        combined[\"Labels\"],\n        stratify=combined[\"Labels\"],\n        random_state=epoch,\n    )\n    model.fit(X_train)\n    y_pred = model.predict(X_test)\n\n    epoch_bench = Benchmarking.evaluate_model(y_test, y_pred, show_printout=False)\n    epoch_bench = epoch_bench.transpose().reset_index(drop=True)\n    results = pd.concat([results, epoch_bench])\n \nresults = results.drop(columns=['Group Accuracy','Penalised Group Accuracy'])\nresults = results.reset_index(drop=True)\n\n\nplt.plot(results['Accuracy'],color='r',label='Accuracy')\nplt.plot(results['Precision'],color='blue',label='Precision')\nplt.plot(results['Recall'],color='green',label='Recall')\nplt.plot(results['Balanced Accuracy'],color='pink',label='Balanced Accuracy')\nplt.legend(loc='upper right')\nplt.title('Metrics for 1000 random splits')\nplt.xlabel('Random State')\nplt.ylabel('Metric')\nplt.show()\n\n\n\n\n\n\n\n\nFor 1000 train-test splits, the model has scores which don’t vary by a high degree.\nThis assumes that the data points are independent and we’re looking at this graph to determine if the scores obtained by previous models were just random chance or does it cause similar scores for different splits where we ignore having to determine groups."
  },
  {
    "objectID": "posts/z_score_model/index.html#using-the-provided-train-files",
    "href": "posts/z_score_model/index.html#using-the-provided-train-files",
    "title": "Z Score Classifier",
    "section": "Using the provided Train Files",
    "text": "Using the provided Train Files\n\nmodel = StatsModel()\n\nfor idx, train in enumerate(train_files):\n    model = StatsModel()\n    model.fit(train[\"Value1\"])\n    y_pred = model.predict(test_files[idx][\"Value1\"])\n\n    y_true = test_files[idx][\"Labels\"]\n    results = Benchmarking.evaluate_model(y_true, y_pred)\n    print(results)\n\n2 anomaly groups identified\n10 anomaly groups identified\nModel predicts 8 less than 10\nModel predicts 8 less than 10\n                          Score\nAccuracy                   8.78\nPrecision                  8.79\nRecall                    98.43\nBalanced Accuracy         49.22\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n6 anomaly groups identified\n10 anomaly groups identified\nModel predicts 4 less than 10\nModel predicts 4 less than 10\n                          Score\nAccuracy                   8.13\nPrecision                  8.34\nRecall                    76.19\nBalanced Accuracy         38.10\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n4 anomaly groups identified\n10 anomaly groups identified\nModel predicts 6 less than 10\nModel predicts 6 less than 10\n                          Score\nAccuracy                   6.98\nPrecision                  7.04\nRecall                    89.03\nBalanced Accuracy         44.52\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n4 anomaly groups identified\n10 anomaly groups identified\nModel predicts 6 less than 10\nModel predicts 6 less than 10\n                          Score\nAccuracy                   7.71\nPrecision                  7.74\nRecall                    95.90\nBalanced Accuracy         47.95\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n7 anomaly groups identified\n10 anomaly groups identified\nModel predicts 3 less than 10\nModel predicts 3 less than 10\n                          Score\nAccuracy                   4.03\nPrecision                  4.07\nRecall                    79.33\nBalanced Accuracy         39.67\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n8 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 less than 10\nModel predicts 2 less than 10\n                          Score\nAccuracy                   8.34\nPrecision                  8.48\nRecall                    83.07\nBalanced Accuracy         41.53\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n7 anomaly groups identified\n10 anomaly groups identified\nModel predicts 3 less than 10\nModel predicts 3 less than 10\n                          Score\nAccuracy                   6.09\nPrecision                  6.22\nRecall                    75.09\nBalanced Accuracy         37.55\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n9 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 less than 10\nModel predicts 1 less than 10\n                          Score\nAccuracy                   7.26\nPrecision                  7.42\nRecall                    77.56\nBalanced Accuracy         38.78\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n1 anomaly groups identified\n10 anomaly groups identified\nModel predicts 9 less than 10\nModel predicts 9 less than 10\n                          Score\nAccuracy                   7.32\nPrecision                  7.32\nRecall                    99.86\nBalanced Accuracy         49.93\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n2 anomaly groups identified\n10 anomaly groups identified\nModel predicts 8 less than 10\nModel predicts 8 less than 10\n                          Score\nAccuracy                   9.63\nPrecision                  9.67\nRecall                    95.54\nBalanced Accuracy         47.77\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n\n\nThere is no anomalies in these files and is not normally distributed meaning it will most likely perform very poorly.\n\n\n\n\n\n\nImportant\n\n\n\nThe data must be normally distributed\n\n\n\ndef run_benchmark(model, show_printout=True):\n    benchmark_df = pd.DataFrame()\n    file_no = 1\n    for idx, df in enumerate(test_files):\n        if show_printout:\n            print(\n                f\"\\n=========================File {file_no}==========================\\n\"\n            )\n        X_train = train_files[idx][\"Value1\"]\n        y_train = train_files[idx][\"Labels\"]\n        X_test = df[\"Value1\"]\n        y_true = df[\"Labels\"]\n        model.fit(X_train)\n\n        y_pred = model.predict(X_test)\n        results = Benchmarking.evaluate_model(\n            y_true, y_pred, show_printout=show_printout\n        )\n        results = results.rename(columns={\"Score\": f\"File {file_no}\"})\n        benchmark_df = pd.concat([benchmark_df, results], axis=1)\n        file_no += 1\n\n    return benchmark_df\n\n\nmodel = StatsModel().set_params(**best_recall_params)\n\ncreate_benchmark_table(run_benchmark(model))\n\n\n=========================File 1==========================\n\n2 anomaly groups identified\n10 anomaly groups identified\nModel predicts 8 less than 10\nModel predicts 8 less than 10\n\n=========================File 2==========================\n\n6 anomaly groups identified\n10 anomaly groups identified\nModel predicts 4 less than 10\nModel predicts 4 less than 10\n\n=========================File 3==========================\n\n4 anomaly groups identified\n10 anomaly groups identified\nModel predicts 6 less than 10\nModel predicts 6 less than 10\n\n=========================File 4==========================\n\n4 anomaly groups identified\n10 anomaly groups identified\nModel predicts 6 less than 10\nModel predicts 6 less than 10\n\n=========================File 5==========================\n\n7 anomaly groups identified\n10 anomaly groups identified\nModel predicts 3 less than 10\nModel predicts 3 less than 10\n\n=========================File 6==========================\n\n8 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 less than 10\nModel predicts 2 less than 10\n\n=========================File 7==========================\n\n7 anomaly groups identified\n10 anomaly groups identified\nModel predicts 3 less than 10\nModel predicts 3 less than 10\n\n=========================File 8==========================\n\n9 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 less than 10\nModel predicts 1 less than 10\n\n=========================File 9==========================\n\n1 anomaly groups identified\n10 anomaly groups identified\nModel predicts 9 less than 10\nModel predicts 9 less than 10\n\n=========================File 10==========================\n\n2 anomaly groups identified\n10 anomaly groups identified\nModel predicts 8 less than 10\nModel predicts 8 less than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Stats Model\n\n\nresults before hyperparameter turning\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n8.77\n8.78\n98.32\n49.16\n0.0\n0.0\n\n\nFile 2\n7.95\n8.17\n74.51\n37.25\n0.0\n0.0\n\n\nFile 3\n6.98\n7.04\n89.03\n44.52\n0.0\n0.0\n\n\nFile 4\n7.71\n7.74\n95.9\n47.95\n0.0\n0.0\n\n\nFile 5\n4.03\n4.07\n79.33\n39.67\n0.0\n0.0\n\n\nFile 6\n8.34\n8.48\n83.07\n41.53\n0.0\n0.0\n\n\nFile 7\n6.09\n6.22\n75.09\n37.55\n0.0\n0.0\n\n\nFile 8\n7.15\n7.31\n76.39\n38.19\n0.0\n0.0\n\n\nFile 9\n7.32\n7.32\n99.86\n49.93\n0.0\n0.0\n\n\nFile 10\n9.63\n9.67\n95.54\n47.77\n0.0\n0.0\n\n\nAGGREGATE\n\n\n0\n7.4\n7.48\n86.7\n43.35\n0.0\n0.0\n\n\n\nSee benchmarks for how penalised group accuracy is calculated"
  }
]