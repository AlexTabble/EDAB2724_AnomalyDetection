[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models",
    "section": "",
    "text": "Z Score Classifier\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIsolation Forest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 9, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 6, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/z_score_model/index.html",
    "href": "posts/z_score_model/index.html",
    "title": "Z Score Classifier",
    "section": "",
    "text": "import pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom great_tables import GT, md\n\nfrom scipy.signal import detrend\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\n#for file in test_file_names:\n#    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\n\n#NOTE: I'm changing the way the data is imported by preprocessing\n# it while being imported. In the official colab, this should be done separately\nfor file in test_file_names:\n    data = pd.read_csv(f\"test/{file}\", sep=\";\")\n    data['Value1'] = detrend(data['Value1'])\n    test_files.append(data)\n\n\ntrain_file_names = os.listdir('train/')\ntrain_files = []\nfor train in train_file_names:\n  train_files.append(pd.read_csv(f'train/{file}',sep=';'))\n\nThe test data is normally distributed meaning normal outlier detection techniques like z scores and IQR is applicable.\n\n\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nclass StatsModel(BaseEstimator, ClassifierMixin):\n  \"\"\"\n  Ensemble stats outlier classifier using:\n  - IQR\n  - Normal z_scores\n  - Rolling z_scores\n  - Mean Absolute Deviation\n  \"\"\"\n  def __init__(self, w_smooth=51, w = 61,\n               iqr_threshold = 1.5,\n               mad_threshold = 2.465,\n               normal_z_threshold = 2,\n               rolling_z_threshold = 2,\n               metric_consensus =3):\n    \n    self.w = w\n    self.w_smooth = w_smooth\n    self.iqr_threshold = iqr_threshold\n    self.mad_threshold = mad_threshold\n    self.normal_z_threshold = normal_z_threshold\n    self.rolling_z_threshold = rolling_z_threshold\n    self.metric_consensus = metric_consensus\n\n\n  def _determine_rolling_z_mask(self, X :np.array) -&gt; np.array:\n    \n    k = np.ones(self.w_smooth) / self.w_smooth\n    smooth = np.convolve(X, k, mode='same') \n\n    # I'm quite sure this step makes it less applicable for machine learning so\n    # its not used in fit\n    resid = X- smooth\n    kw = np.ones(self.w) / self.w\n    mu = np.convolve(resid, kw, mode='same')\n    mu2 = np.convolve(resid * resid, kw, mode='same')\n    var = np.maximum(mu2 - mu*mu, 1e-08)\n    sigma= np.sqrt(var)\n\n    z_rolling = np.abs((resid - mu) / (sigma + 1e-08))\n    return np.abs(z_rolling &gt; self.rolling_z_threshold)\n\n  def fit(self, X : np.array, y=None) -&gt; None:\n    self.iqr_ = np.quantile(X,0.75) - np.quantile(X,0.25)\n    self.mean_ = np.mean(X)\n    self.std_ = np.std(X)\n    self.median_ = np.median(X)\n    self.mad_ = np.sum(np.abs(X - self.mean_)) / len(X)\n\n    self.iqr_ub_ = self.mean_ + self.iqr_threshold * self.iqr_\n    self.iqr_lb_ = self.mean_ - self.iqr_threshold * self.iqr_ \n    return self\n    \n  def predict(self, X = None) -&gt; np.array:\n    \n    IQR_mask = (X &gt; self.iqr_ub_) | (X &lt; self.iqr_lb_)\n    MAD_mask = np.abs(X - self.median_) &gt; self.mad_ * self.mad_threshold\n    Normal_Z_Mask = np.abs((X - self.mean_) / self.std_) &gt; self.normal_z_threshold\n    Rolling_Z_Mask = self._determine_rolling_z_mask(X)\n    \n    mask_df = pd.DataFrame({\n      'IQR' : IQR_mask,\n      \"MAD\" : MAD_mask,\n      'Normal_z' : Normal_Z_Mask,\n      'Rolling_z' : Rolling_Z_Mask \n    })\n\n    mask_df['outlier'] = mask_df.sum(axis=1) &gt;= self.metric_consensus\n    return mask_df['outlier'].to_numpy()\n\n  def score(self, X , y_true) -&gt; float:\n    y_pred = self.predict(X)\n    \n    return recall_score(y_true, y_pred)"
  },
  {
    "objectID": "posts/z_score_model/index.html#preparing-data-and-motivation",
    "href": "posts/z_score_model/index.html#preparing-data-and-motivation",
    "title": "Z Score Classifier",
    "section": "",
    "text": "import pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom great_tables import GT, md\n\nfrom scipy.signal import detrend\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\n#for file in test_file_names:\n#    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\n\n#NOTE: I'm changing the way the data is imported by preprocessing\n# it while being imported. In the official colab, this should be done separately\nfor file in test_file_names:\n    data = pd.read_csv(f\"test/{file}\", sep=\";\")\n    data['Value1'] = detrend(data['Value1'])\n    test_files.append(data)\n\n\ntrain_file_names = os.listdir('train/')\ntrain_files = []\nfor train in train_file_names:\n  train_files.append(pd.read_csv(f'train/{file}',sep=';'))\n\nThe test data is normally distributed meaning normal outlier detection techniques like z scores and IQR is applicable.\n\n\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nclass StatsModel(BaseEstimator, ClassifierMixin):\n  \"\"\"\n  Ensemble stats outlier classifier using:\n  - IQR\n  - Normal z_scores\n  - Rolling z_scores\n  - Mean Absolute Deviation\n  \"\"\"\n  def __init__(self, w_smooth=51, w = 61,\n               iqr_threshold = 1.5,\n               mad_threshold = 2.465,\n               normal_z_threshold = 2,\n               rolling_z_threshold = 2,\n               metric_consensus =3):\n    \n    self.w = w\n    self.w_smooth = w_smooth\n    self.iqr_threshold = iqr_threshold\n    self.mad_threshold = mad_threshold\n    self.normal_z_threshold = normal_z_threshold\n    self.rolling_z_threshold = rolling_z_threshold\n    self.metric_consensus = metric_consensus\n\n\n  def _determine_rolling_z_mask(self, X :np.array) -&gt; np.array:\n    \n    k = np.ones(self.w_smooth) / self.w_smooth\n    smooth = np.convolve(X, k, mode='same') \n\n    # I'm quite sure this step makes it less applicable for machine learning so\n    # its not used in fit\n    resid = X- smooth\n    kw = np.ones(self.w) / self.w\n    mu = np.convolve(resid, kw, mode='same')\n    mu2 = np.convolve(resid * resid, kw, mode='same')\n    var = np.maximum(mu2 - mu*mu, 1e-08)\n    sigma= np.sqrt(var)\n\n    z_rolling = np.abs((resid - mu) / (sigma + 1e-08))\n    return np.abs(z_rolling &gt; self.rolling_z_threshold)\n\n  def fit(self, X : np.array, y=None) -&gt; None:\n    self.iqr_ = np.quantile(X,0.75) - np.quantile(X,0.25)\n    self.mean_ = np.mean(X)\n    self.std_ = np.std(X)\n    self.median_ = np.median(X)\n    self.mad_ = np.sum(np.abs(X - self.mean_)) / len(X)\n\n    self.iqr_ub_ = self.mean_ + self.iqr_threshold * self.iqr_\n    self.iqr_lb_ = self.mean_ - self.iqr_threshold * self.iqr_ \n    return self\n    \n  def predict(self, X = None) -&gt; np.array:\n    \n    IQR_mask = (X &gt; self.iqr_ub_) | (X &lt; self.iqr_lb_)\n    MAD_mask = np.abs(X - self.median_) &gt; self.mad_ * self.mad_threshold\n    Normal_Z_Mask = np.abs((X - self.mean_) / self.std_) &gt; self.normal_z_threshold\n    Rolling_Z_Mask = self._determine_rolling_z_mask(X)\n    \n    mask_df = pd.DataFrame({\n      'IQR' : IQR_mask,\n      \"MAD\" : MAD_mask,\n      'Normal_z' : Normal_Z_Mask,\n      'Rolling_z' : Rolling_Z_Mask \n    })\n\n    mask_df['outlier'] = mask_df.sum(axis=1) &gt;= self.metric_consensus\n    return mask_df['outlier'].to_numpy()\n\n  def score(self, X , y_true) -&gt; float:\n    y_pred = self.predict(X)\n    \n    return recall_score(y_true, y_pred)"
  },
  {
    "objectID": "posts/z_score_model/index.html#helper-functions",
    "href": "posts/z_score_model/index.html#helper-functions",
    "title": "Z Score Classifier",
    "section": "Helper Functions",
    "text": "Helper Functions\n\nfrom benchmarking import Benchmarking\nimport time\n\n\ndef run_benchmark(model,show_printout=True):\n  benchmark_df = pd.DataFrame()\n  file_no = 1\n  for df in test_files:\n    if show_printout:\n      print(f'\\n=========================File {file_no}==========================\\n')\n    model = model\n    X = df['Value1']\n    y_true = df['Labels']\n    model.fit(X)\n    y_pred = model.predict(X)\n    results = Benchmarking.evaluate_model(y_true, y_pred,show_printout=show_printout)\n    results = results.rename(columns={'Score' : f'File {file_no}'})\n    benchmark_df = pd.concat([benchmark_df, results],axis=1)                         \n    file_no += 1\n\n  return benchmark_df\n\ndef create_benchmark_table(benchmark_df: pd.DataFrame,\n                           model_name :str = \"Stats Model\" ,\n                           subtitle: str = \"results **before hyperparameter turning**\") -&gt; None:\n  \n  totals = benchmark_df.sum(axis=1) / 10\n  totals = totals.apply(lambda x: round(x,2))\n\n  totals = totals.to_frame().transpose()\n  totals['group'] = 'AGGREGATE'\n  benchmark_df = benchmark_df.transpose()\n  benchmark_df['group'] = 'File'\n  benchmark_df = pd.concat([benchmark_df, totals])\n\n  benchmark_df = benchmark_df.reset_index().rename(columns={'index': 'File'})\n  benchmark_df\n  (\n    GT(benchmark_df)\n    .tab_header(title=md(f'Benchmark Results: **{model_name}**'),\n                subtitle=md(subtitle))\n    .tab_source_note(md('See benchmarks for how penalised group accuracy is calculated'))\n    .tab_stub(rowname_col='File',groupname_col='group')\n  ).show()"
  },
  {
    "objectID": "posts/z_score_model/index.html#benchmarking-time-complexity",
    "href": "posts/z_score_model/index.html#benchmarking-time-complexity",
    "title": "Z Score Classifier",
    "section": "Benchmarking Time complexity",
    "text": "Benchmarking Time complexity\n\nstart = time.perf_counter()\nms = []\nfor epoch in range(1,1000):\n  start = time.perf_counter()\n  model = StatsModel()\n  run_benchmark(model,show_printout=False)\n  end = time.perf_counter()\n  ms.append((end-start))\nend = time.perf_counter()\nprint('Time for 1000 epochs', end-start)\n\nTime for 1000 epochs 0.1343959019995964\n\n\n\nplt.plot(ms)\nplt.title('Total execution time for 10 Files')\nplt.xlabel('Epoch')\nplt.ylabel('Execution Time(in seconds)')\nplt.show()\n\n\n\n\n\n\n\n\nThe execution time varies from 24 ms to 300 ms meaning we can tune this model for thousands of iterations to determine the optimal parameters"
  },
  {
    "objectID": "posts/z_score_model/index.html#base-benchmark",
    "href": "posts/z_score_model/index.html#base-benchmark",
    "title": "Z Score Classifier",
    "section": "Base Benchmark",
    "text": "Base Benchmark\n\nbenchmark_df = run_benchmark(StatsModel())\ncreate_benchmark_table(benchmark_df)\n\n\n=========================File 1==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 2==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 3==========================\n\n12 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 more than 10\nModel predicts 2 more than 10\n\n=========================File 4==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 5==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 6==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 7==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 8==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 9==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 10==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Stats Model\n\n\nresults before hyperparameter turning\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n98.63\n99.87\n84.75\n92.37\n90.0\n81.82\n\n\nFile 2\n96.66\n99.86\n68.79\n84.39\n80.0\n72.73\n\n\nFile 3\n96.64\n99.78\n57.27\n78.63\n70.0\n58.33\n\n\nFile 4\n97.84\n99.83\n73.26\n86.62\n90.0\n90.0\n\n\nFile 5\n98.99\n99.76\n80.31\n90.15\n90.0\n90.0\n\n\nFile 6\n97.04\n99.86\n70.62\n85.3\n90.0\n81.82\n\n\nFile 7\n98.18\n99.68\n77.81\n88.89\n100.0\n90.91\n\n\nFile 8\n96.0\n99.81\n57.37\n78.68\n70.0\n63.64\n\n\nFile 9\n96.81\n99.76\n56.62\n78.3\n80.0\n80.0\n\n\nFile 10\n97.57\n99.87\n75.99\n87.99\n90.0\n81.82\n\n\nAGGREGATE\n\n\n0\n97.44\n99.81\n70.28\n85.13\n85.0\n79.11\n\n\n\nSee benchmarks for how penalised group accuracy is calculated\n\n\n\n\n\n\n\n\n\n\nBase Benchmark with different grouping hyperparameters\nThe grouping function used for anomalous regions has two hyperparameters:\n\nmerge_tolerance\n\nacceptable gap between predicted anomalous regions for when a merge is appropriate\n\nnoise_tolerance\n\nminimum length of an anomalous region(end - start) for it to be classified as an anomalous region\n\n\nThe group accuracy is penalised if precision or recall is very high. Given each set has a recall of 100%, we can hyperparameter tune the model to maximize recall to get these two metrics closer together.\nGiven precision is 100% (no false positives) this indicates all true anomalies are deteced by the model. The problem is that there is false negatives in the model indicating multiple false indications of anomalous regions.\n\n\nHyperparameter Tuning for Penalised Group Accuracy\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom tqdm import tqdm\n\nparam_grid = {\n    \"w_smooth\": [31, 51, 71, 91, 111, 131, 151, 171],          \n    \"w\": [41, 61, 81, 101, 121, 141, 161, 181],                \n    \"iqr_threshold\": [1.5, 1.75, 2],                   \n    \"mad_threshold\": [ 2.2, 2.465, 2.7, 3],                  \n    \"normal_z_threshold\": [2.25, 2.5, 3],         \n    \"rolling_z_threshold\": [2.0, 2.25, 2.5],\n    \"metric_consensus\": [3, 4]                          \n}\ncombined = pd.concat(test_files)\n\ndef calculate_estimated_tuning_time(params: dict, cv :int):\n  total_iterations = 1\n  for key, value in params.items():\n    total_iterations *= len(value)\n  print(total_iterations)\n  print('Estimated Tuning Time', (total_iterations / 16)*cv, 'seconds')\n\ncalculate_estimated_tuning_time(param_grid,5)\n\nimport itertools\nimport numpy as np\n\ndef grid_search(model, X, y, param_grid):\n    best_score = -np.inf\n    best_params = {}\n    for combination in tqdm(itertools.product(*param_grid.values()),desc='Iterations'):\n        params = dict(zip(param_grid.keys(), combination))\n        model.set_params(**params)\n        model.fit(X, y)\n        score = model.score(X, y)\n        if score &gt; best_score:\n            best_score = score\n            best_params = params\n    return best_params, best_score\n\n13824\nEstimated Tuning Time 4320.0 seconds\n\n\n\nimport os\ngrid = GridSearchCV(StatsModel(),\n                    param_grid=param_grid,\n                    cv=5,\n                    verbose=1,\n                    n_jobs=-1,\n                    scoring='balanced_accuracy')\n#grid.fit(combined['Value1'],combined[\"Labels\"])\n#os.system(\"ffplay ~/Music/notify.mp3\") # I use this to know when its done executing\n\n\n#grid.best_params_\n\n\nbest_recall_params = {'iqr_threshold': 1.5,\n                      'mad_threshold': 2.2,\n                      'metric_consensus': 3,\n                      'normal_z_threshold': 2.25,\n                      'rolling_z_threshold': 2.0,\n                      'w': 41,\n                      'w_smooth': 31}\n\n\nbest_balanced_acc_params = {'iqr_threshold': 1.5,\n                            'mad_threshold': 3,\n                            'metric_consensus': 3,\n                            'normal_z_threshold': 2.25,\n                            'rolling_z_threshold': 2.0,\n                            'w': 41,\n                            'w_smooth': 31}\n\nbest_recall_clf = StatsModel()\nbest_recall_clf.set_params(**best_recall_params)\n\nbenchmark_df = run_benchmark(best_recall_clf)\ncreate_benchmark_table(benchmark_df,model_name='Z_Score Model',\n                       subtitle=\"results **after hyperparameter tuning for recall**\")\n\n\n=========================File 1==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 2==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 3==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 4==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 5==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 6==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 7==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 8==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 9==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 10==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Z_Score Model\n\n\nresults after hyperparameter tuning for recall\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n98.54\n99.87\n83.74\n91.87\n90.0\n81.82\n\n\nFile 2\n96.49\n99.72\n67.29\n83.63\n80.0\n80.0\n\n\nFile 3\n96.51\n99.77\n55.61\n77.8\n70.0\n63.64\n\n\nFile 4\n97.82\n99.83\n73.01\n86.5\n90.0\n90.0\n\n\nFile 5\n98.9\n99.5\n78.74\n89.36\n90.0\n81.82\n\n\nFile 6\n96.86\n99.86\n68.82\n84.41\n90.0\n81.82\n\n\nFile 7\n98.11\n99.68\n76.94\n88.46\n100.0\n90.91\n\n\nFile 8\n95.82\n99.62\n55.56\n77.77\n70.0\n70.0\n\n\nFile 9\n96.76\n99.52\n56.07\n78.02\n80.0\n80.0\n\n\nFile 10\n97.38\n99.87\n74.11\n87.05\n90.0\n81.82\n\n\nAGGREGATE\n\n\n0\n97.32\n99.72\n68.99\n84.49\n85.0\n80.18\n\n\n\nSee benchmarks for how penalised group accuracy is calculated\n\n\n\n\n\n\n\n\n\n\nbest_balacc_clf = StatsModel().set_params(**best_balanced_acc_params)\ncreate_benchmark_table(run_benchmark(best_balacc_clf),model_name='Z Score Model',\n                       subtitle=\"results **after hyperparameter tuning for balanced accuracy**\")\n\n\n=========================File 1==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 2==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 3==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 4==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 5==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 6==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 7==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 8==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 9==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 10==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Z Score Model\n\n\nresults after hyperparameter tuning for balanced accuracy\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n98.55\n100.0\n83.74\n91.87\n90.0\n81.82\n\n\nFile 2\n96.51\n100.0\n67.29\n83.65\n80.0\n80.0\n\n\nFile 3\n96.52\n100.0\n55.61\n77.81\n70.0\n63.64\n\n\nFile 4\n97.83\n100.0\n73.01\n86.5\n90.0\n90.0\n\n\nFile 5\n98.91\n99.75\n78.74\n89.36\n90.0\n81.82\n\n\nFile 6\n96.87\n100.0\n68.82\n84.41\n90.0\n81.82\n\n\nFile 7\n98.12\n99.84\n76.94\n88.47\n100.0\n90.91\n\n\nFile 8\n95.83\n99.81\n55.56\n77.77\n70.0\n70.0\n\n\nFile 9\n96.77\n99.76\n56.07\n78.03\n80.0\n80.0\n\n\nFile 10\n97.39\n100.0\n74.11\n87.05\n90.0\n81.82\n\n\nAGGREGATE\n\n\n0\n97.33\n99.92\n68.99\n84.49\n85.0\n80.18\n\n\n\nSee benchmarks for how penalised group accuracy is calculated\n\n\n\n\n\n\n\n\n\n\nmodel = StatsModel(iqr_threshold=1.5,\n                   normal_z_threshold=3,\n                   rolling_z_threshold=3)\ncreate_benchmark_table(run_benchmark(model))\n\nX = test_files[1]['Value1']\ny = test_files[1]['Labels']\n\nmodel.fit(X)\ny_pred = model.predict(X)\n\npred_groups = np.array(Benchmarking.create_anomaly_groups(pd.Series(y_pred)))\ntrue_groups = np.array(Benchmarking.create_anomaly_groups(y))\n\nprint('Pred Groups\\n',pred_groups)\nprint('Actual Groups\\n',true_groups)\n\n\n=========================File 1==========================\n\n5 anomaly groups identified\n10 anomaly groups identified\nModel predicts 5 less than 10\nModel predicts 5 less than 10\n\n=========================File 2==========================\n\n8 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 less than 10\nModel predicts 2 less than 10\n\n=========================File 3==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 4==========================\n\n9 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 less than 10\nModel predicts 1 less than 10\n\n=========================File 5==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 6==========================\n\n10 anomaly groups identified\n10 anomaly groups identified\nNumber of groups match!\nNumber of groups match!\n\n=========================File 7==========================\n\n11 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 more than 10\nModel predicts 1 more than 10\n\n=========================File 8==========================\n\n12 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 more than 10\nModel predicts 2 more than 10\n\n=========================File 9==========================\n\n12 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 more than 10\nModel predicts 2 more than 10\n\n=========================File 10==========================\n\n7 anomaly groups identified\n10 anomaly groups identified\nModel predicts 3 less than 10\nModel predicts 3 less than 10\n\n\n\n\n\n\n\n\nBenchmark Results: Stats Model\n\n\nresults before hyperparameter turning\n\n\n\nAccuracy\nPrecision\nRecall\nBalanced Accuracy\nGroup Accuracy\nPenalised Group Accuracy\n\n\n\n\nFile\n\n\nFile 1\n93.48\n99.59\n27.02\n63.5\n30.0\n15.0\n\n\nFile 2\n93.86\n99.78\n42.55\n71.27\n60.0\n48.0\n\n\nFile 3\n96.1\n99.75\n50.38\n75.19\n70.0\n63.64\n\n\nFile 4\n95.96\n99.75\n49.88\n74.93\n70.0\n63.0\n\n\nFile 5\n98.8\n99.74\n76.57\n88.28\n90.0\n90.0\n\n\nFile 6\n93.91\n99.75\n39.44\n69.72\n70.0\n70.0\n\n\nFile 7\n95.87\n99.5\n49.32\n74.65\n80.0\n72.73\n\n\nFile 8\n95.17\n99.78\n48.5\n74.25\n60.0\n50.0\n\n\nFile 9\n96.47\n99.74\n51.98\n75.98\n80.0\n66.67\n\n\nFile 10\n94.71\n99.79\n47.62\n73.8\n50.0\n35.0\n\n\nAGGREGATE\n\n\n0\n95.43\n99.72\n48.33\n74.16\n66.0\n57.4\n\n\n\nSee benchmarks for how penalised group accuracy is calculated\n\n\n\n\n\n\n\n\n\n8 anomaly groups identified\n10 anomaly groups identified\nPred Groups\n [[2928 2979]\n [3834 3927]\n [4758 4879]\n [6850 6953]\n [7919 7955]\n [8379 8429]\n [9112 9221]\n [9403 9472]]\nActual Groups\n [[2206 2326]\n [2864 2979]\n [3483 3566]\n [3834 3927]\n [4758 4879]\n [6850 6958]\n [7852 7955]\n [8319 8429]\n [9112 9226]\n [9372 9472]]\n\n\n\nfor file in test_files:\n  model = StatsModel().set_params(**best_balanced_acc_params)\n  X = file['Value1']\n  y = file['Labels']\n  model.fit(X)\n  y_pred = model.predict(X)\n\n  pred_groups = Benchmarking.create_anomaly_groups(pd.Series(y))\n  true_groups = Benchmarking.create_anomaly_groups(y)\n\n\n  plt.figure(figsize=(14,4))\n  plt.plot(X, color='black',lw=1)\n  #for start,end in true_groups:\n  #  plt.axvspan(start, end, color='green',alpha=0.3)\n\n  for start,end in pred_groups:\n    plt.axvspan(start,end, color='red',alpha=0.5)\n  plt.show()\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n10 anomaly groups identified\n10 anomaly groups identified\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import LearningCurveDisplay\n\nLearningCurveDisplay.from_estimator(\n  StatsModel().set_params(**best_balanced_acc_params),\n  combined['Value1'],\n  combined['Labels']\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nLearningCurveDisplay.from_estimator(\n  StatsModel().set_params(**best_recall_params),\n  combined['Value1'],\n  combined['Labels']\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import RocCurveDisplay\n\nmodel = StatsModel().set_params(**best_balanced_acc_params)\n\nTest = combined[-20000:]\nmodel.fit(combined[:80000]['Value1'])\ny_pred = model.predict(Test['Value1'])\ny_true = Test[\"Labels\"]\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\n\nRocCurveDisplay.from_predictions(y_true, y_pred)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nPrecisionRecallDisplay.from_predictions(y_true, y_pred)\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel = StatsModel()\n\nfor idx, train in enumerate(train_files):\n  model = StatsModel()\n  model.fit(train['Value1'])\n  y_pred = model.predict(test_files[idx]['Value1'])\n\n  y_true= test_files[idx]['Labels']\n  results = Benchmarking.evaluate_model(y_true, y_pred)\n  print(results)\n\n2 anomaly groups identified\n10 anomaly groups identified\nModel predicts 8 less than 10\nModel predicts 8 less than 10\n                          Score\nAccuracy                   8.78\nPrecision                  8.79\nRecall                    98.43\nBalanced Accuracy         49.22\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n6 anomaly groups identified\n10 anomaly groups identified\nModel predicts 4 less than 10\nModel predicts 4 less than 10\n                          Score\nAccuracy                   8.13\nPrecision                  8.34\nRecall                    76.19\nBalanced Accuracy         38.10\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n4 anomaly groups identified\n10 anomaly groups identified\nModel predicts 6 less than 10\nModel predicts 6 less than 10\n                          Score\nAccuracy                   6.98\nPrecision                  7.04\nRecall                    89.03\nBalanced Accuracy         44.52\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n4 anomaly groups identified\n10 anomaly groups identified\nModel predicts 6 less than 10\nModel predicts 6 less than 10\n                          Score\nAccuracy                   7.71\nPrecision                  7.74\nRecall                    95.90\nBalanced Accuracy         47.95\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n7 anomaly groups identified\n10 anomaly groups identified\nModel predicts 3 less than 10\nModel predicts 3 less than 10\n                          Score\nAccuracy                   4.03\nPrecision                  4.07\nRecall                    79.33\nBalanced Accuracy         39.67\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n8 anomaly groups identified\n10 anomaly groups identified\nModel predicts 2 less than 10\nModel predicts 2 less than 10\n                          Score\nAccuracy                   8.34\nPrecision                  8.48\nRecall                    83.07\nBalanced Accuracy         41.53\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n7 anomaly groups identified\n10 anomaly groups identified\nModel predicts 3 less than 10\nModel predicts 3 less than 10\n                          Score\nAccuracy                   6.09\nPrecision                  6.22\nRecall                    75.09\nBalanced Accuracy         37.55\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n9 anomaly groups identified\n10 anomaly groups identified\nModel predicts 1 less than 10\nModel predicts 1 less than 10\n                          Score\nAccuracy                   7.26\nPrecision                  7.42\nRecall                    77.56\nBalanced Accuracy         38.78\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n1 anomaly groups identified\n10 anomaly groups identified\nModel predicts 9 less than 10\nModel predicts 9 less than 10\n                          Score\nAccuracy                   7.32\nPrecision                  7.32\nRecall                    99.86\nBalanced Accuracy         49.93\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00\n2 anomaly groups identified\n10 anomaly groups identified\nModel predicts 8 less than 10\nModel predicts 8 less than 10\n                          Score\nAccuracy                   9.63\nPrecision                  9.67\nRecall                    95.54\nBalanced Accuracy         47.77\nGroup Accuracy             0.00\nPenalised Group Accuracy   0.00"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/isolation_forest/index.html",
    "href": "posts/isolation_forest/index.html",
    "title": "Isolation Forest",
    "section": "",
    "text": "Read this - Do not add or remove Python libraries. Stick to the imports already present in this notebook. Changing libraries is an automatic −100%. - You may use machine learning, statistics, or a hybrid approach — but your method must generalize to new, unseen datasets. - Datasets: We have 10 time-series with 10 000 rows each; anomalies: 10 segments per dataset. You can upload the zip to you Google drive and use the ID from Google drive url. - Scoring in class: we will run your detector on novel datasets. #correct/10 × 100 is your percentage. - Over/under-fitting penalties may apply (−50%)."
  },
  {
    "objectID": "posts/isolation_forest/index.html#student-eda",
    "href": "posts/isolation_forest/index.html#student-eda",
    "title": "Isolation Forest",
    "section": "Student EDA",
    "text": "Student EDA\nUse this cell to explore the signal (e.g., plot, summary stats).\n\n# STUDENT EDA\ntry:\n    df = test_files[0]\n    print(df.head())\nexcept Exception as e:\n    print('EDA note: run the original data-loading cells first (the ones that populate train_files/test_files).')\n\n      Value1  Labels\n0  20.801402       0\n1  26.800208       0\n2  33.154527       0\n3  39.189824       0\n4  40.631321       0"
  },
  {
    "objectID": "posts/isolation_forest/index.html#explanation",
    "href": "posts/isolation_forest/index.html#explanation",
    "title": "Isolation Forest",
    "section": "Explanation",
    "text": "Explanation\nThis pipeline works on the idea that:\n    1) it builds upon sliding windows\n    2) gathers normalised anomaly scores from each sub-model and uses them\n    3) averages the anomaly scores\n    4) computes a binary mask by thresholding at the 1st percentile so that it can compare outputs\n    5) stores self.full_anomaly_mask (same length as the placeholder value)\n    6) returns the single index of the lowest‐score window center which closes the loop on the sliding window idea"
  },
  {
    "objectID": "posts/isolation_forest/index.html#student-todo-implement-your-anomaly-detector",
    "href": "posts/isolation_forest/index.html#student-todo-implement-your-anomaly-detector",
    "title": "Isolation Forest",
    "section": "STUDENT TODO — Implement your anomaly detector",
    "text": "STUDENT TODO — Implement your anomaly detector\nImplement Machine Learning/ Statistical models or both. Use the test_files (test series) to train your models and list of anomaly index range for example Anomaly 1: 2001-2005 Anomaly 2: 2010-2012\nConstraints\n\nKeep it efficient; we will run this over 10 datasets and additional novel datasets in class.\n\n#EDA on given model\n\n#Normalization\nscaler = StandardScaler()\n\n\n#Reshape\nrx = df['Value1'].values.reshape(-1,1)\n\nnp_scaled = scaler.fit_transform(rx)\ndata = pd.DataFrame(np_scaled)\n\n\ndata.head()\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n-1.222425\n\n\n1\n-0.824437\n\n\n2\n-0.402862\n\n\n3\n-0.002453\n\n\n4\n0.093183\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\ndef detect_peaks_anomalies(x, min_height=None, distance=20):\n    \"\"\"Peak detection for spike anomalies - WITH StandardScaler\"\"\"\n    from scipy.signal import find_peaks\n\n    # Normalize using StandardScaler\n    scaler = StandardScaler()\n    x_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\n\n    if min_height is None:\n        min_height = 3.0  # Now in standard deviation units\n\n    peaks, _ = find_peaks(x_normalized, height=min_height, distance=distance)\n    neg_peaks, _ = find_peaks(-x_normalized, height=min_height, distance=distance)\n\n    return list(peaks) + list(neg_peaks)\n\ndef detect_change_point_anomalies(x, window_size=50):\n    \"\"\"Detect anomalies based on distribution changes - WITH StandardScaler\"\"\"\n    anomalies = []\n\n    # Normalize using StandardScaler\n    scaler = StandardScaler()\n    x_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\n\n    for i in range(window_size, len(x_normalized) - window_size):\n        before_window = x_normalized[i-window_size:i]\n        after_window = x_normalized[i:i+window_size]\n\n        # Statistical test on standardized data\n        mean_diff = np.abs(np.mean(after_window) - np.mean(before_window))\n        std_before = np.std(before_window)\n\n        # Threshold in standard deviation units\n        if std_before &gt; 0 and mean_diff &gt; 3.0:  # 2 standard deviations\n            anomalies.append(i)\n\n    return anomalies\n\n\n# Implement your anomaly detector/ detectors. You can edit this or use your own\nimport numpy as np\n\ndef student_detect_anomalies(series: np.ndarray) -&gt; list:\n    \"\"\"\n    Input:\n        series: 1D array-like of floats (test series)\n    Output:\n        List of (start, end) index pairs (0-based, end exclusive) for anomaly ranges.\n    \"\"\"\n    x = np.asarray(series, dtype=float)\n    n = len(x)\n    if n == 0:\n        return []\n\n    # Rolling mean/std z-score on a smoothed series\n    # Smooth to get residuals\n    w_smooth = 51\n    k = np.ones(w_smooth) / w_smooth\n    smooth = np.convolve(x, k, mode='same')\n    resid = x - smooth\n\n    # 2) Rolling mean/std using convolution (no extra libs)\n    w = 61  # odd; students may tune\n    kw = np.ones(w) / w\n    mu = np.convolve(resid, kw, mode='same')\n    mu2 = np.convolve(resid*resid, kw, mode='same')\n    var = np.maximum(mu2 - mu*mu, 1e-8)\n    sigma = np.sqrt(var)\n    z = np.abs((resid - mu) / (sigma + 1e-8))\n\n\n\nprint('anomalies identified.')\n\nanomalies identified."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]