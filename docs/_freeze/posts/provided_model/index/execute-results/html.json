{
  "hash": "7906ce3fc260aecc04d33cde7ddce5e6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Provided Model Tuning\nformat: html\n---\n\n::: {#ee4cc3b3 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":206}}' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain_file_names = os.listdir(\"train/\")\ntrain_file_names.sort()\n\ntrain_files = []\nfor file in train_file_names:\n    train_files.append(pd.read_csv(f\"train/{file}\", sep=\";\"))\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\nfor file in test_file_names:\n    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\ntest_files[0].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value1</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20.801402</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26.800208</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33.154527</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39.189824</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40.631321</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Student EDA\nUse this cell to explore the signal (e.g., plot, summary stats).\n\n::: {#3f4a8437 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' execution_count=2}\n``` {.python .cell-code}\n# STUDENT EDA\ntry:\n    df = test_files[0]\n    print(df.head())\nexcept Exception as e:\n    print('EDA note: run the original data-loading cells first (the ones that populate train_files/test_files).')\n    df.info()\n    df.describe()\n\n    df.plot(x='time')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Value1  Labels\n0  20.801402       0\n1  26.800208       0\n2  33.154527       0\n3  39.189824       0\n4  40.631321       0\n```\n:::\n:::\n\n\n# **The Model**\n\n::: {#066aa302 .cell execution_count=3}\n``` {.python .cell-code}\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numba import njit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.covariance import EllipticEnvelope\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Determinant has increased; this should not happen\"\n)\n\n@njit\ndef create_windows_numba(series, window_size):\n    n_windows = len(series) - window_size + 1\n    windows = np.empty((n_windows, window_size), dtype=np.float32)\n    for i in range(n_windows):\n        windows[i, :] = series[i : i + window_size]\n    return windows\n\n@njit\ndef normalize_scores(scores):\n    mn = np.min(scores)\n    mx = np.max(scores)\n    return (scores - mn) / (mx - mn + 1e-8)\n\n\nclass AnomalyDetectionModel:\n    def __init__(self, window_size=30, contamination=0.01):\n        self.window_size = window_size\n        self.offset = window_size // 2\n        self.contamination = contamination\n\n        self.scaler = StandardScaler()\n        self.models = {\n            'IsolationForest': IsolationForest(contamination=contamination, random_state=42),\n            'OneClassSVM': OneClassSVM(kernel='rbf', gamma='scale', nu=contamination),\n            'EllipticEnvelope': EllipticEnvelope(contamination=contamination,\n                                                 support_fraction=0.75,\n                                                 random_state=42),\n        }\n\n        self.use_lof = True\n        self.lof_model = LocalOutlierFactor(n_neighbors=20,\n                                            contamination=contamination,\n                                            novelty=True)\n        self.full_anomaly_mask = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray = None):\n        self.train_windows = self._create_windows(X)\n        self.scaled_train_windows = self.scaler.fit_transform(self.train_windows)\n        for model in self.models.values():\n            model.fit(self.scaled_train_windows)\n        if self.use_lof:\n            self.lof_model.fit(self.scaled_train_windows)\n\n    def predict(self, X: np.ndarray):\n\n        test_windows = self._create_windows(X)\n        scaled = self.scaler.transform(test_windows)\n\n\n        all_scores = []\n        for model in self.models.values():\n            if hasattr(model, \"decision_function\"):\n                s = model.decision_function(scaled)\n                all_scores.append(normalize_scores(s))\n            else:\n                preds = model.predict(scaled)\n                all_scores.append(np.where(preds == -1, 0.0, 1.0))\n\n        if self.use_lof:\n            lof_s = self.lof_model.decision_function(scaled)\n            all_scores.append(normalize_scores(lof_s))\n\n        avg_scores = np.mean(np.stack(all_scores, axis=0), axis=0)\n        thresh = np.percentile(avg_scores, self.contamination * 100)\n        mask = np.zeros(len(X), dtype=int)\n        mask[self.offset : self.offset + len(avg_scores)] = (avg_scores <= thresh).astype(int)\n        self.full_anomaly_mask = mask\n        idx = np.argmin(avg_scores)\n        return idx + self.offset\n    def _create_windows(self, series: np.ndarray):\n        return create_windows_numba(series, self.window_size)\n```\n:::\n\n\n::: {#6f395212 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' execution_count=4}\n``` {.python .cell-code}\n# hybrid_anomaly_detector.py\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import ParameterGrid, train_test_split\nimport itertools\nimport math\nimport random\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Determinant has increased; this should not happen\"\n)\n\n# -------------------------\n# Low-level utilities\n# -------------------------\n@njit\ndef create_windows_numba(series, window_size):\n    n_windows = len(series) - window_size + 1\n    windows = np.empty((n_windows, window_size), dtype=np.float32)\n    for i in range(n_windows):\n        windows[i, :] = series[i : i + window_size]\n    return windows\n\n@njit\ndef normalize_scores_numba(scores):\n    mn = np.min(scores)\n    mx = np.max(scores)\n    out = (scores - mn) / (mx - mn + 1e-8)\n    return out\n\ndef extract_ranges_from_mask(mask):\n    \"\"\"\n    Convert binary mask (1 anomaly, 0 normal) into list of (start, end) ranges [start, end)\n    \"\"\"\n    ranges = []\n    n = len(mask)\n    i = 0\n    while i < n:\n        if mask[i] == 1:\n            start = i\n            j = i + 1\n            while j < n and mask[j] == 1:\n                j += 1\n            ranges.append((start, j))\n            i = j\n        else:\n            i += 1\n    return ranges\n\ndef extract_true_ranges(labels):\n    \"\"\"Assumes labels is 0/1 vector. Returns list of (start,end).\"\"\"\n    return extract_ranges_from_mask(labels.astype(int))\n\n# -------------------------\n# Evaluator\n# -------------------------\nclass AnomalyEvaluator:\n    def __init__(self):\n        pass\n\n    def evaluate_pointwise(self, true_labels, pred_labels):\n        \"\"\"Return dict of standard point-wise metrics.\"\"\"\n        metrics = {}\n        metrics['precision'] = precision_score(true_labels, pred_labels, zero_division=0)\n        metrics['recall'] = recall_score(true_labels, pred_labels, zero_division=0)\n        metrics['f1'] = f1_score(true_labels, pred_labels, zero_division=0)\n        metrics['accuracy'] = accuracy_score(true_labels, pred_labels)\n        # AUC only if both classes present\n        try:\n            if len(np.unique(true_labels)) == 2:\n                metrics['roc_auc'] = roc_auc_score(true_labels, pred_labels)\n            else:\n                metrics['roc_auc'] = np.nan\n        except Exception:\n            metrics['roc_auc'] = np.nan\n        return metrics\n\n    def evaluate_ranges(self, true_ranges, pred_ranges, signal_length=None):\n        \"\"\"\n        Range-level evaluation:\n          - precision = #pred_ranges that overlap any true_range / #pred_ranges\n          - recall = #true_ranges detected / #true_ranges\n          - f1 from precision & recall\n          - avg_overlap_ratio: average (over matched predicted ranges) of intersection/union\n          - avg_delay: average (pred_start - true_start) for matched ranges (if pred_start >= true_start),\n                       negative if predicted earlier.\n        \"\"\"\n        if signal_length is None:\n            # if no ranges and can't determine, set to 0\n            signal_length = 0\n\n        def overlap(a, b):\n            # a and b are (s,e)\n            s = max(a[0], b[0])\n            e = min(a[1], b[1])\n            return max(0, e - s)\n\n        matched_pred = 0\n        matched_true = 0\n        overlap_ratios = []\n        delays = []\n\n        true_matched_flags = [False] * len(true_ranges)\n\n        for p in pred_ranges:\n            # find best overlapping true range (largest overlap)\n            best_overlap = 0\n            best_idx = -1\n            for i, t in enumerate(true_ranges):\n                ov = overlap(p, t)\n                if ov > best_overlap:\n                    best_overlap = ov\n                    best_idx = i\n            if best_overlap > 0:\n                matched_pred += 1\n                true_t = true_ranges[best_idx]\n                if not true_matched_flags[best_idx]:\n                    matched_true += 1\n                    true_matched_flags[best_idx] = True\n\n                # compute union\n                union_len = (max(p[1], true_t[1]) - min(p[0], true_t[0]))\n                overlap_ratios.append(best_overlap / (union_len + 1e-8))\n\n                # detection delay: predicted start - true start\n                delays.append(p[0] - true_t[0])\n\n        n_pred = len(pred_ranges)\n        n_true = len(true_ranges)\n\n        precision = matched_pred / (n_pred + 1e-8) if n_pred > 0 else 0.0\n        recall = matched_true / (n_true + 1e-8) if n_true > 0 else 0.0\n        if precision + recall == 0:\n            f1 = 0.0\n        else:\n            f1 = 2 * precision * recall / (precision + recall)\n\n        avg_overlap = np.mean(overlap_ratios) if len(overlap_ratios) > 0 else 0.0\n        avg_delay = np.mean(delays) if len(delays) > 0 else np.nan\n\n        return {\n            \"range_precision\": precision,\n            \"range_recall\": recall,\n            \"range_f1\": f1,\n            \"avg_overlap_ratio\": float(avg_overlap),\n            \"avg_delay\": float(avg_delay) if not math.isnan(avg_delay) else np.nan,\n            \"n_pred_ranges\": n_pred,\n            \"n_true_ranges\": n_true\n        }\n\n# -------------------------\n# Hybrid Anomaly Detector\n# -------------------------\nclass HybridAnomalyDetector:\n    \"\"\"\n    Hybrid anomaly detector with reproducibility and a tuning method (requires labelled validation set).\n    - window_size: sliding window length used to transform time series -> windows\n    - contamination: expected proportion of anomalies (used as thresholding fallback)\n    - models: default ensemble of IsolationForest, OneClassSVM, EllipticEnvelope, LocalOutlierFactor\n    - random_state: global random seed for reproducibility\n    \"\"\"\n\n    def __init__(self, window_size=30, contamination=0.01, random_state=42, use_lof=True):\n        self.window_size = int(window_size)\n        self.offset = self.window_size // 2\n        self.contamination = contamination\n        self.random_state = int(random_state)\n        np.random.seed(self.random_state)\n        random.seed(self.random_state)\n\n        self.scaler = StandardScaler()\n\n        # instantiate default models with a place to tune their params\n        self.models = {\n            'IsolationForest': IsolationForest(contamination=contamination, random_state=self.random_state),\n            'OneClassSVM': OneClassSVM(kernel='rbf', gamma='scale', nu=contamination),\n            'EllipticEnvelope': EllipticEnvelope(contamination=contamination, support_fraction=0.75, random_state=self.random_state),\n        }\n\n        self.use_lof = use_lof\n        if self.use_lof:\n            self.models['LocalOutlierFactor'] = LocalOutlierFactor(n_neighbors=20, contamination=contamination, novelty=True)\n\n        # placeholders filled after fit:\n        self.train_windows = None\n        self.scaled_train_windows = None\n        self.fitted_models = {}\n        self.full_anomaly_mask = None\n        self.predicted_labels = None\n        self.ranges = None\n        self.scores = None  # ensemble scores per window\n\n    # ---------- window helpers ----------\n    def _create_windows(self, series: np.ndarray):\n        if len(series) < self.window_size:\n            raise ValueError(\"Series shorter than window_size.\")\n        return create_windows_numba(series.astype(np.float32), self.window_size)\n\n    # ---------- fitting ----------\n    def fit(self, X: np.ndarray):\n        \"\"\"\n        Fit all submodels on X (1D numpy array). No labels required.\n        \"\"\"\n        np.random.seed(self.random_state)\n        self.train_windows = self._create_windows(X)\n        self.scaled_train_windows = self.scaler.fit_transform(self.train_windows)\n        self.fitted_models = {}\n        # fit a fresh copy of each model\n        for name, model in self.models.items():\n            # clone by re-instantiating with same parameters to avoid state sharing\n            params = model.get_params()\n            cls = model.__class__\n            params['random_state'] = self.random_state if 'random_state' in params else params.get('random_state', None)\n            new_model = cls(**{k: v for k, v in params.items() if v is not None})\n            new_model.set_params(**{k: v for k, v in model.get_params().items() if k not in ['random_state']})\n            # Fit\n            try:\n                new_model.fit(self.scaled_train_windows)\n            except Exception:\n                # some models may require different interfaces; fallback to original fit\n                model.fit(self.scaled_train_windows)\n                new_model = model\n            self.fitted_models[name] = new_model\n\n    # ---------- score aggregation ----------\n    def _get_model_scores_on_windows(self, windows_scaled):\n        \"\"\"\n        Return dict of arrays: model_name -> normalized score array (higher -> more normal)\n        We'll normalize so that higher = more normal; we'll invert later to have anomaly score.\n        \"\"\"\n        scores = {}\n        for name, model in self.fitted_models.items():\n            # prefer decision_function, then score_samples, else predict\n            if hasattr(model, \"decision_function\"):\n                s = model.decision_function(windows_scaled)\n                s = np.asarray(s, dtype=np.float32)\n                s = normalize_scores_numba(s)  # normalized 0..1\n                scores[name] = s\n            elif hasattr(model, \"score_samples\"):\n                s = model.score_samples(windows_scaled)\n                s = np.asarray(s, dtype=np.float32)\n                s = normalize_scores_numba(s)\n                scores[name] = s\n            else:\n                # predict: returns 1 (inlier) or -1 (outlier)\n                p = model.predict(windows_scaled)\n                s = np.where(np.asarray(p) == -1, 0.0, 1.0).astype(np.float32)\n                scores[name] = s\n        return scores\n\n    def predict_scores(self, X: np.ndarray):\n        \"\"\"\n        Produce ensemble anomaly scores for each sliding window and the aligned point-wise anomaly score.\n        Returns:\n          - avg_anomaly_scores_windows: np.array of length n_windows where higher = more anomalous\n          - aligned_point_scores: np.array length len(X) with 0..1 scores aligned to original samples\n        \"\"\"\n        test_windows = self._create_windows(X)\n        scaled = self.scaler.transform(test_windows)\n        model_scores = self._get_model_scores_on_windows(scaled)\n\n        # combine: convert 'normal' score->anomaly score = 1 - normalized_normal_score\n        stack = []\n        for v in model_scores.values():\n            stack.append(1.0 - v)\n        stacked = np.vstack(stack)  # shape (n_models, n_windows)\n        avg_scores = np.mean(stacked, axis=0)\n\n        # normalize avg_scores 0..1\n        avg_scores = (avg_scores - avg_scores.min()) / (avg_scores.max() - avg_scores.min() + 1e-8)\n\n        # align to original points by placing each window's score at its center index\n        aligned = np.zeros(len(X), dtype=np.float32)\n        n_windows = len(avg_scores)\n        start = self.offset\n        aligned[start : start + n_windows] = avg_scores\n\n        return avg_scores, aligned\n\n    def detect_anomalies(self, X: np.ndarray, top_k_ranges=None):\n        \"\"\"\n        Run detection and produce:\n         - predicted_labels (0/1 array)\n         - ranges list of (start,end) for contiguous anomaly segments\n         - scores aligned with original series\n         - returns peak_index (index of most anomalous point)\n        Optional: top_k_ranges - keep only top_k ranges by average score magnitude (descending)\n        \"\"\"\n        avg_scores_windows, aligned = self.predict_scores(X)\n        self.scores = aligned\n\n        # threshold: percentile based on contamination\n        thresh = np.percentile(avg_scores_windows, 100 * (1 - self.contamination))  # high anomaly = high score\n        # produce mask on aligned\n        mask = np.zeros(len(X), dtype=int)\n        mask[self.offset : self.offset + len(avg_scores_windows)] = (avg_scores_windows >= thresh).astype(int)\n        self.full_anomaly_mask = mask\n\n        # extract ranges\n        ranges = extract_ranges_from_mask(mask)\n        if top_k_ranges is not None and len(ranges) > top_k_ranges:\n            # rank by avg anomaly score within range\n            range_scores = [np.mean(aligned[s:e]) for s, e in ranges]\n            ranked = [r for _, r in sorted(zip(range_scores, ranges), key=lambda x: x[0], reverse=True)]\n            ranges = ranked[:top_k_ranges]\n            # rebuild mask and predicted_labels\n            new_mask = np.zeros_like(mask)\n            for s, e in ranges:\n                new_mask[s:e] = 1\n            mask = new_mask\n\n        self.ranges = ranges\n        self.predicted_labels = mask\n        # return index of most anomalous window center\n        peak_window_idx = int(np.argmax(avg_scores_windows))\n        peak_index = peak_window_idx + self.offset\n        return peak_index\n\n    # ---------- simple per-model hyperparameter tuning using labeled validation set ----------\n    def tune(self, X_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, param_grids: dict = None, max_iters_per_model=None):\n        \"\"\"\n        Tune each model's parameters using a labeled validation set.\n        - X_train: 1D numpy array used to fit scaler & models baseline\n        - X_val: 1D numpy array used for evaluating model behaviour\n        - y_val: 0/1 vector of same length as X_val with ground-truth labels\n        - param_grids: dict: model_name -> param_grid dict (as in sklearn). Example:\n            {\n              'IsolationForest': {'n_estimators': [100,200], 'max_samples': [0.5, 1.0]},\n              'OneClassSVM': {'nu': [0.005, 0.01, 0.05], 'gamma':['scale','auto']},\n            }\n        - If param_grids is None, uses reasonable defaults.\n        Returns best_params dict.\n        \"\"\"\n        # default grids\n        if param_grids is None:\n            param_grids = {\n                'IsolationForest': {'n_estimators': [100, 200], 'max_samples': [0.5, 1.0], 'contamination': [self.contamination]},\n                'OneClassSVM': {'nu': [max(1e-4, self.contamination/2), self.contamination, min(0.5, max(0.1, self.contamination*5))], 'gamma': ['scale', 'auto']},\n                'EllipticEnvelope': {'support_fraction': [0.6, 0.75, 1.0], 'contamination': [self.contamination]},\n                'LocalOutlierFactor': {'n_neighbors': [10, 20, 35], 'contamination': [self.contamination]}\n            }\n\n        # prepare training\n        self.fit(X_train)  # initial fit with default params so scaler available\n        best_params = {}\n\n        X_val_windows = self._create_windows(X_val)\n        scaled_val_windows = self.scaler.transform(X_val_windows)\n\n        # We'll iterate over parameter grid per model, fit on scaled train windows and evaluate using F1 on validation series\n        for model_name, model in list(self.models.items()):\n            grid = param_grids.get(model_name, {})\n            if len(grid) == 0:\n                best_params[model_name] = model.get_params()\n                continue\n\n            best_f1 = -1.0\n            best_param = model.get_params()\n\n            # create iterable of parameter combos\n            combos = list(ParameterGrid(grid))\n            if max_iters_per_model is not None and len(combos) > max_iters_per_model:\n                combos = random.sample(combos, max_iters_per_model)\n\n            for params in combos:\n                # instantiate fresh model with these params\n                cls = model.__class__\n                # ensure random_state consistency if supported\n                if 'random_state' in cls().get_params().keys():\n                    params_local = {**params, 'random_state': self.random_state}\n                else:\n                    params_local = params\n\n                try:\n                    candidate = cls(**params_local)\n                except Exception:\n                    # fallback: try without random_state\n                    candidate = cls(**{k: v for k, v in params.items()})\n\n                # fit candidate on scaled_train_windows\n                try:\n                    candidate.fit(self.scaled_train_windows)\n                except Exception:\n                    # some models (LOF with novelty=True OK). Try continue\n                    continue\n\n                # compute scores on validation windows\n                if hasattr(candidate, \"decision_function\"):\n                    s = candidate.decision_function(scaled_val_windows)\n                    s = normalize_scores_numba(np.asarray(s, dtype=np.float32))\n                    anom_scores = 1.0 - s\n                elif hasattr(candidate, \"score_samples\"):\n                    s = candidate.score_samples(scaled_val_windows)\n                    s = normalize_scores_numba(np.asarray(s, dtype=np.float32))\n                    anom_scores = 1.0 - s\n                else:\n                    p = candidate.predict(scaled_val_windows)\n                    anom_scores = np.where(np.asarray(p) == -1, 1.0, 0.0)\n\n                # align to original X_val\n                aligned = np.zeros(len(X_val), dtype=np.float32)\n                aligned[self.offset : self.offset + len(anom_scores)] = anom_scores\n\n                # threshold using contamination percentile\n                thr = np.percentile(anom_scores, 100 * (1 - self.contamination))\n                mask = np.zeros(len(X_val), dtype=int)\n                mask[self.offset : self.offset + len(anom_scores)] = (anom_scores >= thr).astype(int)\n\n                # compute F1 vs y_val\n                f1 = f1_score(y_val, mask, zero_division=0)\n\n                if f1 > best_f1:\n                    best_f1 = f1\n                    best_param = params\n\n            best_params[model_name] = best_param\n\n            # Update the model in self.models with best params and refit on full train\n            try:\n                cls = model.__class__\n                if 'random_state' in model.get_params().keys():\n                    final_params = {**best_param, 'random_state': self.random_state}\n                else:\n                    final_params = best_param\n                self.models[model_name] = cls(**final_params)\n            except Exception:\n                # if instantiation failed, keep original\n                pass\n\n            # re-fit all models at the end of loop\n            self.fit(X_train)\n\n        return best_params\n\n# -------------------------\n# Example usage with evaluator + test loop the user provided\n# -------------------------\nif __name__ == \"__main__\":\n    # Example showing how to wire up with the evaluation loop you gave:\n    import os\n    from glob import glob\n\n    # create detector and evaluator\n    detector = HybridAnomalyDetector(window_size=30, contamination=0.01, random_state=42, use_lof=True)\n    evaluator = AnomalyEvaluator()\n\n    # test file names (user provided)\n    test_file_names = [\n        \"01.csv\", \"02.csv\", \"03.csv\", \"04.csv\", \"05.csv\",\n        \"06.csv\", \"07.csv\", \"08.csv\", \"09.csv\", \"10.csv\"\n    ]\n\n    point_metrics_list = []\n    range_metrics_list = []\n\n    def simple_top_k_refine(df, detector, k=10):\n        \"\"\"Refine detected ranges to top-k by amplitude / avg anomaly score\"\"\"\n        anomalies = detector.ranges or []\n        if len(anomalies) == 0:\n            return []\n        scores = []\n        for a in anomalies:\n            s = np.mean(np.abs(df['Value1'].iloc[a[0]:a[1]].to_numpy() - df['Value1'].mean()))\n            scores.append(s)\n        if len(anomalies) > k:\n            top_10 = [x for _, x in sorted(zip(scores, anomalies), reverse=True)[:k]]\n        else:\n            top_10 = anomalies\n        return top_10\n\n    for file in test_file_names:\n        path = os.path.join(\"test\", file)\n        if not os.path.exists(path):\n            print(f\"Warning: {path} not found; skipping in example run.\")\n            continue\n\n        df = pd.read_csv(path, sep=\";\")\n        series = df['Value1'].to_numpy()\n\n        # Fit using the file itself for unsupervised case (or you can provide historical normal series)\n        try:\n            detector.fit(series)\n        except Exception as e:\n            print(\"Fit failed:\", e)\n            continue\n\n        # Detect anomalies normally first\n        detector.detect_anomalies(series)\n\n        # Refine to top 10 ranges by magnitude\n        anomalies = detector.ranges or []\n        top_10 = simple_top_k_refine(df, detector, k=10)\n\n        # Update detector output\n        detector.ranges = top_10\n        predicted_labels = np.zeros(len(df), dtype=int)\n        for start, end in top_10:\n            predicted_labels[start:end] = 1\n        detector.predicted_labels = predicted_labels\n\n        # Evaluate\n        true_labels = df['Labels'].to_numpy()\n        true_ranges = extract_true_ranges(true_labels)\n\n        point_metrics = evaluator.evaluate_pointwise(true_labels, detector.predicted_labels)\n        range_metrics = evaluator.evaluate_ranges(true_ranges, detector.ranges)\n\n        point_metrics_list.append(point_metrics)\n        range_metrics_list.append(range_metrics)\n\n        print(f\"\\nFile: {file}\")\n        print(\"Point-wise metrics:\", {k: f\"{v:.4f}\" for k, v in point_metrics.items()})\n        print(\"Range-level metrics:\", {k: f\"{v:.4f}\" for k, v in range_metrics.items()})\n        print(\"Top 10 anomaly ranges:\", top_10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFile: 01.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0213', 'f1': '0.0417', 'accuracy': '0.9127', 'roc_auc': '0.5107'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.7000', 'range_f1': '0.8235', 'avg_overlap_ratio': '0.0362', 'avg_delay': '47.6000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(8737, 8738), (8636, 8641), (2943, 2945), (2870, 2871), (7104, 7106), (7099, 7103), (7703, 7704), (6244, 6245), (6003, 6004), (5606, 5607)]\n\nFile: 02.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0300', 'f1': '0.0582', 'accuracy': '0.8965', 'roc_auc': '0.5150'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.4000', 'range_f1': '0.5714', 'avg_overlap_ratio': '0.0307', 'avg_delay': '52.0000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(9154, 9155), (3552, 3557), (3495, 3498), (2312, 2316), (2217, 2221), (6891, 6893), (6898, 6900), (9195, 9198), (6883, 6887), (9187, 9191)]\n\nFile: 03.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0281', 'f1': '0.0546', 'accuracy': '0.9238', 'roc_auc': '0.5140'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.5000', 'range_f1': '0.6667', 'avg_overlap_ratio': '0.0228', 'avg_delay': '60.2000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(7991, 7992), (7985, 7986), (6695, 6701), (5040, 5041), (3839, 3840), (3815, 3816), (3791, 3792), (2711, 2712), (2687, 2688), (5049, 5057)]\n\nFile: 04.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0858', 'f1': '0.1581', 'accuracy': '0.9265', 'roc_auc': '0.5429'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.8000', 'range_f1': '0.8889', 'avg_overlap_ratio': '0.0850', 'avg_delay': '61.7000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(5017, 5020), (9184, 9185), (6764, 6765), (5857, 5858), (3779, 3780), (2325, 2326), (9651, 9654), (9674, 9680), (9681, 9686), (2847, 2894)]\n\nFile: 05.csv\nPoint-wise metrics: {'precision': '0.8700', 'recall': '0.1713', 'f1': '0.2862', 'accuracy': '0.9566', 'roc_auc': '0.5849'}\nRange-level metrics: {'range_precision': '0.5000', 'range_recall': '0.2000', 'range_f1': '0.2857', 'avg_overlap_ratio': '0.2268', 'avg_delay': '10.2500', 'n_pred_ranges': '8.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(2217, 2218), (9616, 9617), (9638, 9641), (9857, 9860), (9862, 9866), (9873, 9874), (9878, 9959), (9962, 9968)]\n\nFile: 06.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0827', 'f1': '0.1527', 'accuracy': '0.9079', 'roc_auc': '0.5413'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.4000', 'range_f1': '0.5714', 'avg_overlap_ratio': '0.0693', 'avg_delay': '54.2000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(8081, 8083), (7982, 7984), (4578, 4580), (4476, 4478), (3111, 3113), (3019, 3021), (6117, 6120), (6173, 6177), (6122, 6160), (6187, 6213)]\n\nFile: 07.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0456', 'f1': '0.0873', 'accuracy': '0.9226', 'roc_auc': '0.5228'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.6000', 'range_f1': '0.7500', 'avg_overlap_ratio': '0.0830', 'avg_delay': '40.0000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(9426, 9428), (7445, 7449), (7137, 7143), (5888, 5890), (6603, 6607), (5895, 5897), (6589, 6594), (6596, 6599), (3074, 3079), (3047, 3051)]\n\nFile: 08.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0598', 'f1': '0.1129', 'accuracy': '0.9120', 'roc_auc': '0.5299'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.5000', 'range_f1': '0.6667', 'avg_overlap_ratio': '0.0503', 'avg_delay': '67.1000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(8439, 8441), (5082, 5083), (4708, 4709), (4218, 4220), (4267, 4268), (4257, 4265), (4209, 4217), (6536, 6541), (4281, 4289), (4221, 4241)]\n\nFile: 09.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0218', 'f1': '0.0427', 'accuracy': '0.9283', 'roc_auc': '0.5109'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.4000', 'range_f1': '0.5714', 'avg_overlap_ratio': '0.0350', 'avg_delay': '34.5000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(9869, 9870), (8309, 8314), (8304, 8307), (6213, 6214), (6207, 6208), (6201, 6202), (6171, 6172), (2388, 2389), (2376, 2377), (2364, 2365)]\n\nFile: 10.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0278', 'f1': '0.0541', 'accuracy': '0.9020', 'roc_auc': '0.5139'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.8000', 'range_f1': '0.8889', 'avg_overlap_ratio': '0.0256', 'avg_delay': '44.0000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(7891, 7895), (7889, 7890), (5471, 5479), (4149, 4150), (3987, 3990), (6543, 6544), (6547, 6551), (9727, 9729), (9212, 9214), (8743, 8745)]\n```\n:::\n:::\n\n\n## Explanation\n\nThis pipeline works on the idea that:\n\n        1) it builds upon sliding windows\n        2) gathers normalised anomaly scores from each sub-model and uses them\n        3) averages the anomaly scores\n        4) computes a binary mask by thresholding at the 1st percentile so that it can compare outputs\n        5) stores self.full_anomaly_mask (same length as the placeholder value)\n        6) returns the single index of the lowest‐score window center which closes the loop on the sliding window idea\n\n\n## **STUDENT TODO — Implement your anomaly detector**\nImplement Machine Learning/ Statistical models or both. Use the test_files (test series) to train your models and list of anomaly index range for example Anomaly 1:   2001-2005\nAnomaly 2:   2010-2012\n\n\n**Constraints**\n\n- Keep it efficient; we will run this over 10 datasets and additional novel datasets in class.\n\n\n# **Importing Libraries**\nIn this section, we import all required libraries.\nThe anomaly detection system relies on several classical machine learning models (Isolation Forest, One-Class SVM, etc.) and combines them into a hybrid ensemble.\nWe’ll also set warning filters and random seeds for reproducibility.\n\n::: {#c3405b23 .cell execution_count=5}\n``` {.python .cell-code}\n# Suppress unnecessary warnings for cleaner output\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Determinant has increased; this should not happen\"\n)\n\n# Core libraries\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport itertools\n\n# Machine learning models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n)\nfrom sklearn.model_selection import ParameterGrid\n\n# Performance optimization\nfrom numba import njit\n```\n:::\n\n\n# **Normalization of scores functions**\n\nThese helper functions handle window creation (for temporal context) and normalization of model scores.\nThe use of numba’s @njit decorator accelerates numerical operations by compiling them to machine code.\n\n::: {#a38ae2a9 .cell execution_count=6}\n``` {.python .cell-code}\n@njit\ndef create_windows_numba(series, window_size):\n    n_windows = len(series) - window_size + 1\n    windows = np.empty((n_windows, window_size), dtype=np.float32)\n    for i in range(n_windows):\n        windows[i, :] = series[i : i + window_size]\n    return windows\n\n@njit\ndef normalize_scores_numba(scores):\n    mn = np.min(scores)\n    mx = np.max(scores)\n    out = (scores - mn) / (mx - mn + 1e-8)\n    return out\n\ndef extract_ranges_from_mask(mask):\n    \"\"\"\n    Convert binary mask (1 anomaly, 0 normal) into list of (start, end) ranges [start, end)\n    \"\"\"\n    ranges = []\n    n = len(mask)\n    i = 0\n    while i < n:\n        if mask[i] == 1:\n            start = i\n            j = i + 1\n            while j < n and mask[j] == 1:\n                j += 1\n            ranges.append((start, j))\n            i = j\n        else:\n            i += 1\n    return ranges\n\ndef extract_true_ranges(labels):\n    \"\"\"Assumes labels is 0/1 vector. Returns list of (start,end).\"\"\"\n    return extract_ranges_from_mask(labels.astype(int))\n```\n:::\n\n\n# **Evaluation Class**\n\nThe AnomalyEvaluator class provides two complementary evaluation views:\n\n\n*   Point-wise metrics: Treat each time step as an independent prediction.\n\n*   Range-based metrics: Evaluate whether the detected anomaly overlaps with true anomalies, considering timing and coverage.\n\n::: {#e86a19fa .cell execution_count=7}\n``` {.python .cell-code}\n# -------------------------\n# Evaluator\n# -------------------------\nclass AnomalyEvaluator:\n    def __init__(self):\n        pass\n\n    def evaluate_pointwise(self, true_labels, pred_labels):\n        \"\"\"Return dict of standard point-wise metrics.\"\"\"\n        metrics = {}\n        metrics['precision'] = precision_score(true_labels, pred_labels, zero_division=0)\n        metrics['recall'] = recall_score(true_labels, pred_labels, zero_division=0)\n        metrics['f1'] = f1_score(true_labels, pred_labels, zero_division=0)\n        metrics['accuracy'] = accuracy_score(true_labels, pred_labels)\n        # AUC only if both classes present\n        try:\n            if len(np.unique(true_labels)) == 2:\n                metrics['roc_auc'] = roc_auc_score(true_labels, pred_labels)\n            else:\n                metrics['roc_auc'] = np.nan\n        except Exception:\n            metrics['roc_auc'] = np.nan\n        return metrics\n\n    def evaluate_ranges(self, true_ranges, pred_ranges, signal_length=None):\n        \"\"\"\n        Range-level evaluation:\n          - precision = #pred_ranges that overlap any true_range / #pred_ranges\n          - recall = #true_ranges detected / #true_ranges\n          - f1 from precision & recall\n          - avg_overlap_ratio: average (over matched predicted ranges) of intersection/union\n          - avg_delay: average (pred_start - true_start) for matched ranges (if pred_start >= true_start),\n                       negative if predicted earlier.\n        \"\"\"\n        if signal_length is None:\n            # if no ranges and can't determine, set to 0\n            signal_length = 0\n\n        def overlap(a, b):\n            # a and b are (s,e)\n            s = max(a[0], b[0])\n            e = min(a[1], b[1])\n            return max(0, e - s)\n\n        matched_pred = 0\n        matched_true = 0\n        overlap_ratios = []\n        delays = []\n\n        true_matched_flags = [False] * len(true_ranges)\n\n        for p in pred_ranges:\n            # find best overlapping true range (largest overlap)\n            best_overlap = 0\n            best_idx = -1\n            for i, t in enumerate(true_ranges):\n                ov = overlap(p, t)\n                if ov > best_overlap:\n                    best_overlap = ov\n                    best_idx = i\n            if best_overlap > 0:\n                matched_pred += 1\n                true_t = true_ranges[best_idx]\n                if not true_matched_flags[best_idx]:\n                    matched_true += 1\n                    true_matched_flags[best_idx] = True\n\n                # compute union\n                union_len = (max(p[1], true_t[1]) - min(p[0], true_t[0]))\n                overlap_ratios.append(best_overlap / (union_len + 1e-8))\n\n                # detection delay: predicted start - true start\n                delays.append(p[0] - true_t[0])\n\n        n_pred = len(pred_ranges)\n        n_true = len(true_ranges)\n\n        precision = matched_pred / (n_pred + 1e-8) if n_pred > 0 else 0.0\n        recall = matched_true / (n_true + 1e-8) if n_true > 0 else 0.0\n        if precision + recall == 0:\n            f1 = 0.0\n        else:\n            f1 = 2 * precision * recall / (precision + recall)\n\n        avg_overlap = np.mean(overlap_ratios) if len(overlap_ratios) > 0 else 0.0\n        avg_delay = np.mean(delays) if len(delays) > 0 else np.nan\n\n        return {\n            \"range_precision\": precision,\n            \"range_recall\": recall,\n            \"range_f1\": f1,\n            \"avg_overlap_ratio\": float(avg_overlap),\n            \"avg_delay\": float(avg_delay) if not math.isnan(avg_delay) else np.nan,\n            \"n_pred_ranges\": n_pred,\n            \"n_true_ranges\": n_true\n        }\n```\n:::\n\n\n# **Hybrid Anomaly Class**\n\n\n*   Builds sliding windows\n*   Scales data\n*   Fits multiple models\n*   Aggregates anomaly scores\n*   Detects anomaly ranges\n\n::: {#ace604a1 .cell execution_count=8}\n``` {.python .cell-code}\n# -------------------------\n# Hybrid Anomaly Detector\n# -------------------------\nclass HybridAnomalyDetector:\n    \"\"\"\n    Hybrid anomaly detector with reproducibility and a tuning method (requires labelled validation set).\n    - window_size: sliding window length used to transform time series -> windows\n    - contamination: expected proportion of anomalies (used as thresholding fallback)\n    - models: default ensemble of IsolationForest, OneClassSVM, EllipticEnvelope, LocalOutlierFactor\n    - random_state: global random seed for reproducibility\n    \"\"\"\n\n    def __init__(self, window_size=30, contamination=0.01, random_state=42, use_lof=True):\n        self.window_size = int(window_size)\n        self.offset = self.window_size // 2\n        self.contamination = contamination\n        self.random_state = int(random_state)\n        np.random.seed(self.random_state)\n        random.seed(self.random_state)\n\n        self.scaler = StandardScaler()\n\n        # instantiate default models with a place to tune their params\n        self.models = {\n            'IsolationForest': IsolationForest(contamination=contamination, random_state=self.random_state),\n            'OneClassSVM': OneClassSVM(kernel='rbf', gamma='scale', nu=contamination),\n            'EllipticEnvelope': EllipticEnvelope(contamination=contamination, support_fraction=0.75, random_state=self.random_state),\n        }\n\n        self.use_lof = use_lof\n        if self.use_lof:\n            self.models['LocalOutlierFactor'] = LocalOutlierFactor(n_neighbors=20, contamination=contamination, novelty=True)\n\n        # placeholders filled after fit:\n        self.train_windows = None\n        self.scaled_train_windows = None\n        self.fitted_models = {}\n        self.full_anomaly_mask = None\n        self.predicted_labels = None\n        self.ranges = None\n        self.scores = None  # ensemble scores per window\n\n    # ---------- window helpers ----------\n    def _create_windows(self, series: np.ndarray):\n        if len(series) < self.window_size:\n            raise ValueError(\"Series shorter than window_size.\")\n        return create_windows_numba(series.astype(np.float32), self.window_size)\n\n    # ---------- fitting ----------\n    def fit(self, X: np.ndarray):\n        \"\"\"\n        Fit all submodels on X (1D numpy array). No labels required.\n        \"\"\"\n        np.random.seed(self.random_state)\n        self.train_windows = self._create_windows(X)\n        self.scaled_train_windows = self.scaler.fit_transform(self.train_windows)\n        self.fitted_models = {}\n        # fit a fresh copy of each model\n        for name, model in self.models.items():\n            # clone by re-instantiating with same parameters to avoid state sharing\n            params = model.get_params()\n            cls = model.__class__\n            params['random_state'] = self.random_state if 'random_state' in params else params.get('random_state', None)\n            new_model = cls(**{k: v for k, v in params.items() if v is not None})\n            new_model.set_params(**{k: v for k, v in model.get_params().items() if k not in ['random_state']})\n            # Fit\n            try:\n                new_model.fit(self.scaled_train_windows)\n            except Exception:\n                # some models may require different interfaces; fallback to original fit\n                model.fit(self.scaled_train_windows)\n                new_model = model\n            self.fitted_models[name] = new_model\n\n    # ---------- score aggregation ----------\n    def _get_model_scores_on_windows(self, windows_scaled):\n        \"\"\"\n        Return dict of arrays: model_name -> normalized score array (higher -> more normal)\n        We'll normalize so that higher = more normal; we'll invert later to have anomaly score.\n        \"\"\"\n        scores = {}\n        for name, model in self.fitted_models.items():\n            # prefer decision_function, then score_samples, else predict\n            if hasattr(model, \"decision_function\"):\n                s = model.decision_function(windows_scaled)\n                s = np.asarray(s, dtype=np.float32)\n                s = normalize_scores_numba(s)  # normalized 0..1\n                scores[name] = s\n            elif hasattr(model, \"score_samples\"):\n                s = model.score_samples(windows_scaled)\n                s = np.asarray(s, dtype=np.float32)\n                s = normalize_scores_numba(s)\n                scores[name] = s\n            else:\n                # predict: returns 1 (inlier) or -1 (outlier)\n                p = model.predict(windows_scaled)\n                s = np.where(np.asarray(p) == -1, 0.0, 1.0).astype(np.float32)\n                scores[name] = s\n        return scores\n\n    def predict_scores(self, X: np.ndarray):\n        \"\"\"\n        Produce ensemble anomaly scores for each sliding window and the aligned point-wise anomaly score.\n        Returns:\n          - avg_anomaly_scores_windows: np.array of length n_windows where higher = more anomalous\n          - aligned_point_scores: np.array length len(X) with 0..1 scores aligned to original samples\n        \"\"\"\n        test_windows = self._create_windows(X)\n        scaled = self.scaler.transform(test_windows)\n        model_scores = self._get_model_scores_on_windows(scaled)\n\n        # combine: convert 'normal' score->anomaly score = 1 - normalized_normal_score\n        stack = []\n        for v in model_scores.values():\n            stack.append(1.0 - v)\n        stacked = np.vstack(stack)  # shape (n_models, n_windows)\n        avg_scores = np.mean(stacked, axis=0)\n\n        # normalize avg_scores 0..1\n        avg_scores = (avg_scores - avg_scores.min()) / (avg_scores.max() - avg_scores.min() + 1e-8)\n\n        # align to original points by placing each window's score at its center index\n        aligned = np.zeros(len(X), dtype=np.float32)\n        n_windows = len(avg_scores)\n        start = self.offset\n        aligned[start : start + n_windows] = avg_scores\n\n        return avg_scores, aligned\n\n    def detect_anomalies(self, X: np.ndarray, top_k_ranges=None):\n        \"\"\"\n        Run detection and produce:\n         - predicted_labels (0/1 array)\n         - ranges list of (start,end) for contiguous anomaly segments\n         - scores aligned with original series\n         - returns peak_index (index of most anomalous point)\n        Optional: top_k_ranges - keep only top_k ranges by average score magnitude (descending)\n        \"\"\"\n        avg_scores_windows, aligned = self.predict_scores(X)\n        self.scores = aligned\n\n        # threshold: percentile based on contamination\n        thresh = np.percentile(avg_scores_windows, 100 * (1 - self.contamination))  # high anomaly = high score\n        # produce mask on aligned\n        mask = np.zeros(len(X), dtype=int)\n        mask[self.offset : self.offset + len(avg_scores_windows)] = (avg_scores_windows >= thresh).astype(int)\n        self.full_anomaly_mask = mask\n\n        # extract ranges\n        ranges = extract_ranges_from_mask(mask)\n        if top_k_ranges is not None and len(ranges) > top_k_ranges:\n            # rank by avg anomaly score within range\n            range_scores = [np.mean(aligned[s:e]) for s, e in ranges]\n            ranked = [r for _, r in sorted(zip(range_scores, ranges), key=lambda x: x[0], reverse=True)]\n            ranges = ranked[:top_k_ranges]\n            # rebuild mask and predicted_labels\n            new_mask = np.zeros_like(mask)\n            for s, e in ranges:\n                new_mask[s:e] = 1\n            mask = new_mask\n\n        self.ranges = ranges\n        self.predicted_labels = mask\n        # return index of most anomalous window center\n        peak_window_idx = int(np.argmax(avg_scores_windows))\n        peak_index = peak_window_idx + self.offset\n        return peak_index\n\n    # ---------- simple per-model hyperparameter tuning using labeled validation set ----------\n    def tune(self, X_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, param_grids: dict = None, max_iters_per_model=None):\n        \"\"\"\n        Tune each model's parameters using a labeled validation set.\n        - X_train: 1D numpy array used to fit scaler & models baseline\n        - X_val: 1D numpy array used for evaluating model behaviour\n        - y_val: 0/1 vector of same length as X_val with ground-truth labels\n        - param_grids: dict: model_name -> param_grid dict (as in sklearn). Example:\n            {\n              'IsolationForest': {'n_estimators': [100,200], 'max_samples': [0.5, 1.0]},\n              'OneClassSVM': {'nu': [0.005, 0.01, 0.05], 'gamma':['scale','auto']},\n            }\n        - If param_grids is None, uses reasonable defaults.\n        Returns best_params dict.\n        \"\"\"\n        # default grids\n        if param_grids is None:\n            param_grids = {\n                'IsolationForest': {'n_estimators': [100, 200], 'max_samples': [0.5, 1.0], 'contamination': [self.contamination]},\n                'OneClassSVM': {'nu': [max(1e-4, self.contamination/2), self.contamination, min(0.5, max(0.1, self.contamination*5))], 'gamma': ['scale', 'auto']},\n                'EllipticEnvelope': {'support_fraction': [0.6, 0.75, 1.0], 'contamination': [self.contamination]},\n                'LocalOutlierFactor': {'n_neighbors': [10, 20, 35], 'contamination': [self.contamination]}\n            }\n\n        # prepare training\n        self.fit(X_train)  # initial fit with default params so scaler available\n        best_params = {}\n\n        X_val_windows = self._create_windows(X_val)\n        scaled_val_windows = self.scaler.transform(X_val_windows)\n\n        # We'll iterate over parameter grid per model, fit on scaled train windows and evaluate using F1 on validation series\n        for model_name, model in list(self.models.items()):\n            grid = param_grids.get(model_name, {})\n            if len(grid) == 0:\n                best_params[model_name] = model.get_params()\n                continue\n\n            best_f1 = -1.0\n            best_param = model.get_params()\n\n            # create iterable of parameter combos\n            combos = list(ParameterGrid(grid))\n            if max_iters_per_model is not None and len(combos) > max_iters_per_model:\n                combos = random.sample(combos, max_iters_per_model)\n\n            for params in combos:\n                # instantiate fresh model with these params\n                cls = model.__class__\n                # ensure random_state consistency if supported\n                if 'random_state' in cls().get_params().keys():\n                    params_local = {**params, 'random_state': self.random_state}\n                else:\n                    params_local = params\n\n                try:\n                    candidate = cls(**params_local)\n                except Exception:\n                    # fallback: try without random_state\n                    candidate = cls(**{k: v for k, v in params.items()})\n\n                # fit candidate on scaled_train_windows\n                try:\n                    candidate.fit(self.scaled_train_windows)\n                except Exception:\n                    # some models (LOF with novelty=True OK). Try continue\n                    continue\n\n                # compute scores on validation windows\n                if hasattr(candidate, \"decision_function\"):\n                    s = candidate.decision_function(scaled_val_windows)\n                    s = normalize_scores_numba(np.asarray(s, dtype=np.float32))\n                    anom_scores = 1.0 - s\n                elif hasattr(candidate, \"score_samples\"):\n                    s = candidate.score_samples(scaled_val_windows)\n                    s = normalize_scores_numba(np.asarray(s, dtype=np.float32))\n                    anom_scores = 1.0 - s\n                else:\n                    p = candidate.predict(scaled_val_windows)\n                    anom_scores = np.where(np.asarray(p) == -1, 1.0, 0.0)\n\n                # align to original X_val\n                aligned = np.zeros(len(X_val), dtype=np.float32)\n                aligned[self.offset : self.offset + len(anom_scores)] = anom_scores\n\n                # threshold using contamination percentile\n                thr = np.percentile(anom_scores, 100 * (1 - self.contamination))\n                mask = np.zeros(len(X_val), dtype=int)\n                mask[self.offset : self.offset + len(anom_scores)] = (anom_scores >= thr).astype(int)\n\n                # compute F1 vs y_val\n                f1 = f1_score(y_val, mask, zero_division=0)\n\n                if f1 > best_f1:\n                    best_f1 = f1\n                    best_param = params\n\n            best_params[model_name] = best_param\n\n            # Update the model in self.models with best params and refit on full train\n            try:\n                cls = model.__class__\n                if 'random_state' in model.get_params().keys():\n                    final_params = {**best_param, 'random_state': self.random_state}\n                else:\n                    final_params = best_param\n                self.models[model_name] = cls(**final_params)\n            except Exception:\n                # if instantiation failed, keep original\n                pass\n\n            # re-fit all models at the end of loop\n            self.fit(X_train)\n\n        return best_params\n```\n:::\n\n\n# **Evaluation**\nBelow, we show how to use the HybridAnomalyDetector and AnomalyEvaluator classes on test CSVs to detect and evaluate anomalies.\n\n::: {#b52cbdf6 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' execution_count=9}\n``` {.python .cell-code}\nif __name__ == \"__main__\":\n    # Example showing how to wire up with the evaluation loop you gave:\n    import os\n    from glob import glob\n\n    # create detector and evaluator\n    detector = HybridAnomalyDetector(window_size=30, contamination=0.01, random_state=42, use_lof=True)\n    evaluator = AnomalyEvaluator()\n\n    # test file names (user provided)\n    test_file_names = [\n        \"01.csv\", \"02.csv\", \"03.csv\", \"04.csv\", \"05.csv\",\n        \"06.csv\", \"07.csv\", \"08.csv\", \"09.csv\", \"10.csv\"\n    ]\n\n    point_metrics_list = []\n    range_metrics_list = []\n\n    def simple_top_k_refine(df, detector, k=10):\n        \"\"\"Refine detected ranges to top-k by amplitude / avg anomaly score\"\"\"\n        anomalies = detector.ranges or []\n        if len(anomalies) == 0:\n            return []\n        scores = []\n        for a in anomalies:\n            s = np.mean(np.abs(df['Value1'].iloc[a[0]:a[1]].to_numpy() - df['Value1'].mean()))\n            scores.append(s)\n        if len(anomalies) > k:\n            top_10 = [x for _, x in sorted(zip(scores, anomalies), reverse=True)[:k]]\n        else:\n            top_10 = anomalies\n        return top_10\n\n    for file in test_file_names:\n        path = os.path.join(\"test\", file)\n        if not os.path.exists(path):\n            print(f\"Warning: {path} not found; skipping in example run.\")\n            continue\n\n        df = pd.read_csv(path, sep=\";\")\n        series = df['Value1'].to_numpy()\n\n        # Fit using the file itself for unsupervised case (or you can provide historical normal series)\n        try:\n            detector.fit(series)\n        except Exception as e:\n            print(\"Fit failed:\", e)\n            continue\n\n        # Detect anomalies normally first\n        detector.detect_anomalies(series)\n\n        # Refine to top 10 ranges by magnitude\n        anomalies = detector.ranges or []\n        top_10 = simple_top_k_refine(df, detector, k=10)\n\n        # Update detector output\n        detector.ranges = top_10\n        predicted_labels = np.zeros(len(df), dtype=int)\n        for start, end in top_10:\n            predicted_labels[start:end] = 1\n        detector.predicted_labels = predicted_labels\n\n        # Evaluate\n        true_labels = df['Labels'].to_numpy()\n        true_ranges = extract_true_ranges(true_labels)\n\n        point_metrics = evaluator.evaluate_pointwise(true_labels, detector.predicted_labels)\n        range_metrics = evaluator.evaluate_ranges(true_ranges, detector.ranges)\n\n        point_metrics_list.append(point_metrics)\n        range_metrics_list.append(range_metrics)\n\n        print(f\"\\nFile: {file}\")\n        print(\"Point-wise metrics:\", {k: f\"{v:.4f}\" for k, v in point_metrics.items()})\n        print(\"Range-level metrics:\", {k: f\"{v:.4f}\" for k, v in range_metrics.items()})\n        print(\"Top 10 anomaly ranges:\", top_10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFile: 01.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0213', 'f1': '0.0417', 'accuracy': '0.9127', 'roc_auc': '0.5107'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.7000', 'range_f1': '0.8235', 'avg_overlap_ratio': '0.0362', 'avg_delay': '47.6000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(8737, 8738), (8636, 8641), (2943, 2945), (2870, 2871), (7104, 7106), (7099, 7103), (7703, 7704), (6244, 6245), (6003, 6004), (5606, 5607)]\n\nFile: 02.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0300', 'f1': '0.0582', 'accuracy': '0.8965', 'roc_auc': '0.5150'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.4000', 'range_f1': '0.5714', 'avg_overlap_ratio': '0.0307', 'avg_delay': '52.0000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(9154, 9155), (3552, 3557), (3495, 3498), (2312, 2316), (2217, 2221), (6891, 6893), (6898, 6900), (9195, 9198), (6883, 6887), (9187, 9191)]\n\nFile: 03.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0281', 'f1': '0.0546', 'accuracy': '0.9238', 'roc_auc': '0.5140'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.5000', 'range_f1': '0.6667', 'avg_overlap_ratio': '0.0228', 'avg_delay': '60.2000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(7991, 7992), (7985, 7986), (6695, 6701), (5040, 5041), (3839, 3840), (3815, 3816), (3791, 3792), (2711, 2712), (2687, 2688), (5049, 5057)]\n\nFile: 04.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0858', 'f1': '0.1581', 'accuracy': '0.9265', 'roc_auc': '0.5429'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.8000', 'range_f1': '0.8889', 'avg_overlap_ratio': '0.0850', 'avg_delay': '61.7000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(5017, 5020), (9184, 9185), (6764, 6765), (5857, 5858), (3779, 3780), (2325, 2326), (9651, 9654), (9674, 9680), (9681, 9686), (2847, 2894)]\n\nFile: 05.csv\nPoint-wise metrics: {'precision': '0.8700', 'recall': '0.1713', 'f1': '0.2862', 'accuracy': '0.9566', 'roc_auc': '0.5849'}\nRange-level metrics: {'range_precision': '0.5000', 'range_recall': '0.2000', 'range_f1': '0.2857', 'avg_overlap_ratio': '0.2268', 'avg_delay': '10.2500', 'n_pred_ranges': '8.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(2217, 2218), (9616, 9617), (9638, 9641), (9857, 9860), (9862, 9866), (9873, 9874), (9878, 9959), (9962, 9968)]\n\nFile: 06.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0827', 'f1': '0.1527', 'accuracy': '0.9079', 'roc_auc': '0.5413'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.4000', 'range_f1': '0.5714', 'avg_overlap_ratio': '0.0693', 'avg_delay': '54.2000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(8081, 8083), (7982, 7984), (4578, 4580), (4476, 4478), (3111, 3113), (3019, 3021), (6117, 6120), (6173, 6177), (6122, 6160), (6187, 6213)]\n\nFile: 07.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0456', 'f1': '0.0873', 'accuracy': '0.9226', 'roc_auc': '0.5228'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.6000', 'range_f1': '0.7500', 'avg_overlap_ratio': '0.0830', 'avg_delay': '40.0000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(9426, 9428), (7445, 7449), (7137, 7143), (5888, 5890), (6603, 6607), (5895, 5897), (6589, 6594), (6596, 6599), (3074, 3079), (3047, 3051)]\n\nFile: 08.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0598', 'f1': '0.1129', 'accuracy': '0.9120', 'roc_auc': '0.5299'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.5000', 'range_f1': '0.6667', 'avg_overlap_ratio': '0.0503', 'avg_delay': '67.1000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(8439, 8441), (5082, 5083), (4708, 4709), (4218, 4220), (4267, 4268), (4257, 4265), (4209, 4217), (6536, 6541), (4281, 4289), (4221, 4241)]\n\nFile: 09.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0218', 'f1': '0.0427', 'accuracy': '0.9283', 'roc_auc': '0.5109'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.4000', 'range_f1': '0.5714', 'avg_overlap_ratio': '0.0350', 'avg_delay': '34.5000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(9869, 9870), (8309, 8314), (8304, 8307), (6213, 6214), (6207, 6208), (6201, 6202), (6171, 6172), (2388, 2389), (2376, 2377), (2364, 2365)]\n\nFile: 10.csv\nPoint-wise metrics: {'precision': '1.0000', 'recall': '0.0278', 'f1': '0.0541', 'accuracy': '0.9020', 'roc_auc': '0.5139'}\nRange-level metrics: {'range_precision': '1.0000', 'range_recall': '0.8000', 'range_f1': '0.8889', 'avg_overlap_ratio': '0.0256', 'avg_delay': '44.0000', 'n_pred_ranges': '10.0000', 'n_true_ranges': '10.0000'}\nTop 10 anomaly ranges: [(7891, 7895), (7889, 7890), (5471, 5479), (4149, 4150), (3987, 3990), (6543, 6544), (6547, 6551), (9727, 9729), (9212, 9214), (8743, 8745)]\n```\n:::\n:::\n\n\n# **Results Interpretation**\n| Metric Type                | Score                   | Interpretation                                                                                                                                    |\n| -------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n| **Precision ≈ 1.0**        | Very high  |rarely producing false positives(cause for concern). |\n| **Recall ≈ 0.02 – 0.17**   | Very low                          | Detects only a small fraction of true anomaly points.                                    |\n| **F1 ≈ 0.04 – 0.29**       | Low                               |  Model is under-sensitive to anomalies.                                                         |\n| **Accuracy ≈ 0.90 – 0.95** | High but misleading               | Accuracy is inflated .                |\n| **ROC-AUC ≈ 0.51 – 0.58**  | Close to random                   | Is only slightly better than guessing at point level.                |\n\n\n\n\n| Range Metric                           | Score     | Interpretation                                                                                                                             |\n| -------------------------------------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------ |\n| **Range Precision ≈ 1.0**              | Perfect            | Every detected range is correct no false anomaly windows.                                                       |\n| **Range Recall ≈ 0.4 – 0.8**           | Moderate           | The model captures 40–80 % of all true anomalous periods                                         |\n| **Range F1 ≈ 0.57 – 0.89**             | Moderate to strong | It catches the right events but not the full duration. |\n| **Average Overlap ≈ 0.03 – 0.09**      | Small              |  The model flags the start or peak, not the whole event.               |\n| **Average Delay ≈ 40 – 60 points**     | Noticeable lag     | The detector reacts only after anomalies become pronounced.                                                                                |\n| **n_pred_ranges ≈ 10 = n_true_ranges** | Balanced           | The refinement procedure ensures a consistent number of predictions, simplifying evaluation.                                               |\n\n\n\n# **Evaluation**\n The higher the accuracy the better.\n\n::: {#d557bf36 .cell execution_count=10}\n``` {.python .cell-code}\n# Use other various evaluation metrics applicable to your models.\n```\n:::\n\n\n#**Limitations**\nWhile the ensemble sliding-window model seems to be a good fir,it does have some downsides.\n\n##**Computational Cost**:\nBecause the model creates overlapping windows and runs multiple anomaly detection algorithms on each window, it can be computationally intensive—especially for long time series or when using a small window size (which results in many windows).\n\n##**This means it will require increased memory usage**\n\n##**It also means longer runtime compared to a single-model approach**\n\nIt may not be suitable for very large datasets or real-time applications unless optimized or run on powerful hardware and there are some constructive bial issues that still need to be tested.\n\n**_For faster experiments, we could use a larger window size, downsampling the data, or disabling one or more models in the ensemble, but for this we need testing_**\n\n\n# **Visualisation of the anomalies**  \n\nReuse this code to visualize the anomalies.\n\n::: {#aad787cf .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":1000}}' execution_count=11}\n``` {.python .cell-code}\ndef visualize_anomaly_detection(test_df, model, file_idx=None):\n    \"\"\"\n    Visualizes:\n    - Signal (black)\n    - Ground truth anomalies (red)\n    - Predicted anomalies (green)\n    - Most anomalous index (blue dot)\n    \"\"\"\n    series = test_df['Value1'].to_numpy()\n    true_mask = test_df['Labels'].to_numpy().astype(bool)\n    pred_mask = model.full_anomaly_mask.astype(bool)\n    most_anomalous = np.argmin(pred_mask) if pred_mask.any() else None\n    pred_index = model.predict(series)  # triggers .full_anomaly_mask\n\n    plt.figure(figsize=(14, 4))\n    plt.plot(series, color='black', lw=1, label='Signal')\n\n    if pred_mask.any():\n        plt.fill_between(np.arange(len(series)), series,\n                         where=pred_mask, color='green', alpha=0.3,\n                         label='Predicted Anomaly')\n\n    if true_mask.any():\n        plt.fill_between(np.arange(len(series)), series,\n                         where=true_mask, color='red', alpha=0.3,\n                         label='True Anomaly')\n\n    if 0 <= pred_index < len(series):\n        plt.scatter(pred_index, series[pred_index], color='blue', s=50, label='Most Anomalous Point')\n\n    title = f\"File {file_idx}\" if file_idx is not None else \"Anomaly Detection\"\n    plt.title(title)\n    plt.xlabel(\"Time Step\")\n    plt.ylabel(\"Value\")\n    plt.legend(loc=\"upper right\")\n    plt.tight_layout()\n    plt.show()\n\n\n# -- Loop over all files and visualize each --\nfor idx, (train, test) in enumerate(zip(train_files, test_files), 1):\n    model = AnomalyDetectionModel(window_size=30, contamination=0.01)\n    model.fit(train['Value1'].to_numpy(), train['Labels'].to_numpy())\n    model.predict(test['Value1'].to_numpy())  # sets .full_anomaly_mask\n    visualize_anomaly_detection(test, model, file_idx=idx)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-3.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-4.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-5.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-6.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-7.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-8.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-9.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-10.png){width=1334 height=374}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}