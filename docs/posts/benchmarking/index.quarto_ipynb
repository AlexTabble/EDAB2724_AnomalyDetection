{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Benchmarking\n",
        "format: html\n",
        "categories: ['Documentation']\n",
        "---\n",
        "\n",
        "Grouping the anomalous regions have proved to be a challenge and requires a \n",
        "robust method for discovering them.\n",
        "\n",
        "## The Class"
      ],
      "id": "fe02adc0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "from great_tables import GT,md\n",
        "\n",
        "\n",
        "class Benchmarking:\n",
        "    \"\"\"\n",
        "    Class for benchmarking anomaly detection models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__():\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def create_anomaly_groups(data : pd.DataFrame|pd.Series, col='outlier',\n",
        "                              include_single_groups=False,\n",
        "                              show_printout=True,\n",
        "                              merge_tolerance=5,\n",
        "                              noise_tolerance=3) -> list[tuple[int]]:\n",
        "        \"\"\"\n",
        "        Creates list of tuples containing start and end indices of anomalous regions\n",
        "        \n",
        "        ---\n",
        "        Parameters\n",
        "        - data (datalike): Either a dataframe or series\n",
        "        - col (str): Needs to be specified if a dataframe \n",
        "        - include_single_groups (boolean): whether to include anomalous regions\n",
        "                                            which has length of 1\n",
        "        - merge_tolerance (int) : threshold for gaps between anomalies and when\n",
        "                                    its appropriate to merge them.\n",
        "        - noise tolerance (int) : threshold for when groups are considered noise\n",
        "                                    and not truly anomalous regions\n",
        "        ---\n",
        "        Output\n",
        "         [(start_1,end_1),...,(start_n,end_n)]\n",
        "\n",
        "        ---\n",
        "        Example\n",
        "\n",
        "        output = Benchmarking.create_anomaly_groups(data)\n",
        "        print(output)\n",
        "\n",
        "        >>> [(30,35),...,(8000,8029)]\n",
        "        \"\"\"\n",
        "        \n",
        "        group_ids = None\n",
        "        # StackOverflow magic that creates cumsum of anomaly col\n",
        "        if type(data) is pd.DataFrame:\n",
        "            group_ids = data[col].ne(data[col].shift()).cumsum()\n",
        "        else:\n",
        "            group_ids = data.ne(data.shift()).cumsum()\n",
        "            \n",
        "\n",
        "        grouped = data.groupby(group_ids)\n",
        "\n",
        "        groups = []\n",
        "\n",
        "        for group_id, group in grouped:\n",
        "\n",
        "            if type(group) is pd.DataFrame:\n",
        "                if group[col].iloc[0] == False:\n",
        "                    continue\n",
        "            else:\n",
        "                if group.iloc[0] == False:\n",
        "                    continue\n",
        "            \n",
        "            # If a single instance is an anomaly, skip or not?\n",
        "            if len(group) == 1 & include_single_groups: continue \n",
        "            \n",
        "            indices = group.index.tolist()\n",
        "            \n",
        "            groups.append(\n",
        "                (\n",
        "                    indices[0],\n",
        "                    indices[-1] + 1 # Last index is exclusive so increment by 1\n",
        "                )\n",
        "            )\n",
        "\n",
        "        merged = []\n",
        "        start_prev,end_prev = groups[0][0], groups[0][1]\n",
        "        for idx, _ in enumerate(groups):\n",
        "            \n",
        "            if idx == 0: continue\n",
        "            \n",
        "            start_current = groups[idx][0]\n",
        "            end_current = groups[idx][1]\n",
        "            \n",
        "            if (start_current - end_prev) <= merge_tolerance:\n",
        "                end_prev = end_current\n",
        "            else:\n",
        "                merged.append((start_prev, end_prev))\n",
        "                start_prev = start_current\n",
        "                end_prev = end_current\n",
        "        merged.append((start_prev, end_prev))\n",
        "                \n",
        "        groups = merged\n",
        "        \n",
        "        groups = [group for group in groups if (group[1] - group[0]) > noise_tolerance] \n",
        "            \n",
        "            \n",
        "        if show_printout:\n",
        "            print(f'{len(groups)} anomaly groups identified')\n",
        "        return groups\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_model(y_true : np.array, y_pred : np.array,show_printout=True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Returns a DataFrame which contains the metrics for the model\n",
        "\n",
        "        ---\n",
        "        Parameters\n",
        "        - y_true (np.array) : True outlier series\n",
        "        - y_pred (np.array) : Predicted outlier series\n",
        "\n",
        "        ---\n",
        "        Output\n",
        "        pd.DataFrame\n",
        "\n",
        "        |                   | Score  |\n",
        "        |-------------------|--------|\n",
        "        | Accuracy          | 20     |\n",
        "        | Precision         | 40     |\n",
        "        | Recall            | 89     |\n",
        "        | Balanced Accuracy | 89     |\n",
        "        | Groups Accuracy   | 40     |\n",
        "\n",
        "        ---\n",
        "        Example\n",
        "        \n",
        "        metrics = Benchmarking.evaluate_model(y_true,y_pred)\n",
        "        print(metrics)\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        recall = recall_score(y_true, y_pred)\n",
        "        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "        predicted_groups = Benchmarking.create_anomaly_groups(pd.Series(y_pred),\n",
        "                                                              show_printout=show_printout)\n",
        "        true_groups = Benchmarking.create_anomaly_groups(pd.Series(y_true),\n",
        "                                                         show_printout=show_printout)\n",
        "\n",
        "        group_accuracy = Benchmarking._evaluate_groups(predicted_groups, true_groups,\n",
        "                                                       group_penalty=False,\n",
        "                                                       show_printout=show_printout)\n",
        "        penalized_group_accuracy = Benchmarking._evaluate_groups(predicted_groups,\n",
        "                                                                 true_groups,\n",
        "                                                                 show_printout=show_printout)\n",
        "\n",
        "        metrics = pd.DataFrame({\n",
        "            'Score' : [\n",
        "                round(accuracy*100,2),\n",
        "                round(precision*100,2),\n",
        "                round(recall*100,2),\n",
        "                round(balanced_accuracy*100,2),\n",
        "                round(group_accuracy*100,2),\n",
        "                round(penalized_group_accuracy*100,2)\n",
        "            ]\n",
        "        }, index = ['Accuracy','Precision','Recall','Balanced Accuracy',\n",
        "                    'Group Accuracy','Penalised Group Accuracy'])\n",
        "        return metrics\n",
        "\n",
        "    @staticmethod\n",
        "    def _evaluate_groups(predicted_groups : list[tuple[int]],\n",
        "                         true_groups : list[tuple[int]],\n",
        "                         group_penalty : bool = True,\n",
        "                         show_printout :bool = True) -> float:\n",
        "\n",
        "        if show_printout:\n",
        "            if len(predicted_groups) > 10:\n",
        "                print(f'Model predicts {len(predicted_groups) -10} more than 10')\n",
        "            elif len(predicted_groups) < 10:\n",
        "                print(f'Model predicts {10 - len(predicted_groups)} less than 10')\n",
        "            else:\n",
        "                print('Number of groups match!')\n",
        "                   \n",
        "        actual_starts = [idx[0] for idx in true_groups]\n",
        "        actual_ends = [idx[1] for idx in true_groups]\n",
        "\n",
        "        valid_preds = 0\n",
        "        bad_preds = []\n",
        "        for pred in predicted_groups:\n",
        "            is_start_correct = pred[0] in actual_starts\n",
        "            is_end_correct = pred[1] in actual_ends\n",
        "            \n",
        "            if is_end_correct & is_end_correct : valid_preds +=1\n",
        "            else:\n",
        "                bad_preds.append(pred)\n",
        "\n",
        "        accuracy = valid_preds / len(true_groups)\n",
        "\n",
        "        # PERF: I'm not sure wether this is appropriate\n",
        "        # This is on the assumption that the unseen data also contains\n",
        "        # exactly 10 anomalies which means the model does not necessarly generalise\n",
        "        # to unseen data where the anomaly count is known.\n",
        "        # NOTE: I will ask whether this is appropriate\n",
        "        # For testing purposes, I'm including this but use it by keeping data leakage\n",
        "        # in mind\n",
        "        if group_penalty:\n",
        "            penalty = min(len(predicted_groups), len(true_groups)) / max(\n",
        "                len(predicted_groups), len(true_groups)\n",
        "            )\n",
        "            accuracy = accuracy * penalty\n",
        "        \n",
        "        return accuracy\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def print_evaluation(y_true : np.array, y_pred : np.array, model_name: str) -> None:\n",
        "\n",
        "        metrics = Benchmarking.evaluate_model(y_true, y_pred)\n",
        "        metrics = metrics.reset_index().rename({'index' : 'Metric'})\n",
        "\n",
        "        (\n",
        "            GT(metrics)\n",
        "            .tab_header(md(f'Model Results for **{model_name}**'))\n",
        "            .tab_source_note(md(\"Metrics are in percentage(%)\"))\n",
        "        ).show()"
      ],
      "id": "a99ebeca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark Documentation\n",
        "\n",
        "The most important function in the class is `create_anomaly_groups()`\n",
        "\n",
        "#### `create_anomaly_groups`\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "| Parameter               | Type                          | Description                                                                                                      |\n",
        "| ----------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
        "| `data`                  | `pd.DataFrame` or `pd.Series` | Input data containing anomalies. If DataFrame, the column must be specified.                                     |\n",
        "| `col`                   | `str`                         | Column name to check for anomalies (only needed if `data` is a DataFrame). Default is `'outlier'`.               |\n",
        "| `include_single_groups` | `bool`                        | Whether to include anomalous regions of length 1. Default is `False`.                                            |\n",
        "| `show_printout`         | `bool`                        | Whether to print the number of anomaly groups identified. Default is `True`.                                     |\n",
        "| `merge_tolerance`       | `int`                         | Maximum gap between consecutive anomaly regions that will be merged. Default is `5`.                             |\n",
        "| `noise_tolerance`       | `int`                         | Minimum length for a group to be considered a true anomaly; shorter groups are treated as noise. Default is `3`. |\n",
        "\n",
        "**Output**\n",
        "| Output | Type | Description |\n",
        "|----------------|--------------------|-----------------------|\n",
        "| `groups` | `list[tuple[int]]` | ${ (start_1,end_1),(start_2,end_2), \\dots, (start_n, end_n)}$\n",
        "This function can be used regardless of the chosen classifier.\n",
        "\n",
        "### Does it generalise?\n",
        "\n",
        "Most models have good accuracies and precision,recall values so actual classifiers\n",
        "are not the main problem.\n",
        "\n",
        "To accurately state where the anomalous regions are is a challenge as you will\n",
        "have to tune the `merge_tolerance` and `noise_tolerance` parameters for the\n",
        "grouper which could introduce possible bias.\n",
        "\n",
        "If the nature of the data is known(i.e. Financial, Geogolicial, etc) those\n",
        "parameters can be tuned with relevant business knowledge.\n",
        "\n",
        "The dataset is called _ec2_utilization.zip_ which monitors the cpu usage of an\n",
        "AWS EC2 instance over time. More research can be done to determine the norm\n",
        "in terms of deviation frequency and length of deviation. (I'm assuming the\n",
        "data is about cpu usage but we probably need to confirm)\n",
        "\n",
        "To generalise the classification of anomalous period, I believe some leniency in\n",
        "the classification of what an anomalous period is necessary.\n",
        "\n",
        "For example: If the daily LIBOR rate decreased for 5 days during the 2007-2008\n",
        "GFC, it does not mean the entire period is not anomalous. If this 5-day period\n",
        "is not ignored as noise, the GFC would be classified as two anomalous periods(one\n",
        "before the 5 days and one after) which essentially fragments the period.\n",
        "\n",
        "This is seen extensively in the Z-Score predictor where a single period is\n",
        "fragmented into multiple smaller periods with small gaps between them.\n",
        "\n",
        "### Possible Improvements\n",
        "\n",
        "`include_single_groups` might cause a fragmentation of a a single anomalous\n",
        "region into two anomalous regions with incorrect $start_{n-1$ and $end_{n}$\n",
        "values.\n",
        "\n",
        "The reason I included the parameter in the first place is that if an anomaly has\n",
        "length of 1, its start- and end points are the same like $(n,n)$ which I didn't\n",
        "want to deal with.\n",
        "\n",
        "After making the grouper more tolerant towards gaps and noise, I noticed that it\n",
        "might not be a good parameter to include.\n",
        "\n",
        "## Available Functions in Benchmarking.py\n",
        "\n",
        "#### `evaluate_model`\n",
        "\n",
        "**Input**\n",
        "\n",
        "| Parameter | Type       | Description                                               |\n",
        "| --------- | ---------- | --------------------------------------------------------- |\n",
        "| `y_true`  | `np.array` | True binary anomaly labels (1 for anomaly, 0 for normal). |\n",
        "| `y_pred`  | `np.array` | Predicted binary anomaly labels.                          |\n",
        "\n",
        "**Output**\n",
        "\n",
        "| Output    | DataType  |\n",
        "| --------- | --------- |\n",
        "| `metrics` | DataFrame |\n",
        "\n",
        "| Metric                     | Description                                                           |\n",
        "| -------------------------- | --------------------------------------------------------------------- |\n",
        "| `Accuracy`                 | Standard classification accuracy.                                     |\n",
        "| `Precision`                | Fraction of predicted anomalies that are true anomalies.              |\n",
        "| `Recall`                   | Fraction of true anomalies that were detected.                        |\n",
        "| `Balanced Accuracy`        | Average of recall per class (handles imbalance).                      |\n",
        "| `Group Accuracy`           | Accuracy at the anomaly group level (based on `_evaluate_groups`).    |\n",
        "| `Penalised Group Accuracy` | Group accuracy with penalty applied for mismatch in number of groups. |\n",
        "\n",
        "**Example usage**\n",
        "\n",
        "```python\n",
        "metrics = Benchmarking.evaluate_model(y_true, y_pred)\n",
        "print(metrics)\n",
        "```\n",
        "\n",
        "#### `print_evaluation`\n",
        "\n",
        "**Input**\n",
        "| Parameter | Type | Description |\n",
        "| ------------ | ---------- | -------------------------------------------------------- |\n",
        "| `y_true` | `np.array` | True anomaly labels. |\n",
        "| `y_pred` | `np.array` | Predicted anomaly labels. |\n",
        "| `model_name` | `str` | Name of the model, used for display in the table header. |\n",
        "\n",
        "**Output**\n",
        "| Output | DataType |\n",
        "|-----------|------------|\n",
        "| printed DataFrame | None|\n",
        "\n",
        "**Example usage**\n",
        "\n",
        "```python\n",
        "\n",
        "Benchmarking.print_evaluation(y_true, y_pred, \"ARIMA(Tuned)\")\n",
        "```\n",
        "\n",
        "### Benchmarking usage\n",
        "\n",
        "#### For Google Colab\n",
        "\n",
        "Copy and paste the file contents into a cell\n",
        "\n",
        "#### For Personal Machine\n",
        "\n",
        "Make sure benchmarking.py file is in same directory as .ipynb/.qmd file\n",
        "\n",
        "```\n",
        ".\n",
        "├── benchmarking.py\n",
        "├── test\n",
        "│   ├── 01.csv\n",
        "│   ├── 02.csv\n",
        "│   ├── 03.csv\n",
        "│   ├── 04.csv\n",
        "│   ├── 05.csv\n",
        "│   ├── 06.csv\n",
        "│   ├── 07.csv\n",
        "│   ├── 08.csv\n",
        "│   ├── 09.csv\n",
        "│   └── 10.csv\n",
        "└── Z*score_model.qmd -> \\_Same directory level as benchmarking.py*\n",
        "\n",
        "```\n",
        "\n",
        "```python\n",
        "from benchmarking import Benchmarking\n",
        "\n",
        "metrics = Benchmarking.evaluate_model(y_true, y_pred)\n",
        "```\n"
      ],
      "id": "4052c5c0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/alex/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}