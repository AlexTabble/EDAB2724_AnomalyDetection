{
  "hash": "70f46e1add5eb4324ae3a0e3a1417093",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Isolation Forest'\nhtml:\n  theme: darkly\n---\n\n\n\n**Read this**\n- Do **not** add or remove Python libraries. Stick to the imports already present in this notebook. Changing libraries is an automatic **−100%**.\n- You may use **machine learning, statistics, or a hybrid** approach — but your method must generalize to **new, unseen datasets**.\n- Datasets: We have 10 time-series with **10 000 rows** each; anomalies: **10 segments per dataset**.\n You can upload the zip to you Google drive and use the ID from Google drive url.\n- Scoring in class: we will run your detector on **novel datasets**. **#correct/10 × 100** is your percentage.\n- Over/under-fitting penalties may apply (**−50%**).\n\n# What you must do\nImplement your anomaly detector using any means (could it be Machine Learning or statistics or a combination of both to improve the accuracy of the model). Return the index ranges for the anomalies for example 2001-2010.  \n\nYou can also add small EDA (plots/stats) in the **EDA cell** below to justify your approach.\n\n**Do not modify** existing data loading and the libraries.\n\n# **The Anomaly Detection Notebook**\n\n::: {#a9107835 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":206}}' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain_file_names = os.listdir(\"train/\")\ntrain_file_names.sort()\n\ntrain_files = []\nfor file in train_file_names:\n    train_files.append(pd.read_csv(f\"train/{file}\", sep=\";\"))\n\ntest_file_names = os.listdir(\"test/\")\ntest_file_names.sort()\n\ntest_files = []\nfor file in test_file_names:\n    test_files.append(pd.read_csv(f\"test/{file}\", sep=\";\"))\n\ntest_files[0].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value1</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20.801402</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26.800208</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33.154527</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39.189824</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40.631321</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Student EDA\nUse this cell to explore the signal (e.g., plot, summary stats).\n\n::: {#7a1e726b .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' execution_count=2}\n``` {.python .cell-code}\n# STUDENT EDA\ntry:\n    df = test_files[0]\n    print(df.head())\nexcept Exception as e:\n    print('EDA note: run the original data-loading cells first (the ones that populate train_files/test_files).')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Value1  Labels\n0  20.801402       0\n1  26.800208       0\n2  33.154527       0\n3  39.189824       0\n4  40.631321       0\n```\n:::\n:::\n\n\n# **The Model**\n\n::: {#ad0ce502 .cell execution_count=3}\n``` {.python .cell-code}\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numba import njit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.covariance import EllipticEnvelope\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Determinant has increased; this should not happen\"\n)\n\n@njit\ndef create_windows_numba(series, window_size):\n    n_windows = len(series) - window_size + 1\n    windows = np.empty((n_windows, window_size), dtype=np.float32)\n    for i in range(n_windows):\n        windows[i, :] = series[i : i + window_size]\n    return windows\n\n@njit\ndef normalize_scores(scores):\n    mn = np.min(scores)\n    mx = np.max(scores)\n    return (scores - mn) / (mx - mn + 1e-8)\n\n\nclass AnomalyDetectionModel:\n    def __init__(self, window_size=30, contamination=0.01):\n        self.window_size = window_size\n        self.offset = window_size // 2\n        self.contamination = contamination\n\n        self.scaler = StandardScaler()\n        self.models = {\n            'IsolationForest': IsolationForest(contamination=contamination, random_state=42),\n            'OneClassSVM': OneClassSVM(kernel='rbf', gamma='scale', nu=contamination),\n            'EllipticEnvelope': EllipticEnvelope(contamination=contamination,\n                                                 support_fraction=0.75,\n                                                 random_state=42),\n        }\n\n        self.use_lof = True\n        self.lof_model = LocalOutlierFactor(n_neighbors=20,\n                                            contamination=contamination,\n                                            novelty=True)\n        self.full_anomaly_mask = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray = None):\n        self.train_windows = self._create_windows(X)\n        self.scaled_train_windows = self.scaler.fit_transform(self.train_windows)\n        for model in self.models.values():\n            model.fit(self.scaled_train_windows)\n        if self.use_lof:\n            self.lof_model.fit(self.scaled_train_windows)\n\n    def predict(self, X: np.ndarray):\n\n        test_windows = self._create_windows(X)\n        scaled = self.scaler.transform(test_windows)\n\n\n        all_scores = []\n        for model in self.models.values():\n            if hasattr(model, \"decision_function\"):\n                s = model.decision_function(scaled)\n                all_scores.append(normalize_scores(s))\n            else:\n                preds = model.predict(scaled)\n                all_scores.append(np.where(preds == -1, 0.0, 1.0))\n\n        if self.use_lof:\n            lof_s = self.lof_model.decision_function(scaled)\n            all_scores.append(normalize_scores(lof_s))\n\n        avg_scores = np.mean(np.stack(all_scores, axis=0), axis=0)\n        thresh = np.percentile(avg_scores, self.contamination * 100)\n        mask = np.zeros(len(X), dtype=int)\n        mask[self.offset : self.offset + len(avg_scores)] = (avg_scores <= thresh).astype(int)\n        self.full_anomaly_mask = mask\n        idx = np.argmin(avg_scores)\n        return idx + self.offset\n    def _create_windows(self, series: np.ndarray):\n        return create_windows_numba(series, self.window_size)\n```\n:::\n\n\n## Explanation\n\nThis pipeline works on the idea that:\n\n        1) it builds upon sliding windows\n        2) gathers normalised anomaly scores from each sub-model and uses them\n        3) averages the anomaly scores\n        4) computes a binary mask by thresholding at the 1st percentile so that it can compare outputs\n        5) stores self.full_anomaly_mask (same length as the placeholder value)\n        6) returns the single index of the lowest‐score window center which closes the loop on the sliding window idea\n\n\n## **STUDENT TODO — Implement your anomaly detector**\nImplement Machine Learning/ Statistical models or both. Use the test_files (test series) to train your models and list of anomaly index range for example Anomaly 1:   2001-2005\nAnomaly 2:   2010-2012\n\n\n**Constraints**\n\n- Keep it efficient; we will run this over 10 datasets and additional novel datasets in class.\n\n\n#EDA on given model\n\n::: {#c669d832 .cell execution_count=4}\n``` {.python .cell-code}\n#Normalization\nscaler = StandardScaler()\n```\n:::\n\n\n::: {#412e2a01 .cell execution_count=5}\n``` {.python .cell-code}\n#Reshape\nrx = df['Value1'].values.reshape(-1,1)\n\nnp_scaled = scaler.fit_transform(rx)\ndata = pd.DataFrame(np_scaled)\n```\n:::\n\n\n::: {#f493ac9c .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":206}}' execution_count=6}\n``` {.python .cell-code}\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.222425</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.824437</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.402862</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.002453</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.093183</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#59e31064 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\ndef detect_peaks_anomalies(x, min_height=None, distance=20):\n    \"\"\"Peak detection for spike anomalies - WITH StandardScaler\"\"\"\n    from scipy.signal import find_peaks\n\n    # Normalize using StandardScaler\n    scaler = StandardScaler()\n    x_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\n\n    if min_height is None:\n        min_height = 3.0  # Now in standard deviation units\n\n    peaks, _ = find_peaks(x_normalized, height=min_height, distance=distance)\n    neg_peaks, _ = find_peaks(-x_normalized, height=min_height, distance=distance)\n\n    return list(peaks) + list(neg_peaks)\n\ndef detect_change_point_anomalies(x, window_size=50):\n    \"\"\"Detect anomalies based on distribution changes - WITH StandardScaler\"\"\"\n    anomalies = []\n\n    # Normalize using StandardScaler\n    scaler = StandardScaler()\n    x_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\n\n    for i in range(window_size, len(x_normalized) - window_size):\n        before_window = x_normalized[i-window_size:i]\n        after_window = x_normalized[i:i+window_size]\n\n        # Statistical test on standardized data\n        mean_diff = np.abs(np.mean(after_window) - np.mean(before_window))\n        std_before = np.std(before_window)\n\n        # Threshold in standard deviation units\n        if std_before > 0 and mean_diff > 3.0:  # 2 standard deviations\n            anomalies.append(i)\n\n    return anomalies\n```\n:::\n\n\n::: {#873c0ee7 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' execution_count=8}\n``` {.python .cell-code}\n# Implement your anomaly detector/ detectors. You can edit this or use your own\nimport numpy as np\n\ndef student_detect_anomalies(series: np.ndarray) -> list:\n    \"\"\"\n    Input:\n        series: 1D array-like of floats (test series)\n    Output:\n        List of (start, end) index pairs (0-based, end exclusive) for anomaly ranges.\n    \"\"\"\n    x = np.asarray(series, dtype=float)\n    n = len(x)\n    if n == 0:\n        return []\n\n    # Rolling mean/std z-score on a smoothed series\n    # Smooth to get residuals\n    w_smooth = 51\n    k = np.ones(w_smooth) / w_smooth\n    smooth = np.convolve(x, k, mode='same')\n    resid = x - smooth\n\n    # 2) Rolling mean/std using convolution (no extra libs)\n    w = 61  # odd; students may tune\n    kw = np.ones(w) / w\n    mu = np.convolve(resid, kw, mode='same')\n    mu2 = np.convolve(resid*resid, kw, mode='same')\n    var = np.maximum(mu2 - mu*mu, 1e-8)\n    sigma = np.sqrt(var)\n    z = np.abs((resid - mu) / (sigma + 1e-8))\n\n\n\nprint('anomalies identified.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nanomalies identified.\n```\n:::\n:::\n\n\n# **Evaluation**\n The higher the accuracy the better.\n\n::: {#2d04979d .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' execution_count=9}\n``` {.python .cell-code}\ncorrect = 0\nfor train, test in zip(train_files, test_files):\n    model = AnomalyDetectionModel()\n\n    model.fit(train.Value1.to_numpy().flatten(), train.Labels.to_numpy().flatten())\n\n    prediction_index = model.predict(test.Value1.to_numpy().flatten())\n\n\n    if (test.loc[prediction_index, \"Labels\"] == 1):\n        correct += 1\n\nprint(f\"Total score: {correct}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal score: 10%\n```\n:::\n:::\n\n\n::: {#5b1a3a54 .cell execution_count=10}\n``` {.python .cell-code}\n# Use other various evaluation metrics applicable to your models.\n```\n:::\n\n\n#**Limitations**\nWhile the ensemble sliding-window model seems to be a good fir,it does have some downsides.\n\n##**Computational Cost**:\nBecause the model creates overlapping windows and runs multiple anomaly detection algorithms on each window, it can be computationally intensive—especially for long time series or when using a small window size (which results in many windows).\n\n##**This means it will require increased memory usage**\n\n##**It also means longer runtime compared to a single-model approach**\n\nIt may not be suitable for very large datasets or real-time applications unless optimized or run on powerful hardware and there are some constructive bial issues that still need to be tested.\n\n**_For faster experiments, we could use a larger window size, downsampling the data, or disabling one or more models in the ensemble, but for this we need testing_**\n\n\n# **Visualisation of the anomalies**  \n\nReuse this code to visualize the anomalies.\n\n::: {#98dfefa8 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":1000}}' execution_count=11}\n``` {.python .cell-code}\ndef visualize_anomaly_detection(test_df, model, file_idx=None):\n    \"\"\"\n    Visualizes:\n    - Signal (black)\n    - Ground truth anomalies (red)\n    - Predicted anomalies (green)\n    - Most anomalous index (blue dot)\n    \"\"\"\n    series = test_df['Value1'].to_numpy()\n    true_mask = test_df['Labels'].to_numpy().astype(bool)\n    pred_mask = model.full_anomaly_mask.astype(bool)\n    most_anomalous = np.argmin(pred_mask) if pred_mask.any() else None\n    pred_index = model.predict(series)  # triggers .full_anomaly_mask\n\n    plt.figure(figsize=(14, 4))\n    plt.plot(series, color='black', lw=1, label='Signal')\n\n    if pred_mask.any():\n        plt.fill_between(np.arange(len(series)), series,\n                         where=pred_mask, color='green', alpha=0.3,\n                         label='Predicted Anomaly')\n\n    if true_mask.any():\n        plt.fill_between(np.arange(len(series)), series,\n                         where=true_mask, color='red', alpha=0.3,\n                         label='True Anomaly')\n\n    if 0 <= pred_index < len(series):\n        plt.scatter(pred_index, series[pred_index], color='blue', s=50, label='Most Anomalous Point')\n\n    title = f\"File {file_idx}\" if file_idx is not None else \"Anomaly Detection\"\n    plt.title(title)\n    plt.xlabel(\"Time Step\")\n    plt.ylabel(\"Value\")\n    plt.legend(loc=\"upper right\")\n    plt.tight_layout()\n    plt.show()\n\n\n# -- Loop over all files and visualize each --\nfor idx, (train, test) in enumerate(zip(train_files, test_files), 1):\n    model = AnomalyDetectionModel(window_size=30, contamination=0.01)\n    model.fit(train['Value1'].to_numpy(), train['Labels'].to_numpy())\n    model.predict(test['Value1'].to_numpy())  # sets .full_anomaly_mask\n    visualize_anomaly_detection(test, model, file_idx=idx)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-3.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-4.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-5.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-6.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-7.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-8.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-9.png){width=1334 height=374}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-10.png){width=1334 height=374}\n:::\n:::\n\n\n#EDA\n\n::: {#4f041afc .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":206}}' execution_count=12}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value1</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20.801402</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26.800208</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33.154527</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39.189824</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40.631321</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#60964358 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' execution_count=13}\n``` {.python .cell-code}\nprint(df.shape)\n\nprint(\"Data Types\", df.dtypes)\nprint(\"Descriptive stats:\", df['Value1'].describe())\n\nmissing_vals = df['Value1'].isna().sum()\nprint(f\"\\nMissing Total values: {missing_vals}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(10000, 2)\nData Types Value1    float64\nLabels      int64\ndtype: object\nDescriptive stats: count    10000.000000\nmean        39.226792\nstd         15.073575\nmin          0.000000\n25%         32.807308\n50%         39.452857\n75%         45.885332\nmax        100.000000\nName: Value1, dtype: float64\n\nMissing Total values: 0\n```\n:::\n:::\n\n\n::: {#609c5c65 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":300}}' execution_count=14}\n``` {.python .cell-code}\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value1</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>39.226792</td>\n      <td>0.089200</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>15.073575</td>\n      <td>0.285046</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>32.807308</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>39.452857</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>45.885332</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>100.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#b9a90ccf .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":659}}' execution_count=15}\n``` {.python .cell-code}\n# Show histograms - all variables except for the identifier\ndf.hist(bins = 20, figsize =(20, 10))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=1550 height=801}\n:::\n:::\n\n\nComprehensive EDA with statistical analysis and visualizations\n\nProper preprocessing using StandardScaler\n\nFeature engineering with rolling window statistics\n\nIsolation Forest model with configurable parameters\n\nRange-based detection that groups consecutive anomalies\n\nComprehensive evaluation with precision, recall, and F1-score\n\nVisualization to compare true vs predicted anomalies\n\n::: {#359be2f0 .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\",\"height\":1000}}' execution_count=16}\n``` {.python .cell-code}\n# Enhanced Isolation Forest Anomaly Detector with 10-anomaly target\nclass IsolationForestAnomalyDetector:\n    def __init__(self, target_anomalies=10, contamination_range=(0.01, 0.2), random_state=42):\n        self.target_anomalies = target_anomalies\n        self.contamination_range = contamination_range\n        self.random_state = random_state\n        self.scaler = StandardScaler()\n        self.model = None\n        self.anomaly_windows = []\n        self.optimal_contamination = None\n\n    def create_features(self, series, window_size=50):\n        \"\"\"Create rolling window features for better anomaly detection\"\"\"\n        series = np.array(series).reshape(-1, 1)\n\n        # Basic statistical features\n        features = []\n\n        # Original value\n        features.append(series)\n\n        # Rolling statistics\n        for window in [10, 20, 50]:\n            if len(series) >= window:\n                # Rolling mean\n                roll_mean = pd.Series(series.flatten()).rolling(window=window, center=True).mean().values.reshape(-1, 1)\n                features.append(roll_mean)\n\n                # Rolling standard deviation\n                roll_std = pd.Series(series.flatten()).rolling(window=window, center=True).std().values.reshape(-1, 1)\n                features.append(roll_std)\n\n                # Difference from rolling mean\n                diff_from_mean = series - roll_mean\n                features.append(diff_from_mean)\n\n        # Add more features for better detection\n        # Z-score\n        z_score = (series - np.mean(series)) / np.std(series)\n        features.append(z_score)\n\n        # Combine all features\n        feature_matrix = np.hstack([f for f in features if f is not None and not np.any(np.isnan(f))])\n\n        # Handle NaN values that might occur from rolling operations\n        feature_matrix = np.nan_to_num(feature_matrix)\n\n        return feature_matrix\n\n    def find_anomaly_ranges(self, predictions, min_consecutive=3):\n        \"\"\"Convert point anomalies to ranges\"\"\"\n        anomaly_indices = np.where(predictions == -1)[0]\n\n        if len(anomaly_indices) == 0:\n            return []\n\n        # Group consecutive anomalies\n        ranges = []\n        start = anomaly_indices[0]\n        end = anomaly_indices[0]\n        count = 1\n\n        for i in range(1, len(anomaly_indices)):\n            if anomaly_indices[i] == anomaly_indices[i-1] + 1:\n                end = anomaly_indices[i]\n                count += 1\n            else:\n                # Only keep ranges with minimum consecutive anomalies\n                if count >= min_consecutive:\n                    ranges.append((start, end))\n                start = anomaly_indices[i]\n                end = anomaly_indices[i]\n                count = 1\n\n        # Add the last range\n        if count >= min_consecutive:\n            ranges.append((start, end))\n\n        return ranges\n\n    def optimize_contamination(self, X):\n        \"\"\"Find optimal contamination parameter to get close to target anomalies\"\"\"\n        best_contamination = self.contamination_range[0]\n        best_diff = float('inf')\n        best_predictions = None\n\n        # Test multiple contamination values\n        contamination_values = np.linspace(self.contamination_range[0], self.contamination_range[1], 20)\n\n        for contamination in contamination_values:\n            # Create features\n            X_features = self.create_features(X)\n            X_scaled = self.scaler.fit_transform(X_features)\n\n            # Train Isolation Forest\n            model = IsolationForest(\n                contamination=contamination,\n                random_state=self.random_state,\n                n_estimators=100\n            )\n            model.fit(X_scaled)\n\n            # Predict anomalies\n            predictions = model.predict(X_scaled)\n            n_anomalies = np.sum(predictions == -1)\n\n            # Calculate how close we are to target\n            diff = abs(n_anomalies - self.target_anomalies)\n\n            if diff < best_diff:\n                best_diff = diff\n                best_contamination = contamination\n                best_predictions = predictions\n\n        self.optimal_contamination = best_contamination\n        return best_predictions, best_contamination\n\n    def fit(self, X):\n        \"\"\"Fit the model on training data with optimized contamination\"\"\"\n        # Optimize contamination parameter\n        predictions, optimal_contamination = self.optimize_contamination(X)\n\n        # Create features\n        X_features = self.create_features(X)\n\n        # Scale the features\n        X_scaled = self.scaler.fit_transform(X_features)\n\n        # Train final Isolation Forest with optimal contamination\n        self.model = IsolationForest(\n            contamination=optimal_contamination,\n            random_state=self.random_state,\n            n_estimators=100\n        )\n        self.model.fit(X_scaled)\n\n        # Store anomaly windows\n        self.anomaly_windows = self.find_anomaly_ranges(predictions)\n\n        print(f\"Optimal contamination: {optimal_contamination:.4f}\")\n        print(f\"Number of anomaly points detected: {np.sum(predictions == -1)}\")\n        print(f\"Number of anomaly ranges: {len(self.anomaly_windows)}\")\n\n    def predict(self, X):\n        \"\"\"Predict anomalies on test data\"\"\"\n        # Create features\n        X_features = self.create_features(X)\n\n        # Scale the features\n        X_scaled = self.scaler.transform(X_features)\n\n        # Predict anomalies\n        predictions = self.model.predict(X_scaled)\n\n        # Convert to binary (1 = normal, -1 = anomaly)\n        binary_predictions = np.where(predictions == 1, 0, 1)\n\n        # Find anomaly ranges\n        self.anomaly_windows = self.find_anomaly_ranges(predictions)\n\n        return binary_predictions\n\n# Enhanced student anomaly detector with 10-anomaly target\ndef student_detect_anomalies(series: np.ndarray) -> list:\n    \"\"\"\n    Input:\n    series: 1D array-like of floats (test series)\n\n    Output:\n    List of (start, end) index pairs (0-based, end inclusive) for anomaly ranges.\n    \"\"\"\n    x = np.asarray(series, dtype=float)\n    n = len(x)\n\n    if n == 0:\n        return []\n\n    # Initialize and fit the Isolation Forest detector with 10-anomaly target\n    detector = IsolationForestAnomalyDetector(\n        target_anomalies=10,\n        contamination_range=(0.005, 0.3),  # Wider range to find optimal value\n        random_state=42\n    )\n\n    # Fit on the test series\n    detector.fit(x)\n\n    # Return the detected anomaly ranges\n    return detector.anomaly_windows\n\n# Enhanced evaluation function with detailed accuracy metrics\ndef evaluate_anomaly_detection_enhanced(true_ranges, predicted_ranges, tolerance=5, target_anomalies=10):\n    \"\"\"\n    Enhanced evaluation with accuracy metrics and target-based scoring\n    \"\"\"\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    # Convert true ranges from labels\n    true_anomaly_segments = []\n    for true_range in true_ranges:\n        true_anomaly_segments.append((true_range[0], true_range[1]))\n\n    # Check each predicted range against true ranges\n    matched_true = set()\n\n    for pred_start, pred_end in predicted_ranges:\n        matched = False\n        for i, (true_start, true_end) in enumerate(true_anomaly_segments):\n            # Check if predicted range overlaps with true range within tolerance\n            if (pred_start <= true_end + tolerance and pred_end >= true_start - tolerance):\n                if i not in matched_true:\n                    true_positives += 1\n                    matched_true.add(i)\n                    matched = True\n                    break\n\n        if not matched:\n            false_positives += 1\n\n    false_negatives = len(true_anomaly_segments) - len(matched_true)\n\n    # Calculate standard metrics\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / len(true_anomaly_segments) if len(true_anomaly_segments) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    # Calculate accuracy metrics\n    total_predictions = len(predicted_ranges)\n    target_accuracy = 1 - min(abs(total_predictions - target_anomalies) / target_anomalies, 1.0)\n\n    # Combined score (weighted average of F1 and target accuracy)\n    combined_score = 0.7 * f1 + 0.3 * target_accuracy\n\n    # Detection success (1 if at least one true positive, 0 otherwise)\n    detection_success = 1 if true_positives > 0 else 0\n\n    return {\n        'true_positives': true_positives,\n        'false_positives': false_positives,\n        'false_negatives': false_negatives,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'target_accuracy': target_accuracy,\n        'combined_score': combined_score,\n        'detection_success': detection_success,\n        'total_predicted_ranges': total_predictions,\n        'total_true_ranges': len(true_anomaly_segments)\n    }\n\n# Enhanced visualization with accuracy information\ndef visualize_anomaly_detection_with_accuracy(test_df, predicted_ranges, metrics, file_idx=None):\n    \"\"\"Visualize the anomaly detection results with accuracy information\"\"\"\n    series = test_df['Value'].values if 'Value' in test_df.columns else test_df.iloc[:, 0].values\n    true_labels = test_df['Labels'].values if 'Labels' in test_df.columns else test_df.iloc[:, 1].values\n    true_mask = true_labels.astype(bool)\n\n    plt.figure(figsize=(16, 8))\n\n    # Plot the signal\n    plt.plot(series, color='black', linewidth=1, label='Signal')\n\n    # Plot true anomalies\n    if true_mask.any():\n        plt.fill_between(np.arange(len(series)), np.min(series), np.max(series),\n                        where=true_mask, color='red', alpha=0.3, label='True Anomalies')\n\n    # Plot predicted anomalies\n    for start, end in predicted_ranges:\n        plt.axvspan(start, end, alpha=0.3, color='green', label='Predicted Anomalies' if start == predicted_ranges[0][0] else \"\")\n\n    # Add accuracy information to title\n    title = f'File {file_idx} - ' if file_idx is not None else ''\n    title += f'Anomaly Detection (F1: {metrics[\"f1_score\"]:.3f}, Target Acc: {metrics[\"target_accuracy\"]:.3f})'\n    title += f'\\nTP: {metrics[\"true_positives\"]}, FP: {metrics[\"false_positives\"]}, FN: {metrics[\"false_negatives\"]}'\n    title += f', Predicted: {metrics[\"total_predicted_ranges\"]}, True: {metrics[\"total_true_ranges\"]}'\n\n    plt.title(title)\n    plt.xlabel('Time Step')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n# Enhanced main evaluation loop\nprint(\"Starting Enhanced Isolation Forest Anomaly Detection Evaluation\")\nprint(\"TARGET: Detect approximately 10 anomalies in each of the 10 datasets\")\nprint(\"=\" * 70)\n\ntotal_correct = 0\ntotal_files = len(test_files)\nall_metrics = []\ndetailed_results = []\n\nfor idx, test_df in enumerate(test_files):\n    print(f\"\\n--- Processing File {idx} ---\")\n\n    try:\n        # Extract series and labels\n        series = test_df['Value'].values if 'Value' in test_df.columns else test_df.iloc[:, 0].values\n        labels = test_df['Labels'].values if 'Labels' in test_df.columns else test_df.iloc[:, 1].values\n\n        # Get true anomaly ranges\n        true_ranges = extract_true_anomaly_ranges(labels)\n        print(f\"True anomaly ranges: {true_ranges}\")\n        print(f\"Number of true anomaly segments: {len(true_ranges)}\")\n\n        # Detect anomalies using enhanced student's function\n        predicted_ranges = student_detect_anomalies(series)\n        print(f\"Predicted anomaly ranges: {predicted_ranges}\")\n        print(f\"Number of predicted anomaly segments: {len(predicted_ranges)}\")\n\n        # Evaluate performance with target-based metrics\n        metrics = evaluate_anomaly_detection_enhanced(true_ranges, predicted_ranges, target_anomalies=10)\n        all_metrics.append(metrics)\n\n        # Store detailed results\n        detailed_results.append({\n            'file_idx': idx,\n            'true_ranges': true_ranges,\n            'predicted_ranges': predicted_ranges,\n            'metrics': metrics\n        })\n\n        # Count correct detections (at least one true positive)\n        correct_this_file = metrics['detection_success']\n        total_correct += correct_this_file\n\n        print(f\"True Positives: {metrics['true_positives']}\")\n        print(f\"False Positives: {metrics['false_positives']}\")\n        print(f\"False Negatives: {metrics['false_negatives']}\")\n        print(f\"Precision: {metrics['precision']:.4f}\")\n        print(f\"Recall: {metrics['recall']:.4f}\")\n        print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n        print(f\"Target Accuracy: {metrics['target_accuracy']:.4f}\")\n        print(f\"Combined Score: {metrics['combined_score']:.4f}\")\n        print(f\"File {idx} Detection Success: {correct_this_file}/1\")\n\n    except Exception as e:\n        print(f\"Error processing file {idx}: {e}\")\n        import traceback\n        traceback.print_exc()\n        # Add zero metrics for failed files\n        all_metrics.append({\n            'true_positives': 0,\n            'false_positives': 0,\n            'false_negatives': len(true_ranges) if 'true_ranges' in locals() else 0,\n            'precision': 0,\n            'recall': 0,\n            'f1_score': 0,\n            'target_accuracy': 0,\n            'combined_score': 0,\n            'detection_success': 0,\n            'total_predicted_ranges': 0,\n            'total_true_ranges': len(true_ranges) if 'true_ranges' in locals() else 0\n        })\n\n# Calculate overall scores with enhanced metrics\noverall_precision = np.mean([m['precision'] for m in all_metrics])\noverall_recall = np.mean([m['recall'] for m in all_metrics])\noverall_f1 = np.mean([m['f1_score'] for m in all_metrics])\noverall_target_accuracy = np.mean([m['target_accuracy'] for m in all_metrics])\noverall_combined_score = np.mean([m['combined_score'] for m in all_metrics])\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FINAL ENHANCED RESULTS FOR ALL TEST FILES\")\nprint(\"=\" * 70)\nprint(f\"Total files processed: {total_files}\")\nprint(f\"Total correct files: {total_correct}/{total_files}\")\nprint(f\"Detection Accuracy: {(total_correct/total_files)*100:.2f}%\")\nprint(f\"Overall Precision: {overall_precision:.4f}\")\nprint(f\"Overall Recall: {overall_recall:.4f}\")\nprint(f\"Overall F1-Score: {overall_f1:.4f}\")\nprint(f\"Overall Target Accuracy: {overall_target_accuracy:.4f}\")\nprint(f\"Overall Combined Score: {overall_combined_score:.4f}\")\n\n# Enhanced detailed results display\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DETAILED ENHANCED RESULTS BY FILE\")\nprint(\"=\" * 70)\nfor result in detailed_results:\n    m = result['metrics']\n    print(f\"\\nFile {result['file_idx']}:\")\n    print(f\"  True Ranges: {result['true_ranges']} (count: {len(result['true_ranges'])})\")\n    print(f\"  Predicted Ranges: {result['predicted_ranges']} (count: {len(result['predicted_ranges'])})\")\n    print(f\"  Precision: {m['precision']:.4f}\")\n    print(f\"  Recall: {m['recall']:.4f}\")\n    print(f\"  F1-Score: {m['f1_score']:.4f}\")\n    print(f\"  Target Accuracy: {m['target_accuracy']:.4f}\")\n    print(f\"  Combined Score: {m['combined_score']:.4f}\")\n    print(f\"  Detection Success: {m['detection_success']}/1\")\n\n# Enhanced visualization for ALL files\nprint(\"\\nVisualizing enhanced results for ALL files...\")\nfor idx, test_df in enumerate(test_files):\n    try:\n        series = test_df['Value'].values if 'Value' in test_df.columns else test_df.iloc[:, 0].values\n        labels = test_df['Labels'].values if 'Labels' in test_df.columns else test_df.iloc[:, 1].values\n        true_ranges = extract_true_anomaly_ranges(labels)\n        predicted_ranges = student_detect_anomalies(series)\n        metrics = evaluate_anomaly_detection_enhanced(true_ranges, predicted_ranges, target_anomalies=10)\n        visualize_anomaly_detection_with_accuracy(test_df, predicted_ranges, metrics, file_idx=idx)\n    except Exception as e:\n        print(f\"Error visualizing file {idx}: {e}\")\n\n# Create comprehensive summary visualization\nprint(\"\\nCreating comprehensive performance summary...\")\nplt.figure(figsize=(16, 12))\n\n# Plot 1: Overall performance metrics\nplt.subplot(3, 3, 1)\nmetrics_names = ['Precision', 'Recall', 'F1-Score', 'Target Acc', 'Combined']\nmetrics_values = [overall_precision, overall_recall, overall_f1, overall_target_accuracy, overall_combined_score]\ncolors = ['blue', 'green', 'red', 'purple', 'orange']\n\nplt.bar(metrics_names, metrics_values, color=colors)\nplt.title('Overall Performance Metrics')\nplt.ylim(0, 1)\nplt.xticks(rotation=45)\nfor i, v in enumerate(metrics_values):\n    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n\n# Plot 2: Per-file F1 scores\nplt.subplot(3, 3, 2)\nfile_indices = list(range(total_files))\nf1_scores = [m['f1_score'] for m in all_metrics]\nplt.bar(file_indices, f1_scores, color='orange', alpha=0.7)\nplt.axhline(y=overall_f1, color='red', linestyle='--', label=f'Average: {overall_f1:.3f}')\nplt.title('F1-Score by File')\nplt.xlabel('File Index')\nplt.ylabel('F1-Score')\nplt.ylim(0, 1)\nplt.legend()\n\n# Plot 3: Per-file target accuracy\nplt.subplot(3, 3, 3)\ntarget_accuracies = [m['target_accuracy'] for m in all_metrics]\nplt.bar(file_indices, target_accuracies, color='purple', alpha=0.7)\nplt.axhline(y=overall_target_accuracy, color='red', linestyle='--', label=f'Average: {overall_target_accuracy:.3f}')\nplt.title('Target Accuracy by File')\nplt.xlabel('File Index')\nplt.ylabel('Target Accuracy')\nplt.ylim(0, 1)\nplt.legend()\n\n# Plot 4: Detection summary\nplt.subplot(3, 3, 4)\ntotal_tp = sum([m['true_positives'] for m in all_metrics])\ntotal_fp = sum([m['false_positives'] for m in all_metrics])\ntotal_fn = sum([m['false_negatives'] for m in all_metrics])\n\ndetection_types = ['True Positives', 'False Positives', 'False Negatives']\ndetection_counts = [total_tp, total_fp, total_fn]\ncolors = ['green', 'red', 'orange']\n\nplt.bar(detection_types, detection_counts, color=colors)\nplt.title('Overall Detection Summary')\nplt.xticks(rotation=45)\nfor i, v in enumerate(detection_counts):\n    plt.text(i, v + 0.1, str(v), ha='center', va='bottom')\n\n# Plot 5: File-wise detection success\nplt.subplot(3, 3, 5)\nfile_scores = [m['detection_success'] for m in all_metrics]\ncolors = ['red' if score == 0 else 'green' for score in file_scores]\nplt.bar(file_indices, file_scores, color=colors)\nplt.title('File-wise Detection Success (1=Success, 0=Failure)')\nplt.xlabel('File Index')\nplt.ylabel('Detection Success')\nplt.ylim(0, 1)\n\n# Plot 6: Predicted vs Target ranges\nplt.subplot(3, 3, 6)\npredicted_counts = [m['total_predicted_ranges'] for m in all_metrics]\ntarget_line = [10] * total_files  # Target of 10 anomalies per file\n\nplt.plot(file_indices, predicted_counts, 'bo-', label='Predicted Ranges', linewidth=2)\nplt.plot(file_indices, target_line, 'r--', label='Target (10)', linewidth=2)\nplt.title('Predicted vs Target Anomaly Ranges')\nplt.xlabel('File Index')\nplt.ylabel('Number of Ranges')\nplt.legend()\n\n# Plot 7: Combined score by file\nplt.subplot(3, 3, 7)\ncombined_scores = [m['combined_score'] for m in all_metrics]\nplt.bar(file_indices, combined_scores, color='orange', alpha=0.7)\nplt.axhline(y=overall_combined_score, color='red', linestyle='--', label=f'Average: {overall_combined_score:.3f}')\nplt.title('Combined Score by File')\nplt.xlabel('File Index')\nplt.ylabel('Combined Score')\nplt.ylim(0, 1)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nEnhanced anomaly detection completed for all test files!\")\nprint(f\"Final Summary: {total_correct}/{total_files} files successfully detected anomalies\")\nprint(f\"Overall performance score: {overall_combined_score:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting Enhanced Isolation Forest Anomaly Detection Evaluation\nTARGET: Detect approximately 10 anomalies in each of the 10 datasets\n======================================================================\n\n--- Processing File 0 ---\nError processing file 0: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 1 ---\nError processing file 1: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 2 ---\nError processing file 2: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 3 ---\nError processing file 3: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 4 ---\nError processing file 4: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 5 ---\nError processing file 5: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 6 ---\nError processing file 6: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 7 ---\nError processing file 7: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 8 ---\nError processing file 8: name 'extract_true_anomaly_ranges' is not defined\n\n--- Processing File 9 ---\nError processing file 9: name 'extract_true_anomaly_ranges' is not defined\n\n======================================================================\nFINAL ENHANCED RESULTS FOR ALL TEST FILES\n======================================================================\nTotal files processed: 10\nTotal correct files: 0/10\nDetection Accuracy: 0.00%\nOverall Precision: 0.0000\nOverall Recall: 0.0000\nOverall F1-Score: 0.0000\nOverall Target Accuracy: 0.0000\nOverall Combined Score: 0.0000\n\n======================================================================\nDETAILED ENHANCED RESULTS BY FILE\n======================================================================\n\nVisualizing enhanced results for ALL files...\nError visualizing file 0: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 1: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 2: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 3: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 4: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 5: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 6: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 7: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 8: name 'extract_true_anomaly_ranges' is not defined\nError visualizing file 9: name 'extract_true_anomaly_ranges' is not defined\n\nCreating comprehensive performance summary...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_43524/860436678.py\", line 306, in <module>\n    true_ranges = extract_true_anomaly_ranges(labels)\nNameError: name 'extract_true_anomaly_ranges' is not defined\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-3.png){width=1526 height=1142}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEnhanced anomaly detection completed for all test files!\nFinal Summary: 0/10 files successfully detected anomalies\nOverall performance score: 0.0000\n```\n:::\n:::\n\n\n#Isolation Forest and Z-score Hybrid Model\n\n###Data Processing & Feature Engineering\nExtracts true anomaly ranges from label arrays using extract_true_anomaly_ranges()\n\nCreates enhanced features including:\n\nOriginal time series values\n\nMultiple rolling window statistics (mean, std, min, max) across different window sizes\n\nZ-scores based on rolling statistics\n\nGlobal statistics (mean, std, z-score)\n\nRate of change and second derivatives\n\nDifferences from rolling means\n\n### Anomaly Detection Methods\n#### Primary: High-Precision Isolation Forest\nUses lower contamination range (0.05-0.2) for fewer false positives\n\nOptimizes contamination parameter specifically for precision\n\nEnhanced configuration: 150 estimators, smaller samples, feature subsetting\n\nAnti-overfitting measures: max_samples=128, max_features=0.7\n\n#### Secondary: Statistical Z-Score Detection\nMaintains Z-score threshold at 3.0 standard deviations\n\nUses stricter range grouping with minimum 3 consecutive points\n\nOnly keeps substantial anomaly ranges\n\n### Range Processing & Grouping\nStricter range grouping: Maximum 5-point gaps between anomalies (was 10)\n\nMinimum length requirement: 3+ consecutive anomalies to form a range\n\nSelective range combination: Only keeps substantial ranges from both methods\n\nConservative merging: Smaller merge distance (10 points)\n\n::: {#c69f3d8f .cell quarto-private-1='{\"key\":\"colab\",\"value\":{\"base_uri\":\"https://localhost:8080/\"}}' execution_count=17}\n``` {.python .cell-code}\n# Extract true anomaly ranges from labels\ndef extract_true_anomaly_ranges(labels):\n    \"\"\"Extract contiguous anomaly ranges from label array\"\"\"\n    labels = np.array(labels)\n    anomaly_indices = np.where(labels == 1)[0]\n\n    if len(anomaly_indices) == 0:\n        return []\n\n    ranges = []\n    start = anomaly_indices[0]\n    end = anomaly_indices[0]\n\n    for i in range(1, len(anomaly_indices)):\n        if anomaly_indices[i] == anomaly_indices[i-1] + 1:\n            end = anomaly_indices[i]\n        else:\n            ranges.append((start, end))\n            start = anomaly_indices[i]\n            end = anomaly_indices[i]\n\n    ranges.append((start, end))\n    return ranges\n\n# High Precision Isolation Forest Anomaly Detector (Z-score threshold = 3.0)\nclass HighPrecisionIsolationForestDetector:\n    def __init__(self, contamination_range=(0.05, 0.2), random_state=42):  # REDUCED range\n        self.contamination_range = contamination_range  # Lower: 0.05-0.2\n        self.random_state = random_state\n        self.scaler = StandardScaler()\n        self.model = None\n        self.anomaly_windows = []\n\n    def create_enhanced_features(self, series, window_sizes=[10, 20, 50, 100]):\n        \"\"\"Enhanced feature engineering with multiple statistical features\"\"\"\n        series = np.array(series).flatten()  # Ensure 1D array\n        features = []\n\n        # Original series (reshape to 2D for stacking)\n        features.append(series.reshape(-1, 1))\n\n        # Multiple rolling window statistics\n        for window in window_sizes:\n            if len(series) >= window:\n                # Rolling statistics\n                roll_mean = pd.Series(series).rolling(window=window, center=True).mean().values.reshape(-1, 1)\n                roll_std = pd.Series(series).rolling(window=window, center=True).std().values.reshape(-1, 1)\n                roll_min = pd.Series(series).rolling(window=window, center=True).min().values.reshape(-1, 1)\n                roll_max = pd.Series(series).rolling(window=window, center=True).max().values.reshape(-1, 1)\n\n                # Handle NaN values\n                roll_mean = np.nan_to_num(roll_mean)\n                roll_std = np.nan_to_num(roll_std)\n                roll_min = np.nan_to_num(roll_min)\n                roll_max = np.nan_to_num(roll_max)\n\n                features.extend([roll_mean, roll_std, roll_min, roll_max])\n\n                # Z-score based on rolling statistics\n                z_score = (series.reshape(-1, 1) - roll_mean) / (roll_std + 1e-8)\n                features.append(z_score)\n\n                # Differences and changes\n                diff_from_mean = series.reshape(-1, 1) - roll_mean\n                features.append(diff_from_mean)\n\n        # Global statistics\n        global_mean = np.full((len(series), 1), np.mean(series))\n        global_std = np.full((len(series), 1), np.std(series))\n        global_z = (series.reshape(-1, 1) - global_mean) / (global_std + 1e-8)\n        features.extend([global_mean, global_std, global_z])\n\n        # Rate of change and derivatives\n        diff_1 = np.diff(series, prepend=series[0])\n        diff_2 = np.diff(diff_1, prepend=diff_1[0])\n        features.extend([diff_1.reshape(-1, 1), diff_2.reshape(-1, 1)])\n\n        # Combine features\n        feature_matrix = np.hstack([f for f in features if f is not None])\n        return feature_matrix\n\n    def find_anomaly_ranges_stricter(self, predictions, max_gap=5, min_length=3):\n        \"\"\"Stricter range grouping to reduce false positives\"\"\"\n        anomaly_indices = np.where(predictions == -1)[0]\n\n        if len(anomaly_indices) == 0:\n            return []\n\n        # Group anomalies with smaller allowed gaps\n        ranges = []\n        start = anomaly_indices[0]\n        end = anomaly_indices[0]\n        consecutive_count = 1\n\n        for i in range(1, len(anomaly_indices)):\n            if anomaly_indices[i] <= anomaly_indices[i-1] + max_gap:  # Smaller gap\n                end = anomaly_indices[i]\n                consecutive_count += 1\n            else:\n                # Only keep ranges with minimum length\n                if consecutive_count >= min_length:  # Minimum length requirement\n                    ranges.append((start, end))\n                start = anomaly_indices[i]\n                end = anomaly_indices[i]\n                consecutive_count = 1\n\n        # Only keep the last range if it meets minimum length\n        if consecutive_count >= min_length:\n            ranges.append((start, end))\n\n        return ranges\n\n    def statistical_anomaly_detection(self, series, z_threshold=3.0):  # KEEP: 3.0\n        \"\"\"Statistical method using Z-scores with threshold = 3.0\"\"\"\n        z_scores = np.abs((series - np.mean(series)) / (np.std(series) + 1e-8))\n        statistical_anomalies = np.where(z_scores > z_threshold)[0]\n\n        # Convert point anomalies to ranges with STRICTER grouping\n        stat_ranges = []\n        if len(statistical_anomalies) > 0:\n            start = statistical_anomalies[0]\n            end = statistical_anomalies[0]\n            consecutive_count = 1\n\n            for i in range(1, len(statistical_anomalies)):\n                if statistical_anomalies[i] <= statistical_anomalies[i-1] + 3:  # STRICTER: 5 to 3\n                    end = statistical_anomalies[i]\n                    consecutive_count += 1\n                else:\n                    # Only keep ranges with minimum length\n                    if consecutive_count >= 3:  # Minimum 3 consecutive points\n                        stat_ranges.append((start, end))\n                    start = statistical_anomalies[i]\n                    end = statistical_anomalies[i]\n                    consecutive_count = 1\n\n            if consecutive_count >= 3:  # Minimum 3 consecutive points\n                stat_ranges.append((start, end))\n\n        return stat_ranges\n\n    def optimize_contamination_for_precision(self, X, target_precision=0.7):\n        \"\"\"Optimize contamination parameter for better precision\"\"\"\n        best_contamination = self.contamination_range[0]\n        best_score = -1\n\n        contamination_values = np.linspace(self.contamination_range[0], self.contamination_range[1], 20)\n\n        for contamination in contamination_values:\n            try:\n                # Create features\n                X_features = self.create_enhanced_features(X)\n                X_scaled = self.scaler.fit_transform(X_features)\n\n                # Train Isolation Forest\n                model = IsolationForest(\n                    contamination=contamination,\n                    random_state=self.random_state,\n                    n_estimators=150,  # More trees for stability\n                    max_samples=128,   # Smaller samples for less overfitting\n                    max_features=0.7,  # Use subset of features\n                    n_jobs=-1\n                )\n                model.fit(X_scaled)\n\n                # Predict anomalies\n                predictions = model.predict(X_scaled)\n                pred_ranges = self.find_anomaly_ranges_stricter(predictions)\n\n                # Use anomaly count as precision proxy\n                anomaly_ratio = np.sum(predictions == -1) / len(predictions)\n                # Lower anomaly ratio typically means higher precision\n                precision_proxy = 1.0 - min(anomaly_ratio * 3, 0.9)  # Favor lower contamination\n\n                if precision_proxy > best_score:\n                    best_score = precision_proxy\n                    best_contamination = contamination\n\n            except Exception as e:\n                continue\n\n        return best_contamination\n\n    def fit_predict(self, X):\n        \"\"\"Fit and predict with precision optimization\"\"\"\n        try:\n            # Optimize contamination for precision\n            optimal_contamination = self.optimize_contamination_for_precision(X)\n            print(f\"Optimal contamination for precision: {optimal_contamination:.4f}\")\n\n            # Create enhanced features\n            X_features = self.create_enhanced_features(X)\n            X_scaled = self.scaler.fit_transform(X_features)\n\n            # Train Isolation Forest with precision-optimized parameters\n            self.model = IsolationForest(\n                contamination=optimal_contamination,\n                random_state=self.random_state,\n                n_estimators=150,\n                max_samples=128,  # Smaller for less overfitting\n                max_features=0.7, # Use subset of features\n                n_jobs=-1\n            )\n            self.model.fit(X_scaled)\n\n            # Get Isolation Forest predictions with STRICTER range grouping\n            iso_predictions = self.model.predict(X_scaled)\n            iso_ranges = self.find_anomaly_ranges_stricter(iso_predictions, max_gap=5, min_length=3)\n\n            # Get statistical method predictions with Z-threshold = 3.0\n            stat_ranges = self.statistical_anomaly_detection(X, z_threshold=3.0)  # KEEP: 3.0\n\n            # Combine methods but be more selective\n            all_ranges = []\n\n            # Only add Isolation Forest ranges if they're substantial\n            for start, end in iso_ranges:\n                if (end - start) >= 2:  # Minimum 3 points\n                    all_ranges.append((start, end))\n\n            # Only add Z-score ranges if they're very clear\n            for start, end in stat_ranges:\n                if (end - start) >= 2:  # Minimum 3 points for Z-score\n                    all_ranges.append((start, end))\n\n            # Merge overlapping ranges with STRICTER criteria\n            if all_ranges:\n                all_ranges.sort()\n                merged_ranges = []\n                current_start, current_end = all_ranges[0]\n\n                for start, end in all_ranges[1:]:\n                    if start <= current_end + 10:  # Smaller merge distance\n                        current_end = max(current_end, end)\n                    else:\n                        # Only keep merged range if substantial\n                        if (current_end - current_start) >= 2:\n                            merged_ranges.append((current_start, current_end))\n                        current_start, current_end = start, end\n\n                if (current_end - current_start) >= 2:\n                    merged_ranges.append((current_start, current_end))\n                self.anomaly_windows = merged_ranges\n            else:\n                self.anomaly_windows = []\n\n            print(f\"Detected {len(self.anomaly_windows)} high-confidence anomaly ranges\")\n            print(f\"Isolation Forest ranges: {len(iso_ranges)}, Z-score ranges: {len(stat_ranges)}\")\n            return self.anomaly_windows\n\n        except Exception as e:\n            print(f\"Error in fit_predict: {e}\")\n            return self.fallback_detection_precision(X)\n\n    def fallback_detection_precision(self, X):\n        \"\"\"Fallback with high precision settings\"\"\"\n        from sklearn.ensemble import IsolationForest\n\n        # Use lower contamination for higher precision\n        model = IsolationForest(\n            contamination=0.08,  # Much lower\n            random_state=self.random_state,\n            n_estimators=100\n        )\n\n        predictions = model.fit_predict(X.reshape(-1, 1))\n        anomaly_indices = np.where(predictions == -1)[0]\n\n        # Stricter range grouping\n        if len(anomaly_indices) == 0:\n            return []\n\n        ranges = []\n        start = anomaly_indices[0]\n        end = anomaly_indices[0]\n        consecutive_count = 1\n\n        for i in range(1, len(anomaly_indices)):\n            if anomaly_indices[i] <= anomaly_indices[i-1] + 5:  # Stricter\n                end = anomaly_indices[i]\n                consecutive_count += 1\n            else:\n                if consecutive_count >= 3:  # Minimum length\n                    ranges.append((start, end))\n                start = anomaly_indices[i]\n                end = anomaly_indices[i]\n                consecutive_count = 1\n\n        if consecutive_count >= 3:\n            ranges.append((start, end))\n\n        return ranges\n\n# Updated student detection function for higher precision (Z-score threshold = 3.0)\ndef student_detect_anomalies(series: np.ndarray) -> list:\n    \"\"\"\n    High-precision version using stricter parameters with Z-score = 3.0\n    \"\"\"\n    x = np.asarray(series, dtype=float)\n\n    if len(x) == 0:\n        return []\n\n    try:\n        # Use the high-precision detector\n        detector = HighPrecisionIsolationForestDetector(contamination_range=(0.05, 0.2))\n        ranges = detector.fit_predict(x)\n        return ranges\n    except Exception as e:\n        print(f\"Error in student_detect_anomalies: {e}\")\n        return student_detect_anomalies_simple(series)\n\n# Simple fallback function (keep as backup)\ndef student_detect_anomalies_simple(series: np.ndarray) -> list:\n    \"\"\"\n    Simple improved version with just higher contamination\n    \"\"\"\n    x = np.asarray(series, dtype=float)\n\n    if len(x) == 0:\n        return []\n\n    # Simple approach with significantly higher contamination\n    from sklearn.ensemble import IsolationForest\n\n    # Use much higher contamination\n    iso_forest = IsolationForest(\n        contamination=0.2,\n        random_state=42,\n        n_estimators=100\n    )\n\n    predictions = iso_forest.fit_predict(x.reshape(-1, 1))\n    anomaly_indices = np.where(predictions == -1)[0]\n\n    # Lenient range grouping with allowed gaps\n    if len(anomaly_indices) == 0:\n        return []\n\n    ranges = []\n    start = anomaly_indices[0]\n    end = anomaly_indices[0]\n\n    for i in range(1, len(anomaly_indices)):\n        if anomaly_indices[i] <= anomaly_indices[i-1] + 10:\n            end = anomaly_indices[i]\n        else:\n            ranges.append((start, end))\n            start = anomaly_indices[i]\n            end = anomaly_indices[i]\n\n    ranges.append((start, end))\n    return ranges\n\n# Keep the same evaluation function and main loop as before\n# Enhanced evaluation function with detailed range information\ndef evaluate_anomaly_detection_formatted(true_ranges, predicted_ranges, tolerance=5, file_idx=None):\n    \"\"\"\n    Evaluate anomaly detection performance with exact formatting as requested\n    \"\"\"\n    # Convert point-level predictions\n    series_length = max(\n        max([end for _, end in true_ranges]) if true_ranges else 0,\n        max([end for _, end in predicted_ranges]) if predicted_ranges else 0,\n        0\n    ) + 1\n\n    # Create point-level arrays\n    true_point_labels = np.zeros(series_length, dtype=int)\n    pred_point_labels = np.zeros(series_length, dtype=int)\n\n    # Mark true anomalies\n    for start, end in true_ranges:\n        true_point_labels[start:end+1] = 1\n\n    # Mark predicted anomalies\n    for start, end in predicted_ranges:\n        pred_point_labels[start:end+1] = 1\n\n    # Calculate point-level metrics\n    true_positives = np.sum((true_point_labels == 1) & (pred_point_labels == 1))\n    false_positives = np.sum((true_point_labels == 0) & (pred_point_labels == 1))\n    false_negatives = np.sum((true_point_labels == 1) & (pred_point_labels == 0))\n\n    # Basic metrics\n    accuracy = (true_positives + (series_length - true_positives - false_positives - false_negatives)) / series_length\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    # Range-based metrics\n    range_true_positives = 0\n    range_false_positives = 0\n    range_false_negatives = 0\n\n    # Check each predicted range against true ranges\n    matched_true_ranges = set()\n    matched_pairs = []\n\n    for pred_start, pred_end in predicted_ranges:\n        matched = False\n        for i, (true_start, true_end) in enumerate(true_ranges):\n            # Check if predicted range overlaps with true range within tolerance\n            if (pred_start <= true_end + tolerance and pred_end >= true_start - tolerance):\n                if i not in matched_true_ranges:\n                    range_true_positives += 1\n                    matched_true_ranges.add(i)\n                    matched_pairs.append((i, (pred_start, pred_end), (true_start, true_end)))\n                    matched = True\n                    break\n\n        if not matched:\n            range_false_positives += 1\n\n    range_false_negatives = len(true_ranges) - len(matched_true_ranges)\n\n    range_precision = range_true_positives / (range_true_positives + range_false_positives) if (range_true_positives + range_false_positives) > 0 else 0\n    range_recall = range_true_positives / len(true_ranges) if len(true_ranges) > 0 else 0\n    range_f1 = 2 * (range_precision * range_recall) / (range_precision + range_recall) if (range_precision + range_recall) > 0 else 0\n\n    # Segment detection metrics\n    fully_detected = 0\n    partially_detected = 0\n    missed = 0\n    detection_details = []\n\n    for i, (true_start, true_end) in enumerate(true_ranges):\n        detected_points = 0\n        overlapping_ranges = []\n\n        for pred_start, pred_end in predicted_ranges:\n            # Calculate overlap\n            overlap_start = max(true_start, pred_start)\n            overlap_end = min(true_end, pred_end)\n            if overlap_start <= overlap_end:\n                overlap_points = overlap_end - overlap_start + 1\n                detected_points += overlap_points\n                overlapping_ranges.append((pred_start, pred_end, overlap_points))\n\n        total_points = true_end - true_start + 1\n        detection_ratio = detected_points / total_points if total_points > 0 else 0\n\n        if detection_ratio >= 0.8:  # 80% threshold for full detection\n            fully_detected += 1\n            detection_status = \"FULL\"\n        elif detection_ratio > 0:   # Any detection counts as partial\n            partially_detected += 1\n            detection_status = \"PARTIAL\"\n        else:\n            missed += 1\n            detection_status = \"MISSED\"\n\n        detection_details.append({\n            'true_range': (true_start, true_end),\n            'detection_ratio': detection_ratio,\n            'status': detection_status,\n            'overlapping_ranges': overlapping_ranges,\n            'detected_points': detected_points,\n            'total_points': total_points\n        })\n\n    segment_detection_rate = fully_detected / len(true_ranges) if len(true_ranges) > 0 else 0\n    overall_detection_rate = (fully_detected + partially_detected) / len(true_ranges) if len(true_ranges) > 0 else 0\n\n    # Print results in the exact format requested\n    print(f\"EVALUATION RESULTS - FILE {file_idx}\")\n    print(\"=\" * 60)\n\n    # Display range information\n    print(\"Range Information:\")\n    print(f\"  True Ranges: {true_ranges} (count: {len(true_ranges)})\")\n    print(f\"  Predicted Ranges: {predicted_ranges} (count: {len(predicted_ranges)})\")\n    print()\n\n    # Display detection details for each true range\n    if detection_details:\n        print(\"Detection Details by Segment:\")\n        for i, detail in enumerate(detection_details):\n            status_symbol = \"✓\" if detail['status'] == \"FULL\" else \"~\" if detail['status'] == \"PARTIAL\" else \"✗\"\n            print(f\"  Segment {i}: {detail['true_range']} {status_symbol} {detail['status']} \"\n                  f\"({detail['detected_points']}/{detail['total_points']} points, \"\n                  f\"{detail['detection_ratio']:.1%})\")\n            if detail['overlapping_ranges']:\n                for pred_range in detail['overlapping_ranges']:\n                    print(f\"    → Overlap with predicted range {pred_range[0:2]}: {pred_range[2]} points\")\n        print()\n\n    print(\"Basic Metrics:\")\n    print(f\"  Accuracy:  {accuracy:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall:    {recall:.4f}\")\n    print(f\"  F1-Score:  {f1:.4f}\")\n    print()\n    print(\"Anomaly Statistics:\")\n    print(f\"  True Anomalies: {np.sum(true_point_labels)} points\")\n    print(f\"  Predicted Anomalies: {np.sum(pred_point_labels)} points\")\n    print(f\"  Detection Rate: {recall:.4f}\")\n    print()\n    print(\"Range-based Metrics:\")\n    print(f\"  Range Precision: {range_precision:.4f}\")\n    print(f\"  Range Recall:    {range_recall:.4f}\")\n    print(f\"  Range F1:        {range_f1:.4f}\")\n    print()\n    print(\"Point-level Performance:\")\n    print(f\"  True Positives:  {true_positives} points\")\n    print(f\"  False Positives: {false_positives} points\")\n    print(f\"  False Negatives: {false_negatives} points\")\n    print()\n    print(\"Segment Detection Metrics:\")\n    print(f\"  Total Anomaly Segments: {len(true_ranges)}\")\n    print(f\"  Fully Detected: {fully_detected}\")\n    print(f\"  Partially Detected: {partially_detected}\")\n    print(f\"  Missed: {missed}\")\n    print(f\"  Segment Detection Rate: {segment_detection_rate:.4f}\")\n    print(f\"  Overall Detection Rate: {overall_detection_rate:.4f}\")\n    print()\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'true_positives': true_positives,\n        'false_positives': false_positives,\n        'false_negatives': false_negatives,\n        'range_precision': range_precision,\n        'range_recall': range_recall,\n        'range_f1': range_f1,\n        'fully_detected': fully_detected,\n        'partially_detected': partially_detected,\n        'missed': missed,\n        'segment_detection_rate': segment_detection_rate,\n        'overall_detection_rate': overall_detection_rate,\n        'true_ranges': true_ranges,\n        'predicted_ranges': predicted_ranges,\n        'detection_details': detection_details,\n        'matched_pairs': matched_pairs\n    }\n\n# Updated main evaluation loop with high-precision settings (Z-score threshold = 3.0)\nprint(\"Starting HIGH-PRECISION Anomaly Detection Evaluation\")\nprint(\"Key improvements for higher precision:\")\nprint(\"- Contamination range: 0.05-0.2 (reduced from 0.1-0.4)\")\nprint(\"- Z-score threshold: 3.0 (maintained)\")\nprint(\"- Stricter range grouping (max_gap=5, min_length=3)\")\nprint(\"- Anti-overfitting measures (more trees, smaller samples)\")\nprint(\"- More selective range merging and combination\")\nprint(\"=\" * 80)\n\ntotal_files = len(test_files)\nall_metrics = []\ndetailed_results = []\n\nfor idx, test_df in enumerate(test_files):\n    print(f\"\\n{'='*80}\")\n\n    try:\n        # Extract series and labels\n        series = test_df['Value'].values if 'Value' in test_df.columns else test_df.iloc[:, 0].values\n        labels = test_df['Labels'].values if 'Labels' in test_df.columns else test_df.iloc[:, 1].values\n\n        # Get true anomaly ranges\n        true_ranges = extract_true_anomaly_ranges(labels)\n\n        # Detect anomalies using HIGH-PRECISION student's function\n        predicted_ranges = student_detect_anomalies(series)\n\n        # Evaluate with formatted output\n        metrics = evaluate_anomaly_detection_formatted(true_ranges, predicted_ranges, file_idx=idx)\n        all_metrics.append(metrics)\n\n        # Store detailed results\n        detailed_results.append({\n            'file_idx': idx,\n            'true_ranges': true_ranges,\n            'predicted_ranges': predicted_ranges,\n            'metrics': metrics\n        })\n\n    except Exception as e:\n        print(f\"Error processing file {idx}: {e}\")\n        import traceback\n        traceback.print_exc()\n        # Add zero metrics for failed files\n        all_metrics.append({\n            'accuracy': 0,\n            'precision': 0,\n            'recall': 0,\n            'f1_score': 0,\n            'true_positives': 0,\n            'false_positives': 0,\n            'false_negatives': 0,\n            'range_precision': 0,\n            'range_recall': 0,\n            'range_f1': 0,\n            'fully_detected': 0,\n            'partially_detected': 0,\n            'missed': 0,\n            'segment_detection_rate': 0,\n            'overall_detection_rate': 0,\n            'true_ranges': [],\n            'predicted_ranges': [],\n            'detection_details': [],\n            'matched_pairs': []\n        })\n\n# Calculate overall summary statistics\nprint(\"\\n\" + \"=\" * 80)\nprint(\"OVERALL SUMMARY ACROSS ALL FILES - HIGH-PRECISION VERSION\")\nprint(\"=\" * 80)\n\n# Calculate basic metrics\noverall_accuracy = np.mean([m['accuracy'] for m in all_metrics])\noverall_precision = np.mean([m['precision'] for m in all_metrics])\noverall_recall = np.mean([m['recall'] for m in all_metrics])\noverall_f1 = np.mean([m['f1_score'] for m in all_metrics])\n\n# Calculate range statistics\ntotal_true_ranges = sum([len(m['true_ranges']) for m in all_metrics])\ntotal_predicted_ranges = sum([len(m['predicted_ranges']) for m in all_metrics])\ntotal_matched_ranges = sum([len(m['matched_pairs']) for m in all_metrics])\n\n# Calculate other metrics\noverall_range_precision = np.mean([m['range_precision'] for m in all_metrics])\noverall_range_recall = np.mean([m['range_recall'] for m in all_metrics])\noverall_range_f1 = np.mean([m['range_f1'] for m in all_metrics])\n\ntotal_true_positives = sum([m['true_positives'] for m in all_metrics])\ntotal_false_positives = sum([m['false_positives'] for m in all_metrics])\ntotal_false_negatives = sum([m['false_negatives'] for m in all_metrics])\n\ntotal_fully_detected = sum([m['fully_detected'] for m in all_metrics])\ntotal_partially_detected = sum([m['partially_detected'] for m in all_metrics])\ntotal_missed = sum([m['missed'] for m in all_metrics])\ntotal_segments = total_fully_detected + total_partially_detected + total_missed\n\noverall_segment_detection_rate = total_fully_detected / total_segments if total_segments > 0 else 0\noverall_detection_rate = (total_fully_detected + total_partially_detected) / total_segments if total_segments > 0 else 0\n\nprint(\"Overall Basic Metrics:\")\nprint(f\"  Accuracy:  {overall_accuracy:.4f}\")\nprint(f\"  Precision: {overall_precision:.4f}\")\nprint(f\"  Recall:    {overall_recall:.4f}\")\nprint(f\"  F1-Score:  {overall_f1:.4f}\")\nprint()\nprint(\"Overall Range Statistics:\")\nprint(f\"  Total True Ranges: {total_true_ranges}\")\nprint(f\"  Total Predicted Ranges: {total_predicted_ranges}\")\nprint(f\"  Total Matched Ranges: {total_matched_ranges}\")\nprint(f\"  Range Match Rate: {total_matched_ranges/total_true_ranges:.4f}\" if total_true_ranges > 0 else \"  Range Match Rate: 0.0000\")\nprint()\nprint(\"Overall Range-based Metrics:\")\nprint(f\"  Range Precision: {overall_range_precision:.4f}\")\nprint(f\"  Range Recall:    {overall_range_recall:.4f}\")\nprint(f\"  Range F1:        {overall_range_f1:.4f}\")\nprint()\nprint(\"Overall Point-level Performance:\")\nprint(f\"  True Positives:  {total_true_positives} points\")\nprint(f\"  False Positives: {total_false_positives} points\")\nprint(f\"  False Negatives: {total_false_negatives} points\")\nprint()\nprint(\"Overall Segment Detection Metrics:\")\nprint(f\"  Total Anomaly Segments: {total_segments}\")\nprint(f\"  Fully Detected: {total_fully_detected}\")\nprint(f\"  Partially Detected: {total_partially_detected}\")\nprint(f\"  Missed: {total_missed}\")\nprint(f\"  Segment Detection Rate: {overall_segment_detection_rate:.4f}\")\nprint(f\"  Overall Detection Rate: {overall_detection_rate:.4f}\")\n\n# File-by-file summary table\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FILE-BY-FILE SUMMARY - HIGH-PRECISION VERSION\")\nprint(\"=\" * 80)\nprint(f\"{'File':<6} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'True Ranges':<12} {'Pred Ranges':<12} {'Fully Det.':<12} {'Part. Det.':<12} {'Missed':<8}\")\nprint(\"-\" * 100)\n\nfor idx, metrics in enumerate(all_metrics):\n    true_range_count = len(metrics['true_ranges'])\n    pred_range_count = len(metrics['predicted_ranges'])\n\n    print(f\"{idx:<6} {metrics['accuracy']:.4f}    {metrics['precision']:.4f}     {metrics['recall']:.4f}     {metrics['f1_score']:.4f}     \"\n          f\"{true_range_count:<11} {pred_range_count:<11} {metrics['fully_detected']:<11} {metrics['partially_detected']:<11} {metrics['missed']:<8}\")\n\nprint(\"-\" * 100)\n\n# Detailed range information for each file\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DETAILED RANGE INFORMATION BY FILE\")\nprint(\"=\" * 80)\n\nfor result in detailed_results:\n    if result['metrics']['true_ranges'] or result['metrics']['predicted_ranges']:\n        print(f\"\\nFile {result['file_idx']}:\")\n        print(f\"  True Ranges: {result['true_ranges']} (count: {len(result['true_ranges'])})\")\n        print(f\"  Predicted Ranges: {result['predicted_ranges']} (count: {len(result['predicted_ranges'])})\")\n\n        # Show matching information\n        if result['metrics']['matched_pairs']:\n            print(f\"  Matched Pairs:\")\n            for match in result['metrics']['matched_pairs']:\n                true_idx, pred_range, true_range = match\n                print(f\"    True Range {true_idx} {true_range} ↔ Predicted {pred_range}\")\n\nprint(\"\\nEvaluation completed for all files with high-precision detection methods!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting HIGH-PRECISION Anomaly Detection Evaluation\nKey improvements for higher precision:\n- Contamination range: 0.05-0.2 (reduced from 0.1-0.4)\n- Z-score threshold: 3.0 (maintained)\n- Stricter range grouping (max_gap=5, min_length=3)\n- Anti-overfitting measures (more trees, smaller samples)\n- More selective range merging and combination\n================================================================================\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 16 high-confidence anomaly ranges\nIsolation Forest ranges: 19, Z-score ranges: 3\nEVALUATION RESULTS - FILE 0\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2533), np.int64(2620)), (np.int64(2864), np.int64(2949)), (np.int64(4811), np.int64(4910)), (np.int64(5148), np.int64(5304)), (np.int64(5492), np.int64(5619)), (np.int64(5986), np.int64(6016)), (np.int64(6222), np.int64(6257)), (np.int64(7088), np.int64(7117)), (np.int64(7599), np.int64(7716)), (np.int64(8633), np.int64(8750))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2533), np.int64(2617)), (np.int64(2860), np.int64(2954)), (np.int64(4808), np.int64(4817)), (np.int64(4900), np.int64(4912)), (np.int64(5236), np.int64(5245)), (np.int64(5291), np.int64(5308)), (np.int64(5491), np.int64(5503)), (np.int64(5608), np.int64(5624)), (np.int64(5982), np.int64(6017)), (np.int64(6219), np.int64(6258)), (np.int64(7084), np.int64(7122)), (np.int64(7595), np.int64(7613)), (np.int64(7711), np.int64(7718)), (np.int64(8628), np.int64(8756)), (np.int64(9986), np.int64(9999))] (count: 16)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2533), np.int64(2620)) ✓ FULL (85/88 points, 96.6%)\n    → Overlap with predicted range (np.int64(2533), np.int64(2617)): 85 points\n  Segment 1: (np.int64(2864), np.int64(2949)) ✓ FULL (86/86 points, 100.0%)\n    → Overlap with predicted range (np.int64(2860), np.int64(2954)): 86 points\n  Segment 2: (np.int64(4811), np.int64(4910)) ~ PARTIAL (18/100 points, 18.0%)\n    → Overlap with predicted range (np.int64(4808), np.int64(4817)): 7 points\n    → Overlap with predicted range (np.int64(4900), np.int64(4912)): 11 points\n  Segment 3: (np.int64(5148), np.int64(5304)) ~ PARTIAL (24/157 points, 15.3%)\n    → Overlap with predicted range (np.int64(5236), np.int64(5245)): 10 points\n    → Overlap with predicted range (np.int64(5291), np.int64(5308)): 14 points\n  Segment 4: (np.int64(5492), np.int64(5619)) ~ PARTIAL (24/128 points, 18.8%)\n    → Overlap with predicted range (np.int64(5491), np.int64(5503)): 12 points\n    → Overlap with predicted range (np.int64(5608), np.int64(5624)): 12 points\n  Segment 5: (np.int64(5986), np.int64(6016)) ✓ FULL (31/31 points, 100.0%)\n    → Overlap with predicted range (np.int64(5982), np.int64(6017)): 31 points\n  Segment 6: (np.int64(6222), np.int64(6257)) ✓ FULL (36/36 points, 100.0%)\n    → Overlap with predicted range (np.int64(6219), np.int64(6258)): 36 points\n  Segment 7: (np.int64(7088), np.int64(7117)) ✓ FULL (30/30 points, 100.0%)\n    → Overlap with predicted range (np.int64(7084), np.int64(7122)): 30 points\n  Segment 8: (np.int64(7599), np.int64(7716)) ~ PARTIAL (21/118 points, 17.8%)\n    → Overlap with predicted range (np.int64(7595), np.int64(7613)): 15 points\n    → Overlap with predicted range (np.int64(7711), np.int64(7718)): 6 points\n  Segment 9: (np.int64(8633), np.int64(8750)) ✓ FULL (118/118 points, 100.0%)\n    → Overlap with predicted range (np.int64(8628), np.int64(8756)): 118 points\n\nBasic Metrics:\n  Accuracy:  0.9498\n  Precision: 0.8507\n  Recall:    0.5303\n  F1-Score:  0.6533\n\nAnomaly Statistics:\n  True Anomalies: 892 points\n  Predicted Anomalies: 556 points\n  Detection Rate: 0.5303\n\nRange-based Metrics:\n  Range Precision: 0.6250\n  Range Recall:    1.0000\n  Range F1:        0.7692\n\nPoint-level Performance:\n  True Positives:  473 points\n  False Positives: 83 points\n  False Negatives: 419 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 6\n  Partially Detected: 4\n  Missed: 0\n  Segment Detection Rate: 0.6000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 16 high-confidence anomaly ranges\nIsolation Forest ranges: 23, Z-score ranges: 8\nEVALUATION RESULTS - FILE 1\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2206), np.int64(2325)), (np.int64(2864), np.int64(2978)), (np.int64(3483), np.int64(3565)), (np.int64(3834), np.int64(3926)), (np.int64(4758), np.int64(4878)), (np.int64(6850), np.int64(6957)), (np.int64(7852), np.int64(7954)), (np.int64(8319), np.int64(8428)), (np.int64(9112), np.int64(9225)), (np.int64(9372), np.int64(9471))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2205), np.int64(2327)), (np.int64(2930), np.int64(2981)), (np.int64(3482), np.int64(3567)), (np.int64(3830), np.int64(3931)), (np.int64(4757), np.int64(4882)), (np.int64(6856), np.int64(6862)), (np.int64(6904), np.int64(6910)), (np.int64(6928), np.int64(6934)), (np.int64(7901), np.int64(7906)), (np.int64(7918), np.int64(7959)), (np.int64(8377), np.int64(8430)), (np.int64(9160), np.int64(9166)), (np.int64(9184), np.int64(9190)), (np.int64(9391), np.int64(9476)), (np.int64(9990), np.int64(9999))] (count: 16)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2206), np.int64(2325)) ✓ FULL (120/120 points, 100.0%)\n    → Overlap with predicted range (np.int64(2205), np.int64(2327)): 120 points\n  Segment 1: (np.int64(2864), np.int64(2978)) ~ PARTIAL (49/115 points, 42.6%)\n    → Overlap with predicted range (np.int64(2930), np.int64(2981)): 49 points\n  Segment 2: (np.int64(3483), np.int64(3565)) ✓ FULL (83/83 points, 100.0%)\n    → Overlap with predicted range (np.int64(3482), np.int64(3567)): 83 points\n  Segment 3: (np.int64(3834), np.int64(3926)) ✓ FULL (93/93 points, 100.0%)\n    → Overlap with predicted range (np.int64(3830), np.int64(3931)): 93 points\n  Segment 4: (np.int64(4758), np.int64(4878)) ✓ FULL (121/121 points, 100.0%)\n    → Overlap with predicted range (np.int64(4757), np.int64(4882)): 121 points\n  Segment 5: (np.int64(6850), np.int64(6957)) ~ PARTIAL (21/108 points, 19.4%)\n    → Overlap with predicted range (np.int64(6856), np.int64(6862)): 7 points\n    → Overlap with predicted range (np.int64(6904), np.int64(6910)): 7 points\n    → Overlap with predicted range (np.int64(6928), np.int64(6934)): 7 points\n  Segment 6: (np.int64(7852), np.int64(7954)) ~ PARTIAL (43/103 points, 41.7%)\n    → Overlap with predicted range (np.int64(7901), np.int64(7906)): 6 points\n    → Overlap with predicted range (np.int64(7918), np.int64(7959)): 37 points\n  Segment 7: (np.int64(8319), np.int64(8428)) ~ PARTIAL (52/110 points, 47.3%)\n    → Overlap with predicted range (np.int64(8377), np.int64(8430)): 52 points\n  Segment 8: (np.int64(9112), np.int64(9225)) ~ PARTIAL (14/114 points, 12.3%)\n    → Overlap with predicted range (np.int64(9160), np.int64(9166)): 7 points\n    → Overlap with predicted range (np.int64(9184), np.int64(9190)): 7 points\n  Segment 9: (np.int64(9372), np.int64(9471)) ✓ FULL (81/100 points, 81.0%)\n    → Overlap with predicted range (np.int64(9391), np.int64(9476)): 81 points\n\nBasic Metrics:\n  Accuracy:  0.9555\n  Precision: 0.9249\n  Recall:    0.6345\n  F1-Score:  0.7526\n\nAnomaly Statistics:\n  True Anomalies: 1067 points\n  Predicted Anomalies: 732 points\n  Detection Rate: 0.6345\n\nRange-based Metrics:\n  Range Precision: 0.6250\n  Range Recall:    1.0000\n  Range F1:        0.7692\n\nPoint-level Performance:\n  True Positives:  677 points\n  False Positives: 55 points\n  False Negatives: 390 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 5\n  Partially Detected: 5\n  Missed: 0\n  Segment Detection Rate: 0.5000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 19 high-confidence anomaly ranges\nIsolation Forest ranges: 19, Z-score ranges: 9\nEVALUATION RESULTS - FILE 2\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2657), np.int64(2755)), (np.int64(3755), np.int64(3860)), (np.int64(3991), np.int64(4011)), (np.int64(4429), np.int64(4558)), (np.int64(4968), np.int64(5070)), (np.int64(6624), np.int64(6713)), (np.int64(7931), np.int64(8008)), (np.int64(8129), np.int64(8159)), (np.int64(9307), np.int64(9393)), (np.int64(9752), np.int64(9790))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2681), np.int64(2687)), (np.int64(2699), np.int64(2711)), (np.int64(2729), np.int64(2735)), (np.int64(2747), np.int64(2755)), (np.int64(3761), np.int64(3767)), (np.int64(3785), np.int64(3791)), (np.int64(3803), np.int64(3815)), (np.int64(3833), np.int64(3839)), (np.int64(3988), np.int64(4013)), (np.int64(4487), np.int64(4562)), (np.int64(4964), np.int64(5075)), (np.int64(6655), np.int64(6718)), (np.int64(7961), np.int64(7967)), (np.int64(7979), np.int64(7991)), (np.int64(8126), np.int64(8163)), (np.int64(9350), np.int64(9397)), (np.int64(9749), np.int64(9795)), (np.int64(9989), np.int64(9999))] (count: 19)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2657), np.int64(2755)) ~ PARTIAL (36/99 points, 36.4%)\n    → Overlap with predicted range (np.int64(2681), np.int64(2687)): 7 points\n    → Overlap with predicted range (np.int64(2699), np.int64(2711)): 13 points\n    → Overlap with predicted range (np.int64(2729), np.int64(2735)): 7 points\n    → Overlap with predicted range (np.int64(2747), np.int64(2755)): 9 points\n  Segment 1: (np.int64(3755), np.int64(3860)) ~ PARTIAL (34/106 points, 32.1%)\n    → Overlap with predicted range (np.int64(3761), np.int64(3767)): 7 points\n    → Overlap with predicted range (np.int64(3785), np.int64(3791)): 7 points\n    → Overlap with predicted range (np.int64(3803), np.int64(3815)): 13 points\n    → Overlap with predicted range (np.int64(3833), np.int64(3839)): 7 points\n  Segment 2: (np.int64(3991), np.int64(4011)) ✓ FULL (21/21 points, 100.0%)\n    → Overlap with predicted range (np.int64(3988), np.int64(4013)): 21 points\n  Segment 3: (np.int64(4429), np.int64(4558)) ~ PARTIAL (72/130 points, 55.4%)\n    → Overlap with predicted range (np.int64(4487), np.int64(4562)): 72 points\n  Segment 4: (np.int64(4968), np.int64(5070)) ✓ FULL (103/103 points, 100.0%)\n    → Overlap with predicted range (np.int64(4964), np.int64(5075)): 103 points\n  Segment 5: (np.int64(6624), np.int64(6713)) ~ PARTIAL (59/90 points, 65.6%)\n    → Overlap with predicted range (np.int64(6655), np.int64(6718)): 59 points\n  Segment 6: (np.int64(7931), np.int64(8008)) ~ PARTIAL (20/78 points, 25.6%)\n    → Overlap with predicted range (np.int64(7961), np.int64(7967)): 7 points\n    → Overlap with predicted range (np.int64(7979), np.int64(7991)): 13 points\n  Segment 7: (np.int64(8129), np.int64(8159)) ✓ FULL (31/31 points, 100.0%)\n    → Overlap with predicted range (np.int64(8126), np.int64(8163)): 31 points\n  Segment 8: (np.int64(9307), np.int64(9393)) ~ PARTIAL (44/87 points, 50.6%)\n    → Overlap with predicted range (np.int64(9350), np.int64(9397)): 44 points\n  Segment 9: (np.int64(9752), np.int64(9790)) ✓ FULL (39/39 points, 100.0%)\n    → Overlap with predicted range (np.int64(9749), np.int64(9795)): 39 points\n\nBasic Metrics:\n  Accuracy:  0.9612\n  Precision: 0.8793\n  Recall:    0.5855\n  F1-Score:  0.7029\n\nAnomaly Statistics:\n  True Anomalies: 784 points\n  Predicted Anomalies: 522 points\n  Detection Rate: 0.5855\n\nRange-based Metrics:\n  Range Precision: 0.5263\n  Range Recall:    1.0000\n  Range F1:        0.6897\n\nPoint-level Performance:\n  True Positives:  459 points\n  False Positives: 63 points\n  False Negatives: 325 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 4\n  Partially Detected: 6\n  Missed: 0\n  Segment Detection Rate: 0.4000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 20 high-confidence anomaly ranges\nIsolation Forest ranges: 21, Z-score ranges: 3\nEVALUATION RESULTS - FILE 3\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2240), np.int64(2338)), (np.int64(2824), np.int64(2912)), (np.int64(3685), np.int64(3792)), (np.int64(4212), np.int64(4244)), (np.int64(5007), np.int64(5029)), (np.int64(5758), np.int64(5870)), (np.int64(6665), np.int64(6777)), (np.int64(8492), np.int64(8521)), (np.int64(9099), np.int64(9197)), (np.int64(9628), np.int64(9724))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(24)), (np.int64(2282), np.int64(2291)), (np.int64(2334), np.int64(2342)), (np.int64(2824), np.int64(2909)), (np.int64(3681), np.int64(3689)), (np.int64(3732), np.int64(3744)), (np.int64(3787), np.int64(3797)), (np.int64(4210), np.int64(4248)), (np.int64(5003), np.int64(5034)), (np.int64(5755), np.int64(5762)), (np.int64(5804), np.int64(5824)), (np.int64(5865), np.int64(5874)), (np.int64(6661), np.int64(6678)), (np.int64(6713), np.int64(6729)), (np.int64(6772), np.int64(6781)), (np.int64(8486), np.int64(8527)), (np.int64(9137), np.int64(9155)), (np.int64(9184), np.int64(9202)), (np.int64(9628), np.int64(9725)), (np.int64(9976), np.int64(9999))] (count: 20)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2240), np.int64(2338)) ~ PARTIAL (15/99 points, 15.2%)\n    → Overlap with predicted range (np.int64(2282), np.int64(2291)): 10 points\n    → Overlap with predicted range (np.int64(2334), np.int64(2342)): 5 points\n  Segment 1: (np.int64(2824), np.int64(2912)) ✓ FULL (86/89 points, 96.6%)\n    → Overlap with predicted range (np.int64(2824), np.int64(2909)): 86 points\n  Segment 2: (np.int64(3685), np.int64(3792)) ~ PARTIAL (24/108 points, 22.2%)\n    → Overlap with predicted range (np.int64(3681), np.int64(3689)): 5 points\n    → Overlap with predicted range (np.int64(3732), np.int64(3744)): 13 points\n    → Overlap with predicted range (np.int64(3787), np.int64(3797)): 6 points\n  Segment 3: (np.int64(4212), np.int64(4244)) ✓ FULL (33/33 points, 100.0%)\n    → Overlap with predicted range (np.int64(4210), np.int64(4248)): 33 points\n  Segment 4: (np.int64(5007), np.int64(5029)) ✓ FULL (23/23 points, 100.0%)\n    → Overlap with predicted range (np.int64(5003), np.int64(5034)): 23 points\n  Segment 5: (np.int64(5758), np.int64(5870)) ~ PARTIAL (32/113 points, 28.3%)\n    → Overlap with predicted range (np.int64(5755), np.int64(5762)): 5 points\n    → Overlap with predicted range (np.int64(5804), np.int64(5824)): 21 points\n    → Overlap with predicted range (np.int64(5865), np.int64(5874)): 6 points\n  Segment 6: (np.int64(6665), np.int64(6777)) ~ PARTIAL (37/113 points, 32.7%)\n    → Overlap with predicted range (np.int64(6661), np.int64(6678)): 14 points\n    → Overlap with predicted range (np.int64(6713), np.int64(6729)): 17 points\n    → Overlap with predicted range (np.int64(6772), np.int64(6781)): 6 points\n  Segment 7: (np.int64(8492), np.int64(8521)) ✓ FULL (30/30 points, 100.0%)\n    → Overlap with predicted range (np.int64(8486), np.int64(8527)): 30 points\n  Segment 8: (np.int64(9099), np.int64(9197)) ~ PARTIAL (33/99 points, 33.3%)\n    → Overlap with predicted range (np.int64(9137), np.int64(9155)): 19 points\n    → Overlap with predicted range (np.int64(9184), np.int64(9202)): 14 points\n  Segment 9: (np.int64(9628), np.int64(9724)) ✓ FULL (97/97 points, 100.0%)\n    → Overlap with predicted range (np.int64(9628), np.int64(9725)): 97 points\n\nBasic Metrics:\n  Accuracy:  0.9496\n  Precision: 0.7885\n  Recall:    0.5100\n  F1-Score:  0.6193\n\nAnomaly Statistics:\n  True Anomalies: 804 points\n  Predicted Anomalies: 520 points\n  Detection Rate: 0.5100\n\nRange-based Metrics:\n  Range Precision: 0.5000\n  Range Recall:    1.0000\n  Range F1:        0.6667\n\nPoint-level Performance:\n  True Positives:  410 points\n  False Positives: 110 points\n  False Negatives: 394 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 5\n  Partially Detected: 5\n  Missed: 0\n  Segment Detection Rate: 0.5000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 12 high-confidence anomaly ranges\nIsolation Forest ranges: 15, Z-score ranges: 9\nEVALUATION RESULTS - FILE 4\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2229), np.int64(2272)), (np.int64(2744), np.int64(2840)), (np.int64(3010), np.int64(3034)), (np.int64(3761), np.int64(3783)), (np.int64(5028), np.int64(5065)), (np.int64(6270), np.int64(6304)), (np.int64(8383), np.int64(8422)), (np.int64(8650), np.int64(8730)), (np.int64(9602), np.int64(9627)), (np.int64(9862), np.int64(9960))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2225), np.int64(2276)), (np.int64(2740), np.int64(2845)), (np.int64(3008), np.int64(3038)), (np.int64(3759), np.int64(3786)), (np.int64(5025), np.int64(5068)), (np.int64(6267), np.int64(6305)), (np.int64(8380), np.int64(8427)), (np.int64(8685), np.int64(8735)), (np.int64(9599), np.int64(9632)), (np.int64(9874), np.int64(9960)), (np.int64(9990), np.int64(9999))] (count: 12)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2229), np.int64(2272)) ✓ FULL (44/44 points, 100.0%)\n    → Overlap with predicted range (np.int64(2225), np.int64(2276)): 44 points\n  Segment 1: (np.int64(2744), np.int64(2840)) ✓ FULL (97/97 points, 100.0%)\n    → Overlap with predicted range (np.int64(2740), np.int64(2845)): 97 points\n  Segment 2: (np.int64(3010), np.int64(3034)) ✓ FULL (25/25 points, 100.0%)\n    → Overlap with predicted range (np.int64(3008), np.int64(3038)): 25 points\n  Segment 3: (np.int64(3761), np.int64(3783)) ✓ FULL (23/23 points, 100.0%)\n    → Overlap with predicted range (np.int64(3759), np.int64(3786)): 23 points\n  Segment 4: (np.int64(5028), np.int64(5065)) ✓ FULL (38/38 points, 100.0%)\n    → Overlap with predicted range (np.int64(5025), np.int64(5068)): 38 points\n  Segment 5: (np.int64(6270), np.int64(6304)) ✓ FULL (35/35 points, 100.0%)\n    → Overlap with predicted range (np.int64(6267), np.int64(6305)): 35 points\n  Segment 6: (np.int64(8383), np.int64(8422)) ✓ FULL (40/40 points, 100.0%)\n    → Overlap with predicted range (np.int64(8380), np.int64(8427)): 40 points\n  Segment 7: (np.int64(8650), np.int64(8730)) ~ PARTIAL (46/81 points, 56.8%)\n    → Overlap with predicted range (np.int64(8685), np.int64(8735)): 46 points\n  Segment 8: (np.int64(9602), np.int64(9627)) ✓ FULL (26/26 points, 100.0%)\n    → Overlap with predicted range (np.int64(9599), np.int64(9632)): 26 points\n  Segment 9: (np.int64(9862), np.int64(9960)) ✓ FULL (87/99 points, 87.9%)\n    → Overlap with predicted range (np.int64(9874), np.int64(9960)): 87 points\n\nBasic Metrics:\n  Accuracy:  0.9874\n  Precision: 0.8537\n  Recall:    0.9075\n  F1-Score:  0.8798\n\nAnomaly Statistics:\n  True Anomalies: 508 points\n  Predicted Anomalies: 540 points\n  Detection Rate: 0.9075\n\nRange-based Metrics:\n  Range Precision: 0.8333\n  Range Recall:    1.0000\n  Range F1:        0.9091\n\nPoint-level Performance:\n  True Positives:  461 points\n  False Positives: 79 points\n  False Negatives: 47 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 9\n  Partially Detected: 1\n  Missed: 0\n  Segment Detection Rate: 0.9000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 21 high-confidence anomaly ranges\nIsolation Forest ranges: 23, Z-score ranges: 7\nEVALUATION RESULTS - FILE 5\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2661), np.int64(2775)), (np.int64(2863), np.int64(2894)), (np.int64(3006), np.int64(3124)), (np.int64(4118), np.int64(4269)), (np.int64(4463), np.int64(4591)), (np.int64(5953), np.int64(5994)), (np.int64(6107), np.int64(6225)), (np.int64(6429), np.int64(6577)), (np.int64(7969), np.int64(8094)), (np.int64(8886), np.int64(8906))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2679), np.int64(2685)), (np.int64(2703), np.int64(2715)), (np.int64(2727), np.int64(2733)), (np.int64(2863), np.int64(2897)), (np.int64(3004), np.int64(3010)), (np.int64(3056), np.int64(3075)), (np.int64(3121), np.int64(3127)), (np.int64(4180), np.int64(4274)), (np.int64(4463), np.int64(4466)), (np.int64(4513), np.int64(4542)), (np.int64(4588), np.int64(4594)), (np.int64(5950), np.int64(5998)), (np.int64(6137), np.int64(6167)), (np.int64(6179), np.int64(6215)), (np.int64(6484), np.int64(6581)), (np.int64(7968), np.int64(7979)), (np.int64(8019), np.int64(8045)), (np.int64(8083), np.int64(8098)), (np.int64(8885), np.int64(8909)), (np.int64(9984), np.int64(9999))] (count: 21)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2661), np.int64(2775)) ~ PARTIAL (27/115 points, 23.5%)\n    → Overlap with predicted range (np.int64(2679), np.int64(2685)): 7 points\n    → Overlap with predicted range (np.int64(2703), np.int64(2715)): 13 points\n    → Overlap with predicted range (np.int64(2727), np.int64(2733)): 7 points\n  Segment 1: (np.int64(2863), np.int64(2894)) ✓ FULL (32/32 points, 100.0%)\n    → Overlap with predicted range (np.int64(2863), np.int64(2897)): 32 points\n  Segment 2: (np.int64(3006), np.int64(3124)) ~ PARTIAL (29/119 points, 24.4%)\n    → Overlap with predicted range (np.int64(3004), np.int64(3010)): 5 points\n    → Overlap with predicted range (np.int64(3056), np.int64(3075)): 20 points\n    → Overlap with predicted range (np.int64(3121), np.int64(3127)): 4 points\n  Segment 3: (np.int64(4118), np.int64(4269)) ~ PARTIAL (90/152 points, 59.2%)\n    → Overlap with predicted range (np.int64(4180), np.int64(4274)): 90 points\n  Segment 4: (np.int64(4463), np.int64(4591)) ~ PARTIAL (38/129 points, 29.5%)\n    → Overlap with predicted range (np.int64(4463), np.int64(4466)): 4 points\n    → Overlap with predicted range (np.int64(4513), np.int64(4542)): 30 points\n    → Overlap with predicted range (np.int64(4588), np.int64(4594)): 4 points\n  Segment 5: (np.int64(5953), np.int64(5994)) ✓ FULL (42/42 points, 100.0%)\n    → Overlap with predicted range (np.int64(5950), np.int64(5998)): 42 points\n  Segment 6: (np.int64(6107), np.int64(6225)) ~ PARTIAL (68/119 points, 57.1%)\n    → Overlap with predicted range (np.int64(6137), np.int64(6167)): 31 points\n    → Overlap with predicted range (np.int64(6179), np.int64(6215)): 37 points\n  Segment 7: (np.int64(6429), np.int64(6577)) ~ PARTIAL (94/149 points, 63.1%)\n    → Overlap with predicted range (np.int64(6484), np.int64(6581)): 94 points\n  Segment 8: (np.int64(7969), np.int64(8094)) ~ PARTIAL (50/126 points, 39.7%)\n    → Overlap with predicted range (np.int64(7968), np.int64(7979)): 11 points\n    → Overlap with predicted range (np.int64(8019), np.int64(8045)): 27 points\n    → Overlap with predicted range (np.int64(8083), np.int64(8098)): 12 points\n  Segment 9: (np.int64(8886), np.int64(8906)) ✓ FULL (21/21 points, 100.0%)\n    → Overlap with predicted range (np.int64(8885), np.int64(8909)): 21 points\n\nBasic Metrics:\n  Accuracy:  0.9425\n  Precision: 0.8879\n  Recall:    0.4890\n  F1-Score:  0.6307\n\nAnomaly Statistics:\n  True Anomalies: 1004 points\n  Predicted Anomalies: 553 points\n  Detection Rate: 0.4890\n\nRange-based Metrics:\n  Range Precision: 0.4762\n  Range Recall:    1.0000\n  Range F1:        0.6452\n\nPoint-level Performance:\n  True Positives:  491 points\n  False Positives: 62 points\n  False Negatives: 513 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 3\n  Partially Detected: 7\n  Missed: 0\n  Segment Detection Rate: 0.3000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 13 high-confidence anomaly ranges\nIsolation Forest ranges: 23, Z-score ranges: 9\nEVALUATION RESULTS - FILE 6\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2543), np.int64(2670)), (np.int64(3004), np.int64(3087)), (np.int64(3766), np.int64(3826)), (np.int64(5188), np.int64(5304)), (np.int64(5882), np.int64(5908)), (np.int64(6584), np.int64(6609)), (np.int64(7041), np.int64(7143)), (np.int64(7441), np.int64(7539)), (np.int64(9294), np.int64(9429)), (np.int64(9817), np.int64(9846))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2541), np.int64(2672)), (np.int64(3047), np.int64(3089)), (np.int64(3784), np.int64(3790)), (np.int64(3808), np.int64(3814)), (np.int64(5255), np.int64(5308)), (np.int64(5880), np.int64(5911)), (np.int64(6584), np.int64(6612)), (np.int64(7038), np.int64(7148)), (np.int64(7438), np.int64(7542)), (np.int64(9360), np.int64(9434)), (np.int64(9815), np.int64(9849)), (np.int64(9991), np.int64(9999))] (count: 13)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2543), np.int64(2670)) ✓ FULL (128/128 points, 100.0%)\n    → Overlap with predicted range (np.int64(2541), np.int64(2672)): 128 points\n  Segment 1: (np.int64(3004), np.int64(3087)) ~ PARTIAL (41/84 points, 48.8%)\n    → Overlap with predicted range (np.int64(3047), np.int64(3089)): 41 points\n  Segment 2: (np.int64(3766), np.int64(3826)) ~ PARTIAL (14/61 points, 23.0%)\n    → Overlap with predicted range (np.int64(3784), np.int64(3790)): 7 points\n    → Overlap with predicted range (np.int64(3808), np.int64(3814)): 7 points\n  Segment 3: (np.int64(5188), np.int64(5304)) ~ PARTIAL (50/117 points, 42.7%)\n    → Overlap with predicted range (np.int64(5255), np.int64(5308)): 50 points\n  Segment 4: (np.int64(5882), np.int64(5908)) ✓ FULL (27/27 points, 100.0%)\n    → Overlap with predicted range (np.int64(5880), np.int64(5911)): 27 points\n  Segment 5: (np.int64(6584), np.int64(6609)) ✓ FULL (26/26 points, 100.0%)\n    → Overlap with predicted range (np.int64(6584), np.int64(6612)): 26 points\n  Segment 6: (np.int64(7041), np.int64(7143)) ✓ FULL (103/103 points, 100.0%)\n    → Overlap with predicted range (np.int64(7038), np.int64(7148)): 103 points\n  Segment 7: (np.int64(7441), np.int64(7539)) ✓ FULL (99/99 points, 100.0%)\n    → Overlap with predicted range (np.int64(7438), np.int64(7542)): 99 points\n  Segment 8: (np.int64(9294), np.int64(9429)) ~ PARTIAL (70/136 points, 51.5%)\n    → Overlap with predicted range (np.int64(9360), np.int64(9434)): 70 points\n  Segment 9: (np.int64(9817), np.int64(9846)) ✓ FULL (30/30 points, 100.0%)\n    → Overlap with predicted range (np.int64(9815), np.int64(9849)): 30 points\n\nBasic Metrics:\n  Accuracy:  0.9716\n  Precision: 0.9060\n  Recall:    0.7250\n  F1-Score:  0.8055\n\nAnomaly Statistics:\n  True Anomalies: 811 points\n  Predicted Anomalies: 649 points\n  Detection Rate: 0.7250\n\nRange-based Metrics:\n  Range Precision: 0.7692\n  Range Recall:    1.0000\n  Range F1:        0.8696\n\nPoint-level Performance:\n  True Positives:  588 points\n  False Positives: 61 points\n  False Negatives: 223 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 6\n  Partially Detected: 4\n  Missed: 0\n  Segment Detection Rate: 0.6000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 15 high-confidence anomaly ranges\nIsolation Forest ranges: 18, Z-score ranges: 8\nEVALUATION RESULTS - FILE 7\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2165), np.int64(2305)), (np.int64(2570), np.int64(2611)), (np.int64(4188), np.int64(4306)), (np.int64(4572), np.int64(4721)), (np.int64(5008), np.int64(5095)), (np.int64(6491), np.int64(6560)), (np.int64(7365), np.int64(7484)), (np.int64(8348), np.int64(8452)), (np.int64(8592), np.int64(8614)), (np.int64(9537), np.int64(9614))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2255), np.int64(2309)), (np.int64(2568), np.int64(2613)), (np.int64(4218), np.int64(4224)), (np.int64(4236), np.int64(4248)), (np.int64(4260), np.int64(4272)), (np.int64(4639), np.int64(4726)), (np.int64(5023), np.int64(5099)), (np.int64(6521), np.int64(6527)), (np.int64(7421), np.int64(7488)), (np.int64(8344), np.int64(8455)), (np.int64(8592), np.int64(8616)), (np.int64(9567), np.int64(9579)), (np.int64(9591), np.int64(9597)), (np.int64(9991), np.int64(9999))] (count: 15)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2165), np.int64(2305)) ~ PARTIAL (51/141 points, 36.2%)\n    → Overlap with predicted range (np.int64(2255), np.int64(2309)): 51 points\n  Segment 1: (np.int64(2570), np.int64(2611)) ✓ FULL (42/42 points, 100.0%)\n    → Overlap with predicted range (np.int64(2568), np.int64(2613)): 42 points\n  Segment 2: (np.int64(4188), np.int64(4306)) ~ PARTIAL (33/119 points, 27.7%)\n    → Overlap with predicted range (np.int64(4218), np.int64(4224)): 7 points\n    → Overlap with predicted range (np.int64(4236), np.int64(4248)): 13 points\n    → Overlap with predicted range (np.int64(4260), np.int64(4272)): 13 points\n  Segment 3: (np.int64(4572), np.int64(4721)) ~ PARTIAL (83/150 points, 55.3%)\n    → Overlap with predicted range (np.int64(4639), np.int64(4726)): 83 points\n  Segment 4: (np.int64(5008), np.int64(5095)) ✓ FULL (73/88 points, 83.0%)\n    → Overlap with predicted range (np.int64(5023), np.int64(5099)): 73 points\n  Segment 5: (np.int64(6491), np.int64(6560)) ~ PARTIAL (7/70 points, 10.0%)\n    → Overlap with predicted range (np.int64(6521), np.int64(6527)): 7 points\n  Segment 6: (np.int64(7365), np.int64(7484)) ~ PARTIAL (64/120 points, 53.3%)\n    → Overlap with predicted range (np.int64(7421), np.int64(7488)): 64 points\n  Segment 7: (np.int64(8348), np.int64(8452)) ✓ FULL (105/105 points, 100.0%)\n    → Overlap with predicted range (np.int64(8344), np.int64(8455)): 105 points\n  Segment 8: (np.int64(8592), np.int64(8614)) ✓ FULL (23/23 points, 100.0%)\n    → Overlap with predicted range (np.int64(8592), np.int64(8616)): 23 points\n  Segment 9: (np.int64(9537), np.int64(9614)) ~ PARTIAL (20/78 points, 25.6%)\n    → Overlap with predicted range (np.int64(9567), np.int64(9579)): 13 points\n    → Overlap with predicted range (np.int64(9591), np.int64(9597)): 7 points\n\nBasic Metrics:\n  Accuracy:  0.9516\n  Precision: 0.9109\n  Recall:    0.5353\n  F1-Score:  0.6743\n\nAnomaly Statistics:\n  True Anomalies: 936 points\n  Predicted Anomalies: 550 points\n  Detection Rate: 0.5353\n\nRange-based Metrics:\n  Range Precision: 0.6667\n  Range Recall:    1.0000\n  Range F1:        0.8000\n\nPoint-level Performance:\n  True Positives:  501 points\n  False Positives: 49 points\n  False Negatives: 435 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 4\n  Partially Detected: 6\n  Missed: 0\n  Segment Detection Rate: 0.4000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 14 high-confidence anomaly ranges\nIsolation Forest ranges: 23, Z-score ranges: 9\nEVALUATION RESULTS - FILE 8\n============================================================\nRange Information:\n  True Ranges: [(np.int64(2340), np.int64(2407)), (np.int64(2594), np.int64(2698)), (np.int64(4708), np.int64(4787)), (np.int64(5387), np.int64(5480)), (np.int64(6153), np.int64(6267)), (np.int64(6479), np.int64(6580)), (np.int64(7548), np.int64(7569)), (np.int64(8021), np.int64(8058)), (np.int64(8299), np.int64(8329)), (np.int64(9827), np.int64(9904))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2346), np.int64(2352)), (np.int64(2364), np.int64(2400)), (np.int64(2633), np.int64(2700)), (np.int64(4741), np.int64(4789)), (np.int64(5418), np.int64(5482)), (np.int64(6159), np.int64(6165)), (np.int64(6177), np.int64(6267)), (np.int64(6495), np.int64(6582)), (np.int64(7544), np.int64(7574)), (np.int64(8019), np.int64(8059)), (np.int64(8295), np.int64(8335)), (np.int64(9833), np.int64(9900)), (np.int64(9990), np.int64(9999))] (count: 14)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(2340), np.int64(2407)) ~ PARTIAL (44/68 points, 64.7%)\n    → Overlap with predicted range (np.int64(2346), np.int64(2352)): 7 points\n    → Overlap with predicted range (np.int64(2364), np.int64(2400)): 37 points\n  Segment 1: (np.int64(2594), np.int64(2698)) ~ PARTIAL (66/105 points, 62.9%)\n    → Overlap with predicted range (np.int64(2633), np.int64(2700)): 66 points\n  Segment 2: (np.int64(4708), np.int64(4787)) ~ PARTIAL (47/80 points, 58.8%)\n    → Overlap with predicted range (np.int64(4741), np.int64(4789)): 47 points\n  Segment 3: (np.int64(5387), np.int64(5480)) ~ PARTIAL (63/94 points, 67.0%)\n    → Overlap with predicted range (np.int64(5418), np.int64(5482)): 63 points\n  Segment 4: (np.int64(6153), np.int64(6267)) ✓ FULL (98/115 points, 85.2%)\n    → Overlap with predicted range (np.int64(6159), np.int64(6165)): 7 points\n    → Overlap with predicted range (np.int64(6177), np.int64(6267)): 91 points\n  Segment 5: (np.int64(6479), np.int64(6580)) ✓ FULL (86/102 points, 84.3%)\n    → Overlap with predicted range (np.int64(6495), np.int64(6582)): 86 points\n  Segment 6: (np.int64(7548), np.int64(7569)) ✓ FULL (22/22 points, 100.0%)\n    → Overlap with predicted range (np.int64(7544), np.int64(7574)): 22 points\n  Segment 7: (np.int64(8021), np.int64(8058)) ✓ FULL (38/38 points, 100.0%)\n    → Overlap with predicted range (np.int64(8019), np.int64(8059)): 38 points\n  Segment 8: (np.int64(8299), np.int64(8329)) ✓ FULL (31/31 points, 100.0%)\n    → Overlap with predicted range (np.int64(8295), np.int64(8335)): 31 points\n  Segment 9: (np.int64(9827), np.int64(9904)) ✓ FULL (68/78 points, 87.2%)\n    → Overlap with predicted range (np.int64(9833), np.int64(9900)): 68 points\n\nBasic Metrics:\n  Accuracy:  0.9780\n  Precision: 0.9184\n  Recall:    0.7681\n  F1-Score:  0.8366\n\nAnomaly Statistics:\n  True Anomalies: 733 points\n  Predicted Anomalies: 613 points\n  Detection Rate: 0.7681\n\nRange-based Metrics:\n  Range Precision: 0.7143\n  Range Recall:    1.0000\n  Range F1:        0.8333\n\nPoint-level Performance:\n  True Positives:  563 points\n  False Positives: 50 points\n  False Negatives: 170 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 6\n  Partially Detected: 4\n  Missed: 0\n  Segment Detection Rate: 0.6000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOptimal contamination for precision: 0.0500\nDetected 14 high-confidence anomaly ranges\nIsolation Forest ranges: 28, Z-score ranges: 6\nEVALUATION RESULTS - FILE 9\n============================================================\nRange Information:\n  True Ranges: [(np.int64(3463), np.int64(3494)), (np.int64(3896), np.int64(3993)), (np.int64(4143), np.int64(4224)), (np.int64(5471), np.int64(5578)), (np.int64(6064), np.int64(6145)), (np.int64(6543), np.int64(6636)), (np.int64(7889), np.int64(8021)), (np.int64(8650), np.int64(8756)), (np.int64(9074), np.int64(9225)), (np.int64(9621), np.int64(9740))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(3473), np.int64(3495)), (np.int64(3893), np.int64(3998)), (np.int64(4143), np.int64(4149)), (np.int64(4161), np.int64(4223)), (np.int64(5467), np.int64(5583)), (np.int64(6102), np.int64(6105)), (np.int64(6117), np.int64(6148)), (np.int64(6539), np.int64(6640)), (np.int64(7886), np.int64(8025)), (np.int64(8715), np.int64(8757)), (np.int64(9155), np.int64(9227)), (np.int64(9685), np.int64(9741)), (np.int64(9991), np.int64(9999))] (count: 14)\n\nDetection Details by Segment:\n  Segment 0: (np.int64(3463), np.int64(3494)) ~ PARTIAL (22/32 points, 68.8%)\n    → Overlap with predicted range (np.int64(3473), np.int64(3495)): 22 points\n  Segment 1: (np.int64(3896), np.int64(3993)) ✓ FULL (98/98 points, 100.0%)\n    → Overlap with predicted range (np.int64(3893), np.int64(3998)): 98 points\n  Segment 2: (np.int64(4143), np.int64(4224)) ✓ FULL (70/82 points, 85.4%)\n    → Overlap with predicted range (np.int64(4143), np.int64(4149)): 7 points\n    → Overlap with predicted range (np.int64(4161), np.int64(4223)): 63 points\n  Segment 3: (np.int64(5471), np.int64(5578)) ✓ FULL (108/108 points, 100.0%)\n    → Overlap with predicted range (np.int64(5467), np.int64(5583)): 108 points\n  Segment 4: (np.int64(6064), np.int64(6145)) ~ PARTIAL (33/82 points, 40.2%)\n    → Overlap with predicted range (np.int64(6102), np.int64(6105)): 4 points\n    → Overlap with predicted range (np.int64(6117), np.int64(6148)): 29 points\n  Segment 5: (np.int64(6543), np.int64(6636)) ✓ FULL (94/94 points, 100.0%)\n    → Overlap with predicted range (np.int64(6539), np.int64(6640)): 94 points\n  Segment 6: (np.int64(7889), np.int64(8021)) ✓ FULL (133/133 points, 100.0%)\n    → Overlap with predicted range (np.int64(7886), np.int64(8025)): 133 points\n  Segment 7: (np.int64(8650), np.int64(8756)) ~ PARTIAL (42/107 points, 39.3%)\n    → Overlap with predicted range (np.int64(8715), np.int64(8757)): 42 points\n  Segment 8: (np.int64(9074), np.int64(9225)) ~ PARTIAL (71/152 points, 46.7%)\n    → Overlap with predicted range (np.int64(9155), np.int64(9227)): 71 points\n  Segment 9: (np.int64(9621), np.int64(9740)) ~ PARTIAL (56/120 points, 46.7%)\n    → Overlap with predicted range (np.int64(9685), np.int64(9741)): 56 points\n\nBasic Metrics:\n  Accuracy:  0.9660\n  Precision: 0.9249\n  Recall:    0.7212\n  F1-Score:  0.8105\n\nAnomaly Statistics:\n  True Anomalies: 1008 points\n  Predicted Anomalies: 786 points\n  Detection Rate: 0.7212\n\nRange-based Metrics:\n  Range Precision: 0.7143\n  Range Recall:    1.0000\n  Range F1:        0.8333\n\nPoint-level Performance:\n  True Positives:  727 points\n  False Positives: 59 points\n  False Negatives: 281 points\n\nSegment Detection Metrics:\n  Total Anomaly Segments: 10\n  Fully Detected: 5\n  Partially Detected: 5\n  Missed: 0\n  Segment Detection Rate: 0.5000\n  Overall Detection Rate: 1.0000\n\n\n================================================================================\nOVERALL SUMMARY ACROSS ALL FILES - HIGH-PRECISION VERSION\n================================================================================\nOverall Basic Metrics:\n  Accuracy:  0.9613\n  Precision: 0.8845\n  Recall:    0.6406\n  F1-Score:  0.7365\n\nOverall Range Statistics:\n  Total True Ranges: 100\n  Total Predicted Ranges: 160\n  Total Matched Ranges: 100\n  Range Match Rate: 1.0000\n\nOverall Range-based Metrics:\n  Range Precision: 0.6450\n  Range Recall:    1.0000\n  Range F1:        0.7785\n\nOverall Point-level Performance:\n  True Positives:  5350 points\n  False Positives: 671 points\n  False Negatives: 3197 points\n\nOverall Segment Detection Metrics:\n  Total Anomaly Segments: 100\n  Fully Detected: 53\n  Partially Detected: 47\n  Missed: 0\n  Segment Detection Rate: 0.5300\n  Overall Detection Rate: 1.0000\n\n================================================================================\nFILE-BY-FILE SUMMARY - HIGH-PRECISION VERSION\n================================================================================\nFile   Accuracy   Precision  Recall     F1-Score   True Ranges  Pred Ranges  Fully Det.   Part. Det.   Missed  \n----------------------------------------------------------------------------------------------------\n0      0.9498    0.8507     0.5303     0.6533     10          16          6           4           0       \n1      0.9555    0.9249     0.6345     0.7526     10          16          5           5           0       \n2      0.9612    0.8793     0.5855     0.7029     10          19          4           6           0       \n3      0.9496    0.7885     0.5100     0.6193     10          20          5           5           0       \n4      0.9874    0.8537     0.9075     0.8798     10          12          9           1           0       \n5      0.9425    0.8879     0.4890     0.6307     10          21          3           7           0       \n6      0.9716    0.9060     0.7250     0.8055     10          13          6           4           0       \n7      0.9516    0.9109     0.5353     0.6743     10          15          4           6           0       \n8      0.9780    0.9184     0.7681     0.8366     10          14          6           4           0       \n9      0.9660    0.9249     0.7212     0.8105     10          14          5           5           0       \n----------------------------------------------------------------------------------------------------\n\n================================================================================\nDETAILED RANGE INFORMATION BY FILE\n================================================================================\n\nFile 0:\n  True Ranges: [(np.int64(2533), np.int64(2620)), (np.int64(2864), np.int64(2949)), (np.int64(4811), np.int64(4910)), (np.int64(5148), np.int64(5304)), (np.int64(5492), np.int64(5619)), (np.int64(5986), np.int64(6016)), (np.int64(6222), np.int64(6257)), (np.int64(7088), np.int64(7117)), (np.int64(7599), np.int64(7716)), (np.int64(8633), np.int64(8750))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2533), np.int64(2617)), (np.int64(2860), np.int64(2954)), (np.int64(4808), np.int64(4817)), (np.int64(4900), np.int64(4912)), (np.int64(5236), np.int64(5245)), (np.int64(5291), np.int64(5308)), (np.int64(5491), np.int64(5503)), (np.int64(5608), np.int64(5624)), (np.int64(5982), np.int64(6017)), (np.int64(6219), np.int64(6258)), (np.int64(7084), np.int64(7122)), (np.int64(7595), np.int64(7613)), (np.int64(7711), np.int64(7718)), (np.int64(8628), np.int64(8756)), (np.int64(9986), np.int64(9999))] (count: 16)\n  Matched Pairs:\n    True Range 0 (np.int64(2533), np.int64(2620)) ↔ Predicted (np.int64(2533), np.int64(2617))\n    True Range 1 (np.int64(2864), np.int64(2949)) ↔ Predicted (np.int64(2860), np.int64(2954))\n    True Range 2 (np.int64(4811), np.int64(4910)) ↔ Predicted (np.int64(4808), np.int64(4817))\n    True Range 3 (np.int64(5148), np.int64(5304)) ↔ Predicted (np.int64(5236), np.int64(5245))\n    True Range 4 (np.int64(5492), np.int64(5619)) ↔ Predicted (np.int64(5491), np.int64(5503))\n    True Range 5 (np.int64(5986), np.int64(6016)) ↔ Predicted (np.int64(5982), np.int64(6017))\n    True Range 6 (np.int64(6222), np.int64(6257)) ↔ Predicted (np.int64(6219), np.int64(6258))\n    True Range 7 (np.int64(7088), np.int64(7117)) ↔ Predicted (np.int64(7084), np.int64(7122))\n    True Range 8 (np.int64(7599), np.int64(7716)) ↔ Predicted (np.int64(7595), np.int64(7613))\n    True Range 9 (np.int64(8633), np.int64(8750)) ↔ Predicted (np.int64(8628), np.int64(8756))\n\nFile 1:\n  True Ranges: [(np.int64(2206), np.int64(2325)), (np.int64(2864), np.int64(2978)), (np.int64(3483), np.int64(3565)), (np.int64(3834), np.int64(3926)), (np.int64(4758), np.int64(4878)), (np.int64(6850), np.int64(6957)), (np.int64(7852), np.int64(7954)), (np.int64(8319), np.int64(8428)), (np.int64(9112), np.int64(9225)), (np.int64(9372), np.int64(9471))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2205), np.int64(2327)), (np.int64(2930), np.int64(2981)), (np.int64(3482), np.int64(3567)), (np.int64(3830), np.int64(3931)), (np.int64(4757), np.int64(4882)), (np.int64(6856), np.int64(6862)), (np.int64(6904), np.int64(6910)), (np.int64(6928), np.int64(6934)), (np.int64(7901), np.int64(7906)), (np.int64(7918), np.int64(7959)), (np.int64(8377), np.int64(8430)), (np.int64(9160), np.int64(9166)), (np.int64(9184), np.int64(9190)), (np.int64(9391), np.int64(9476)), (np.int64(9990), np.int64(9999))] (count: 16)\n  Matched Pairs:\n    True Range 0 (np.int64(2206), np.int64(2325)) ↔ Predicted (np.int64(2205), np.int64(2327))\n    True Range 1 (np.int64(2864), np.int64(2978)) ↔ Predicted (np.int64(2930), np.int64(2981))\n    True Range 2 (np.int64(3483), np.int64(3565)) ↔ Predicted (np.int64(3482), np.int64(3567))\n    True Range 3 (np.int64(3834), np.int64(3926)) ↔ Predicted (np.int64(3830), np.int64(3931))\n    True Range 4 (np.int64(4758), np.int64(4878)) ↔ Predicted (np.int64(4757), np.int64(4882))\n    True Range 5 (np.int64(6850), np.int64(6957)) ↔ Predicted (np.int64(6856), np.int64(6862))\n    True Range 6 (np.int64(7852), np.int64(7954)) ↔ Predicted (np.int64(7901), np.int64(7906))\n    True Range 7 (np.int64(8319), np.int64(8428)) ↔ Predicted (np.int64(8377), np.int64(8430))\n    True Range 8 (np.int64(9112), np.int64(9225)) ↔ Predicted (np.int64(9160), np.int64(9166))\n    True Range 9 (np.int64(9372), np.int64(9471)) ↔ Predicted (np.int64(9391), np.int64(9476))\n\nFile 2:\n  True Ranges: [(np.int64(2657), np.int64(2755)), (np.int64(3755), np.int64(3860)), (np.int64(3991), np.int64(4011)), (np.int64(4429), np.int64(4558)), (np.int64(4968), np.int64(5070)), (np.int64(6624), np.int64(6713)), (np.int64(7931), np.int64(8008)), (np.int64(8129), np.int64(8159)), (np.int64(9307), np.int64(9393)), (np.int64(9752), np.int64(9790))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2681), np.int64(2687)), (np.int64(2699), np.int64(2711)), (np.int64(2729), np.int64(2735)), (np.int64(2747), np.int64(2755)), (np.int64(3761), np.int64(3767)), (np.int64(3785), np.int64(3791)), (np.int64(3803), np.int64(3815)), (np.int64(3833), np.int64(3839)), (np.int64(3988), np.int64(4013)), (np.int64(4487), np.int64(4562)), (np.int64(4964), np.int64(5075)), (np.int64(6655), np.int64(6718)), (np.int64(7961), np.int64(7967)), (np.int64(7979), np.int64(7991)), (np.int64(8126), np.int64(8163)), (np.int64(9350), np.int64(9397)), (np.int64(9749), np.int64(9795)), (np.int64(9989), np.int64(9999))] (count: 19)\n  Matched Pairs:\n    True Range 0 (np.int64(2657), np.int64(2755)) ↔ Predicted (np.int64(2681), np.int64(2687))\n    True Range 1 (np.int64(3755), np.int64(3860)) ↔ Predicted (np.int64(3761), np.int64(3767))\n    True Range 2 (np.int64(3991), np.int64(4011)) ↔ Predicted (np.int64(3988), np.int64(4013))\n    True Range 3 (np.int64(4429), np.int64(4558)) ↔ Predicted (np.int64(4487), np.int64(4562))\n    True Range 4 (np.int64(4968), np.int64(5070)) ↔ Predicted (np.int64(4964), np.int64(5075))\n    True Range 5 (np.int64(6624), np.int64(6713)) ↔ Predicted (np.int64(6655), np.int64(6718))\n    True Range 6 (np.int64(7931), np.int64(8008)) ↔ Predicted (np.int64(7961), np.int64(7967))\n    True Range 7 (np.int64(8129), np.int64(8159)) ↔ Predicted (np.int64(8126), np.int64(8163))\n    True Range 8 (np.int64(9307), np.int64(9393)) ↔ Predicted (np.int64(9350), np.int64(9397))\n    True Range 9 (np.int64(9752), np.int64(9790)) ↔ Predicted (np.int64(9749), np.int64(9795))\n\nFile 3:\n  True Ranges: [(np.int64(2240), np.int64(2338)), (np.int64(2824), np.int64(2912)), (np.int64(3685), np.int64(3792)), (np.int64(4212), np.int64(4244)), (np.int64(5007), np.int64(5029)), (np.int64(5758), np.int64(5870)), (np.int64(6665), np.int64(6777)), (np.int64(8492), np.int64(8521)), (np.int64(9099), np.int64(9197)), (np.int64(9628), np.int64(9724))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(24)), (np.int64(2282), np.int64(2291)), (np.int64(2334), np.int64(2342)), (np.int64(2824), np.int64(2909)), (np.int64(3681), np.int64(3689)), (np.int64(3732), np.int64(3744)), (np.int64(3787), np.int64(3797)), (np.int64(4210), np.int64(4248)), (np.int64(5003), np.int64(5034)), (np.int64(5755), np.int64(5762)), (np.int64(5804), np.int64(5824)), (np.int64(5865), np.int64(5874)), (np.int64(6661), np.int64(6678)), (np.int64(6713), np.int64(6729)), (np.int64(6772), np.int64(6781)), (np.int64(8486), np.int64(8527)), (np.int64(9137), np.int64(9155)), (np.int64(9184), np.int64(9202)), (np.int64(9628), np.int64(9725)), (np.int64(9976), np.int64(9999))] (count: 20)\n  Matched Pairs:\n    True Range 0 (np.int64(2240), np.int64(2338)) ↔ Predicted (np.int64(2282), np.int64(2291))\n    True Range 1 (np.int64(2824), np.int64(2912)) ↔ Predicted (np.int64(2824), np.int64(2909))\n    True Range 2 (np.int64(3685), np.int64(3792)) ↔ Predicted (np.int64(3681), np.int64(3689))\n    True Range 3 (np.int64(4212), np.int64(4244)) ↔ Predicted (np.int64(4210), np.int64(4248))\n    True Range 4 (np.int64(5007), np.int64(5029)) ↔ Predicted (np.int64(5003), np.int64(5034))\n    True Range 5 (np.int64(5758), np.int64(5870)) ↔ Predicted (np.int64(5755), np.int64(5762))\n    True Range 6 (np.int64(6665), np.int64(6777)) ↔ Predicted (np.int64(6661), np.int64(6678))\n    True Range 7 (np.int64(8492), np.int64(8521)) ↔ Predicted (np.int64(8486), np.int64(8527))\n    True Range 8 (np.int64(9099), np.int64(9197)) ↔ Predicted (np.int64(9137), np.int64(9155))\n    True Range 9 (np.int64(9628), np.int64(9724)) ↔ Predicted (np.int64(9628), np.int64(9725))\n\nFile 4:\n  True Ranges: [(np.int64(2229), np.int64(2272)), (np.int64(2744), np.int64(2840)), (np.int64(3010), np.int64(3034)), (np.int64(3761), np.int64(3783)), (np.int64(5028), np.int64(5065)), (np.int64(6270), np.int64(6304)), (np.int64(8383), np.int64(8422)), (np.int64(8650), np.int64(8730)), (np.int64(9602), np.int64(9627)), (np.int64(9862), np.int64(9960))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2225), np.int64(2276)), (np.int64(2740), np.int64(2845)), (np.int64(3008), np.int64(3038)), (np.int64(3759), np.int64(3786)), (np.int64(5025), np.int64(5068)), (np.int64(6267), np.int64(6305)), (np.int64(8380), np.int64(8427)), (np.int64(8685), np.int64(8735)), (np.int64(9599), np.int64(9632)), (np.int64(9874), np.int64(9960)), (np.int64(9990), np.int64(9999))] (count: 12)\n  Matched Pairs:\n    True Range 0 (np.int64(2229), np.int64(2272)) ↔ Predicted (np.int64(2225), np.int64(2276))\n    True Range 1 (np.int64(2744), np.int64(2840)) ↔ Predicted (np.int64(2740), np.int64(2845))\n    True Range 2 (np.int64(3010), np.int64(3034)) ↔ Predicted (np.int64(3008), np.int64(3038))\n    True Range 3 (np.int64(3761), np.int64(3783)) ↔ Predicted (np.int64(3759), np.int64(3786))\n    True Range 4 (np.int64(5028), np.int64(5065)) ↔ Predicted (np.int64(5025), np.int64(5068))\n    True Range 5 (np.int64(6270), np.int64(6304)) ↔ Predicted (np.int64(6267), np.int64(6305))\n    True Range 6 (np.int64(8383), np.int64(8422)) ↔ Predicted (np.int64(8380), np.int64(8427))\n    True Range 7 (np.int64(8650), np.int64(8730)) ↔ Predicted (np.int64(8685), np.int64(8735))\n    True Range 8 (np.int64(9602), np.int64(9627)) ↔ Predicted (np.int64(9599), np.int64(9632))\n    True Range 9 (np.int64(9862), np.int64(9960)) ↔ Predicted (np.int64(9874), np.int64(9960))\n\nFile 5:\n  True Ranges: [(np.int64(2661), np.int64(2775)), (np.int64(2863), np.int64(2894)), (np.int64(3006), np.int64(3124)), (np.int64(4118), np.int64(4269)), (np.int64(4463), np.int64(4591)), (np.int64(5953), np.int64(5994)), (np.int64(6107), np.int64(6225)), (np.int64(6429), np.int64(6577)), (np.int64(7969), np.int64(8094)), (np.int64(8886), np.int64(8906))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2679), np.int64(2685)), (np.int64(2703), np.int64(2715)), (np.int64(2727), np.int64(2733)), (np.int64(2863), np.int64(2897)), (np.int64(3004), np.int64(3010)), (np.int64(3056), np.int64(3075)), (np.int64(3121), np.int64(3127)), (np.int64(4180), np.int64(4274)), (np.int64(4463), np.int64(4466)), (np.int64(4513), np.int64(4542)), (np.int64(4588), np.int64(4594)), (np.int64(5950), np.int64(5998)), (np.int64(6137), np.int64(6167)), (np.int64(6179), np.int64(6215)), (np.int64(6484), np.int64(6581)), (np.int64(7968), np.int64(7979)), (np.int64(8019), np.int64(8045)), (np.int64(8083), np.int64(8098)), (np.int64(8885), np.int64(8909)), (np.int64(9984), np.int64(9999))] (count: 21)\n  Matched Pairs:\n    True Range 0 (np.int64(2661), np.int64(2775)) ↔ Predicted (np.int64(2679), np.int64(2685))\n    True Range 1 (np.int64(2863), np.int64(2894)) ↔ Predicted (np.int64(2863), np.int64(2897))\n    True Range 2 (np.int64(3006), np.int64(3124)) ↔ Predicted (np.int64(3004), np.int64(3010))\n    True Range 3 (np.int64(4118), np.int64(4269)) ↔ Predicted (np.int64(4180), np.int64(4274))\n    True Range 4 (np.int64(4463), np.int64(4591)) ↔ Predicted (np.int64(4463), np.int64(4466))\n    True Range 5 (np.int64(5953), np.int64(5994)) ↔ Predicted (np.int64(5950), np.int64(5998))\n    True Range 6 (np.int64(6107), np.int64(6225)) ↔ Predicted (np.int64(6137), np.int64(6167))\n    True Range 7 (np.int64(6429), np.int64(6577)) ↔ Predicted (np.int64(6484), np.int64(6581))\n    True Range 8 (np.int64(7969), np.int64(8094)) ↔ Predicted (np.int64(7968), np.int64(7979))\n    True Range 9 (np.int64(8886), np.int64(8906)) ↔ Predicted (np.int64(8885), np.int64(8909))\n\nFile 6:\n  True Ranges: [(np.int64(2543), np.int64(2670)), (np.int64(3004), np.int64(3087)), (np.int64(3766), np.int64(3826)), (np.int64(5188), np.int64(5304)), (np.int64(5882), np.int64(5908)), (np.int64(6584), np.int64(6609)), (np.int64(7041), np.int64(7143)), (np.int64(7441), np.int64(7539)), (np.int64(9294), np.int64(9429)), (np.int64(9817), np.int64(9846))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2541), np.int64(2672)), (np.int64(3047), np.int64(3089)), (np.int64(3784), np.int64(3790)), (np.int64(3808), np.int64(3814)), (np.int64(5255), np.int64(5308)), (np.int64(5880), np.int64(5911)), (np.int64(6584), np.int64(6612)), (np.int64(7038), np.int64(7148)), (np.int64(7438), np.int64(7542)), (np.int64(9360), np.int64(9434)), (np.int64(9815), np.int64(9849)), (np.int64(9991), np.int64(9999))] (count: 13)\n  Matched Pairs:\n    True Range 0 (np.int64(2543), np.int64(2670)) ↔ Predicted (np.int64(2541), np.int64(2672))\n    True Range 1 (np.int64(3004), np.int64(3087)) ↔ Predicted (np.int64(3047), np.int64(3089))\n    True Range 2 (np.int64(3766), np.int64(3826)) ↔ Predicted (np.int64(3784), np.int64(3790))\n    True Range 3 (np.int64(5188), np.int64(5304)) ↔ Predicted (np.int64(5255), np.int64(5308))\n    True Range 4 (np.int64(5882), np.int64(5908)) ↔ Predicted (np.int64(5880), np.int64(5911))\n    True Range 5 (np.int64(6584), np.int64(6609)) ↔ Predicted (np.int64(6584), np.int64(6612))\n    True Range 6 (np.int64(7041), np.int64(7143)) ↔ Predicted (np.int64(7038), np.int64(7148))\n    True Range 7 (np.int64(7441), np.int64(7539)) ↔ Predicted (np.int64(7438), np.int64(7542))\n    True Range 8 (np.int64(9294), np.int64(9429)) ↔ Predicted (np.int64(9360), np.int64(9434))\n    True Range 9 (np.int64(9817), np.int64(9846)) ↔ Predicted (np.int64(9815), np.int64(9849))\n\nFile 7:\n  True Ranges: [(np.int64(2165), np.int64(2305)), (np.int64(2570), np.int64(2611)), (np.int64(4188), np.int64(4306)), (np.int64(4572), np.int64(4721)), (np.int64(5008), np.int64(5095)), (np.int64(6491), np.int64(6560)), (np.int64(7365), np.int64(7484)), (np.int64(8348), np.int64(8452)), (np.int64(8592), np.int64(8614)), (np.int64(9537), np.int64(9614))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2255), np.int64(2309)), (np.int64(2568), np.int64(2613)), (np.int64(4218), np.int64(4224)), (np.int64(4236), np.int64(4248)), (np.int64(4260), np.int64(4272)), (np.int64(4639), np.int64(4726)), (np.int64(5023), np.int64(5099)), (np.int64(6521), np.int64(6527)), (np.int64(7421), np.int64(7488)), (np.int64(8344), np.int64(8455)), (np.int64(8592), np.int64(8616)), (np.int64(9567), np.int64(9579)), (np.int64(9591), np.int64(9597)), (np.int64(9991), np.int64(9999))] (count: 15)\n  Matched Pairs:\n    True Range 0 (np.int64(2165), np.int64(2305)) ↔ Predicted (np.int64(2255), np.int64(2309))\n    True Range 1 (np.int64(2570), np.int64(2611)) ↔ Predicted (np.int64(2568), np.int64(2613))\n    True Range 2 (np.int64(4188), np.int64(4306)) ↔ Predicted (np.int64(4218), np.int64(4224))\n    True Range 3 (np.int64(4572), np.int64(4721)) ↔ Predicted (np.int64(4639), np.int64(4726))\n    True Range 4 (np.int64(5008), np.int64(5095)) ↔ Predicted (np.int64(5023), np.int64(5099))\n    True Range 5 (np.int64(6491), np.int64(6560)) ↔ Predicted (np.int64(6521), np.int64(6527))\n    True Range 6 (np.int64(7365), np.int64(7484)) ↔ Predicted (np.int64(7421), np.int64(7488))\n    True Range 7 (np.int64(8348), np.int64(8452)) ↔ Predicted (np.int64(8344), np.int64(8455))\n    True Range 8 (np.int64(8592), np.int64(8614)) ↔ Predicted (np.int64(8592), np.int64(8616))\n    True Range 9 (np.int64(9537), np.int64(9614)) ↔ Predicted (np.int64(9567), np.int64(9579))\n\nFile 8:\n  True Ranges: [(np.int64(2340), np.int64(2407)), (np.int64(2594), np.int64(2698)), (np.int64(4708), np.int64(4787)), (np.int64(5387), np.int64(5480)), (np.int64(6153), np.int64(6267)), (np.int64(6479), np.int64(6580)), (np.int64(7548), np.int64(7569)), (np.int64(8021), np.int64(8058)), (np.int64(8299), np.int64(8329)), (np.int64(9827), np.int64(9904))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(2346), np.int64(2352)), (np.int64(2364), np.int64(2400)), (np.int64(2633), np.int64(2700)), (np.int64(4741), np.int64(4789)), (np.int64(5418), np.int64(5482)), (np.int64(6159), np.int64(6165)), (np.int64(6177), np.int64(6267)), (np.int64(6495), np.int64(6582)), (np.int64(7544), np.int64(7574)), (np.int64(8019), np.int64(8059)), (np.int64(8295), np.int64(8335)), (np.int64(9833), np.int64(9900)), (np.int64(9990), np.int64(9999))] (count: 14)\n  Matched Pairs:\n    True Range 0 (np.int64(2340), np.int64(2407)) ↔ Predicted (np.int64(2346), np.int64(2352))\n    True Range 1 (np.int64(2594), np.int64(2698)) ↔ Predicted (np.int64(2633), np.int64(2700))\n    True Range 2 (np.int64(4708), np.int64(4787)) ↔ Predicted (np.int64(4741), np.int64(4789))\n    True Range 3 (np.int64(5387), np.int64(5480)) ↔ Predicted (np.int64(5418), np.int64(5482))\n    True Range 4 (np.int64(6153), np.int64(6267)) ↔ Predicted (np.int64(6159), np.int64(6165))\n    True Range 5 (np.int64(6479), np.int64(6580)) ↔ Predicted (np.int64(6495), np.int64(6582))\n    True Range 6 (np.int64(7548), np.int64(7569)) ↔ Predicted (np.int64(7544), np.int64(7574))\n    True Range 7 (np.int64(8021), np.int64(8058)) ↔ Predicted (np.int64(8019), np.int64(8059))\n    True Range 8 (np.int64(8299), np.int64(8329)) ↔ Predicted (np.int64(8295), np.int64(8335))\n    True Range 9 (np.int64(9827), np.int64(9904)) ↔ Predicted (np.int64(9833), np.int64(9900))\n\nFile 9:\n  True Ranges: [(np.int64(3463), np.int64(3494)), (np.int64(3896), np.int64(3993)), (np.int64(4143), np.int64(4224)), (np.int64(5471), np.int64(5578)), (np.int64(6064), np.int64(6145)), (np.int64(6543), np.int64(6636)), (np.int64(7889), np.int64(8021)), (np.int64(8650), np.int64(8756)), (np.int64(9074), np.int64(9225)), (np.int64(9621), np.int64(9740))] (count: 10)\n  Predicted Ranges: [(np.int64(0), np.int64(9)), (np.int64(3473), np.int64(3495)), (np.int64(3893), np.int64(3998)), (np.int64(4143), np.int64(4149)), (np.int64(4161), np.int64(4223)), (np.int64(5467), np.int64(5583)), (np.int64(6102), np.int64(6105)), (np.int64(6117), np.int64(6148)), (np.int64(6539), np.int64(6640)), (np.int64(7886), np.int64(8025)), (np.int64(8715), np.int64(8757)), (np.int64(9155), np.int64(9227)), (np.int64(9685), np.int64(9741)), (np.int64(9991), np.int64(9999))] (count: 14)\n  Matched Pairs:\n    True Range 0 (np.int64(3463), np.int64(3494)) ↔ Predicted (np.int64(3473), np.int64(3495))\n    True Range 1 (np.int64(3896), np.int64(3993)) ↔ Predicted (np.int64(3893), np.int64(3998))\n    True Range 2 (np.int64(4143), np.int64(4224)) ↔ Predicted (np.int64(4143), np.int64(4149))\n    True Range 3 (np.int64(5471), np.int64(5578)) ↔ Predicted (np.int64(5467), np.int64(5583))\n    True Range 4 (np.int64(6064), np.int64(6145)) ↔ Predicted (np.int64(6102), np.int64(6105))\n    True Range 5 (np.int64(6543), np.int64(6636)) ↔ Predicted (np.int64(6539), np.int64(6640))\n    True Range 6 (np.int64(7889), np.int64(8021)) ↔ Predicted (np.int64(7886), np.int64(8025))\n    True Range 7 (np.int64(8650), np.int64(8756)) ↔ Predicted (np.int64(8715), np.int64(8757))\n    True Range 8 (np.int64(9074), np.int64(9225)) ↔ Predicted (np.int64(9155), np.int64(9227))\n    True Range 9 (np.int64(9621), np.int64(9740)) ↔ Predicted (np.int64(9685), np.int64(9741))\n\nEvaluation completed for all files with high-precision detection methods!\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}